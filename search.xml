<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[Yolo2代码解析]]></title>
      <url>/2017/11/11/Yolo2%E4%BB%A3%E7%A0%81%E8%A7%A3%E6%9E%90/</url>
      <content type="html"><![CDATA[<h2 id="1-激活层"><a href="#1-激活层" class="headerlink" title="1. 激活层"></a>1. 激活层</h2><figure class="highlight ada"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">/*</div><div class="line">计算激活函数对加权输入的导数，并乘以<span class="keyword">delta</span>，得到当前层最终的<span class="keyword">delta</span>（敏感度图）</div><div class="line">输入： x  当前层的所有输出</div><div class="line">n    l.output的维度，即为l.batch * l.out_c * l.out_w * l.out_h（包含整个batch的）</div><div class="line">ACTIVATION    激活函数类型</div><div class="line"><span class="keyword">delta</span>     当前层敏感度图（与当前层输出x维度一样）</div><div class="line"></div><div class="line">说明<span class="number">1</span>： 该函数不但计算了激活函数对于加权输入的导数，还将该导数乘以了之前完成大部分计算的敏感度图<span class="keyword">delta</span>（对应元素相乘），</div><div class="line">因此调用改函数之后，将得到该层最终的敏感度图</div><div class="line"></div><div class="line">说明<span class="number">2</span>： 这里直接利用输出值求激活函数关于输入的导数值是因为神经网络中所使用的绝大部分激活函数，</div><div class="line">其关于输入的导数值都可以描述为输出值的函数表达式，比如对于Sigmoid激活函数（记作f(x)），其导数值为f(x)'=f(x)*(<span class="number">1</span>-f(x)),</div><div class="line">因此如果给出y=f(x)，那么f(x)'=y*(<span class="number">1</span>-y)，只需要输出值y就可以了，不需要输入x的值，</div><div class="line">（暂时不确定darknet中有没有使用特殊的激活函数，以致于必须要输入值才能够求出导数值，</div><div class="line">在activiation.c文件中，有几个激活函数暂时没看懂，也没在网上查到）。</div><div class="line"></div><div class="line">说明<span class="number">3</span>： 关于l.<span class="keyword">delta</span>的初值，可能你有注意到在看某一类型网络层的时候，比如卷积层中的backward_convolutional_layer()函数，</div><div class="line">没有发现在此之前对l.<span class="keyword">delta</span>赋初值的语句，只是用calloc为其动态分配了内存，这样的l.<span class="keyword">delta</span>其所有元素的值都为<span class="number">0</span>,</div><div class="line">那么这里使用*=运算符得到的值将恒为<span class="number">0</span>。是的，如果只看某一层，或者说某一类型的层，的确有这个疑惑，但是整个网络是有很多层的，</div><div class="line">且有多种类型，一般来说，不会以卷积层为最后一层，而回以COST或者REGION为最后一层，这些层中，会对l.<span class="keyword">delta</span>赋初值，</div><div class="line">又由于l.<span class="keyword">delta</span>是由后网前逐层传播的，因此，当反向运行到某一层时，l.<span class="keyword">delta</span>的值将都不会为<span class="number">0</span>.</div><div class="line">*/</div><div class="line"></div><div class="line">void gradient_array(const float *x, const int n, const ACTIVATION a, float *<span class="keyword">delta</span>)</div><div class="line">&#123;</div><div class="line">    int i;</div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</div><div class="line">        <span class="keyword">delta</span>[i] *= gradient(x[i], a);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="2-Softmax层"><a href="#2-Softmax层" class="headerlink" title="2. Softmax层"></a>2. Softmax层</h2><h3 id="2-1-前向传播函数"><a href="#2-1-前向传播函数" class="headerlink" title="2.1 前向传播函数"></a>2.1 前向传播函数</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** softmax层前向传播函数</span></div><div class="line"><span class="comment">** 输入： l   当前softmax层</span></div><div class="line"><span class="comment">**       net 整个网络</span></div><div class="line"><span class="comment">** 说明：softmax层的前向比较简单，只需要对输入中的每个元素做softmax处理就可以，</span></div><div class="line"><span class="comment">**      但是darknet的实现引入了softmax_tree，这个参数的用法尚需要去推敲。</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">forward_softmax_layer</span><span class="params">(<span class="keyword">const</span> softmax_layer l, network net)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="keyword">if</span>(l.softmax_tree)&#123;</div><div class="line">        <span class="keyword">int</span> i;</div><div class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; l.softmax_tree-&gt;groups; ++i) &#123;</div><div class="line">            <span class="keyword">int</span> group_size = l.softmax_tree-&gt;group_size[i];</div><div class="line">            softmax_cpu(net.input + count, group_size, l.batch, l.inputs, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, l.temperature, l.output + count);</div><div class="line">            count += group_size;</div><div class="line">        &#125;</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// 调用softmax_cpu()对输入的每一个元素进行softmax处理</span></div><div class="line">        softmax_cpu(net.input, l.inputs/l.groups, l.batch, l.inputs, l.groups, </div><div class="line">        l.inputs/l.groups, <span class="number">1</span>, l.temperature, l.output);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 输入： input   一组输入图片数据（含义见下面softmax_cpu()注释，下同）</span></div><div class="line"><span class="comment">**       n       一组输入数据中含有的元素个数n=l.inputs/l.groups</span></div><div class="line"><span class="comment">**       temp    温度参数，关于softmax的温度参数，可以搜索一下softmax with temperature，应该会有很多的</span></div><div class="line"><span class="comment">**       stride  跨度</span></div><div class="line"><span class="comment">**       output  这一组输入图片数据对应的输出（也即l.output中与这一组输入对应的某一部分）</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">** 说明：本函数实现的就是标准的softmax函数处理，唯一有点变化的就是在做指数运算之前，</span></div><div class="line"><span class="comment">将每个输入元素减去了该组输入元素中的最大值，以增加数值稳定性，</span></div><div class="line"><span class="comment">**关于此，可以参考博客：</span></div><div class="line"><span class="comment">http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/，</span></div><div class="line"><span class="comment">**这篇博客写的不错，博客中还提到了softmax-loss，此处没有实现（此处实现的也即博客中提到的softmax函数，将softmax-loss分开实现了）。</span></div><div class="line"><span class="comment">*/</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">softmax</span><span class="params">(<span class="keyword">float</span> *input, <span class="keyword">int</span> n, <span class="keyword">float</span> temp, <span class="keyword">int</span> stride, <span class="keyword">float</span> *output)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="keyword">int</span> i;</div><div class="line">    <span class="keyword">float</span> sum = <span class="number">0</span>;</div><div class="line">    <span class="comment">// 赋初始最大值为float中的最小值-FLT_MAX（定义在float.h中）</span></div><div class="line">    <span class="keyword">float</span> largest = -FLT_MAX;</div><div class="line">    <span class="comment">// 寻找输入中的最大值，至于为什么要找出最大值，是为了数值计算上的稳定，详细请戳：</span></div><div class="line">    <span class="comment">//http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/</span></div><div class="line">    <span class="comment">// 这篇博客写的不错，博客在接近尾声的时候，提到了为什么要减去输入中的最大值。</span></div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</div><div class="line">        <span class="keyword">if</span>(input[i*stride] &gt; largest) largest = input[i*stride];</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</div><div class="line"> <span class="comment">// 在进行指数运算之间，如上面博客所说，首先减去最大值（当然温度参数也要除）</span></div><div class="line">        <span class="keyword">float</span> e = <span class="built_in">exp</span>(input[i*stride]/temp - largest/temp);</div><div class="line">        sum += e;                       <span class="comment">// 求和</span></div><div class="line">        output[i*stride] = e;          </div><div class="line">        <span class="comment">// 并将每一个输入的结果保存在相应的输出中</span></div><div class="line">    &#125;</div><div class="line">    <span class="comment">// 最后一步：归一化转换为概率（就是softmax函数的原型～），最后的输出结果保存在output中</span></div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</div><div class="line">        output[i*stride] /= sum;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="2-2-反向传播函数"><a href="#2-2-反向传播函数" class="headerlink" title="2.2 反向传播函数"></a>2.2 反向传播函数</h3><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">/<span class="strong">*</span></div><div class="line"><span class="strong">*</span>* softmax层反向传播函数</div><div class="line"><span class="bullet">** </span>输入： l   当前softmax层</div><div class="line"><span class="bullet">**       </span>net  整个网络</div><div class="line"><span class="bullet">** </span>说明：</div><div class="line"><span class="comment">//      下面注释理解应该有错，另外，对此处softmax反向的实现存在很大的疑问？？？？？？？</span></div><div class="line"><span class="bullet">**      </span>softmax层的反向很简单，由于自身没有训练参数，虽然有激活函数（即softmax函数），</div><div class="line"><span class="bullet">**      </span>但是又因其所处位置特殊，一般处于网络导数第二层，下一层就是cost层，</div><div class="line"><span class="bullet">**      </span>其自身的敏感度图l.delta已经计算完成（如果此处不太明白再说什么，可以参看卷积层的注释，</div><div class="line"><span class="bullet">**      </span>（完成大部分计算，或者全连接层的注释，以及最大池化层的注释），</div><div class="line"><span class="comment">//</span></div><div class="line"><span class="bullet">**      </span>剩下要做的仅剩下利用自身的l.delta计算其上一层的敏感度图</div><div class="line"><span class="bullet">**      </span>还差乘以上一层激活函数关于其加权输入的导数值），</div><div class="line"><span class="bullet">**      </span>即将l.delta中的元素乘以对应的当前层与上一次层之间的权重，</div><div class="line"><span class="bullet">**      </span>而softmax层与上一层输出之间没有权重或者说权重都为1（因为是将输入直接送进softmax函数处理的，</div><div class="line"><span class="bullet">**      </span>并没有加权什么的），且softmax的输出与上一层的输出存在一一对应的关系，</div><div class="line"><span class="bullet">**      </span>所以求取上一层的敏感度图也是很简单，很直接，详见下面的注释。</div><div class="line"><span class="strong">*/</span></div><div class="line"><span class="strong">void backward_softmax_layer(const softmax_layer l, network net)</span></div><div class="line"><span class="strong">&#123;</span></div><div class="line"><span class="strong">    // 由当前softmax层的敏感度图l.delta计算上一层的敏感度图net.delta，调用的函数为axpy_cpu()，</span></div><div class="line"><span class="strong">    // 为什么调用axpy_cpu()函数，因为softmax层的输出元素与上一层的输出元素存在一一对应的关系</span></div><div class="line"><span class="strong">    //（由此可以看出softmax的stride取值为1是必然的，</span></div><div class="line"><span class="strong">    // 再次照应blas.c中softmax_cpu()的注释，如果不为1,肯定不能是一一对应关系），</span></div><div class="line"><span class="strong">    //所以由softmax层的敏感度图计算上一层的敏感度图，</span></div><div class="line"><span class="strong">    // 可以逐个逐个元素计算，不需要类似全连接层中的矩阵乘法，更没有卷积层中的那般复杂。</span></div><div class="line"><span class="strong">    axpy_cpu(l.inputs*</span>l.batch, 1, l.delta, 1, net.delta, 1);</div><div class="line">&#125;</div><div class="line"></div><div class="line">/<span class="strong">*</span></div><div class="line"><span class="strong">*</span>* axpy是线性代数中一种基本操作，完成y= alpha*x + y操作，其中x,y为矢量，alpha为实数系数</div><div class="line"><span class="bullet">** </span>可以参考：</div><div class="line"></div><div class="line"><span class="bullet">** </span>输入：  N       X中包含的有效元素个数</div><div class="line"><span class="bullet">**        </span>ALPHA   系数alpha</div><div class="line"><span class="bullet">**        </span>X       参与运算的矢量X</div><div class="line"><span class="bullet">**        </span>INCX    步长（倍数步长），即X中凡是INCX的倍数编号参与运算</div><div class="line"><span class="bullet">**        </span>Y       参与运算的矢量，也相当于是输出</div><div class="line"><span class="strong">*/</span></div><div class="line"><span class="strong">void axpy_cpu(int N, float ALPHA, float *</span>X, int INCX, float <span class="strong">*Y, int INCY)</span></div><div class="line"><span class="strong">&#123;</span></div><div class="line"><span class="strong">    int i;</span></div><div class="line"><span class="strong">    for(i = 0; i &lt; N; ++i) Y[i*</span>INCY] += ALPHA*X[i*INCX];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="3-全连接层"><a href="#3-全连接层" class="headerlink" title="3. 全连接层"></a>3. 全连接层</h2><h3 id="3-1-全连接层前向传播函数"><a href="#3-1-全连接层前向传播函数" class="headerlink" title="3.1 全连接层前向传播函数"></a>3.1 全连接层前向传播函数</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 全连接层前向传播函数</span></div><div class="line"><span class="comment">** 输入： l     当前全连接层</span></div><div class="line"><span class="comment">**       net   整个网络</span></div><div class="line"><span class="comment">** 流程： 全连接层的前向传播相对简单，首先初始化输出l.output全为0,在进行相关参数赋值之后，直接调用gemm_nt()完成Wx操作，</span></div><div class="line"><span class="comment">**       而后根据判断是否需要BN，如果需要，则进行BN操作，完了之后为每一个输出元素添加偏置得到Wx+b，最后使用激活函数处理</span></div><div class="line"><span class="comment">**       每一个输出元素，得到f(Wx+b)</span></div><div class="line"><span class="comment">*/</span></div><div class="line">void forward_connected_layer(connected_layer l, network net)</div><div class="line">&#123;</div><div class="line">    int i;</div><div class="line">    <span class="comment">// 初始化全连接层的所有输出（包含所有batch）为0值</span></div><div class="line">    fill_cpu(l.outputs*l.batch, <span class="number">0</span>, l.output, <span class="number">1</span>);</div><div class="line"></div><div class="line">    <span class="comment">// m：全连接层接收的一个batch的图片张数</span></div><div class="line">    <span class="comment">// k：全连接层单张输入图片元素个数</span></div><div class="line">    <span class="comment">// n：全连接层对应单张输入图片的输出元素个数</span></div><div class="line">    int m = l.batch;</div><div class="line">    int k = l.inputs;</div><div class="line">    int n = l.outputs;</div><div class="line"></div><div class="line">    <span class="type">float</span> *a = net.input;</div><div class="line">    <span class="type">float</span> *b = l.weights;</div><div class="line">    <span class="type">float</span> *c = l.output;</div><div class="line"></div><div class="line">    <span class="comment">// a：全连接层的输入数据，维度为l.batch*l.inputs（包含整个batch的输入），可视作l.batch行，l.inputs列，每行就是一张输入图片</span></div><div class="line">    <span class="comment">// b：全连接层的所有权重，维度为l.outputs*l.inputs(见make_connected_layer())</span></div><div class="line">    <span class="comment">// c：全连接层的所有输出（包含所有batch），维度为l.batch*l.outputs（包含整个batch的输出）</span></div><div class="line">    <span class="comment">// 根据维度匹配规则，显然需要对b进行转置，故而调用gemm_nt()函数，最终计算得到的c的维度为l.batch*l.outputs,</span></div><div class="line">    <span class="comment">// 全连接层的的输出很好计算，直接矩阵相承就可以了，所谓全连接，就是全连接层的输出与输入的每一个元素都有关联（当然是同一张图片内的，</span></div><div class="line">    <span class="comment">// 最中得到的c有l.batch行,l.outputs列，每行就是一张输入图片对应的输出）</span></div><div class="line">    <span class="comment">// m：a的行，值为l.batch，含义为全连接层接收的一个batch的图片张数</span></div><div class="line">    <span class="comment">// n：b'的列数，值为l.outputs，含义为全连接层对应单张输入图片的输出元素个数</span></div><div class="line">    <span class="comment">// k：a的列数，值为l.inputs，含义为全连接层单张输入图片元素个数</span></div><div class="line">    gemm(<span class="number">0</span>,<span class="number">1</span>,m,n,k,<span class="number">1</span>,a,k,b,k,<span class="number">1</span>,c,n);</div><div class="line"></div><div class="line">    if(l.batch_normalize)&#123;</div><div class="line">        if(net.train)&#123;</div><div class="line">            <span class="comment">// 计算全连接层l.output中每个元素的的均值，得到的l.mean是一个维度为l.outputs的矢量，</span></div><div class="line">            <span class="comment">// 也即全连接层每一个输出元素都有一个平均值（有batch张输入图片，需要计算这batch图片对应输出元素的平均值），</span></div><div class="line">            <span class="comment">// 对全连接层而言，每个输出就是一个通道，且每张特征图的维度为1*1</span></div><div class="line">            mean_cpu(l.output, l.batch, l.outputs, <span class="number">1</span>, l.mean);</div><div class="line">            <span class="comment">// 计算全连接层每个输出元素的方差l,variance，其维度与l.mean一样</span></div><div class="line">            variance_cpu(l.output, l.mean, l.batch, l.outputs, <span class="number">1</span>, l.variance);</div><div class="line"></div><div class="line">            scal_cpu(l.outputs, <span class="number">.95</span>, l.rolling_mean, <span class="number">1</span>);</div><div class="line">            axpy_cpu(l.outputs, <span class="number">.05</span>, l.mean, <span class="number">1</span>, l.rolling_mean, <span class="number">1</span>);</div><div class="line">            scal_cpu(l.outputs, <span class="number">.95</span>, l.rolling_variance, <span class="number">1</span>);</div><div class="line">            axpy_cpu(l.outputs, <span class="number">.05</span>, l.variance, <span class="number">1</span>, l.rolling_variance, <span class="number">1</span>);</div><div class="line"></div><div class="line">            copy_cpu(l.outputs*l.batch, l.output, <span class="number">1</span>, l.x, <span class="number">1</span>);</div><div class="line">            normalize_cpu(l.output, l.mean, l.variance, l.batch, l.outputs, <span class="number">1</span>);   </div><div class="line">            copy_cpu(l.outputs*l.batch, l.output, <span class="number">1</span>, l.x_norm, <span class="number">1</span>);</div><div class="line">        &#125; else &#123;</div><div class="line">            normalize_cpu(l.output, l.rolling_mean, l.rolling_variance, l.batch, l.outputs, <span class="number">1</span>);</div><div class="line">        &#125;</div><div class="line">        scale_bias(l.output, l.scales, l.batch, l.outputs, <span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// 前面得到的是全连接层每个输出元素的加权输入Wx，下面这个循环就是为每个元素加上偏置，最终得到每个输出元素上的加权输入：Wx+b</span></div><div class="line">    <span class="comment">// 循环次数为l.batch，不是l.outputs，是因为对于全连接层来说，l.batch = l.outputs，无所谓了～</span></div><div class="line">    for(i = <span class="number">0</span>; i &lt; l.batch; ++i)&#123;</div><div class="line">        <span class="comment">// axpy_cpu()完成l.output + i*l.outputs = l.biases + (l.output + i*l.outputs)操作</span></div><div class="line">        <span class="comment">// l.biases的维度为l.outputs;l.output的维度为l.batch*l.outputs，包含整个batch的输出，所以需要注意移位</span></div><div class="line">        axpy_cpu(l.outputs, <span class="number">1</span>, l.biases, <span class="number">1</span>, l.output + i*l.outputs, <span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="comment">// 前向传播最后一步：前面得到每一个输出元素的加权输入Wx+b,这一步利用激活函数处理l.output中的每一个输出元素，</span></div><div class="line">    <span class="comment">// 最终得到全连接层的输出f(Wx+b)</span></div><div class="line">    activate_array(l.output, l.outputs*l.batch, l.activation);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="3-2-全连接层反向传播函数"><a href="#3-2-全连接层反向传播函数" class="headerlink" title="3.2 全连接层反向传播函数"></a>3.2 全连接层反向传播函数</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 全连接层反向传播函数</span></div><div class="line"><span class="comment">** 输入： l     当前全连接层</span></div><div class="line"><span class="comment">**       net   整个网络</span></div><div class="line"><span class="comment">** 流程：先完成之前为完成的计算：计算当前层的敏感度图l.delta（注意是反向传播），</span></div><div class="line"><span class="comment">//而后调用axpy_cpu()函数计算当前全连接层的偏置更新值（基于完全计算完的l.delta），</span></div><div class="line"><span class="comment">**      然后判断是否进行BN，如果进行，则完成BN操作，再接着计算当前层权重更新值，最后计算上一层的敏感度图（完成大部分计算）。</span></div><div class="line"><span class="comment">//相比于卷积神经网络，全连接层很多的计算变得更为直接，不需要调用诸如im2col_cpu()或者col2im_cpu()函数</span></div><div class="line"><span class="comment">//对数据重排来重排去，直接矩阵相乘就可以搞定。</span></div><div class="line"><span class="comment">*/</span></div><div class="line">void backward_connected_layer(connected_layer l, network net)</div><div class="line">&#123;</div><div class="line">    int i;</div><div class="line">    <span class="comment">// 完成当前层敏感度图的计算：当前全连接层下一层不管是什么类型的网络，都会完成当前层敏感度图的绝大部分计算（上一层敏感度乘以上一层与当前层之间的权重）</span></div><div class="line">    <span class="comment">// （注意是反向传播），此处只需要再将l.delta中的每一个元素乘以激活函数对加权输入的导数即可</span></div><div class="line">    <span class="comment">// gradient_array()函数完成激活函数对加权输入的导数，并乘以之前得到的l.delta，得到当前层最终的l.delta（误差函数对加权输入的导数）</span></div><div class="line">    gradient_array(l.output, l.outputs*l.batch, l.activation, l.delta);</div><div class="line"></div><div class="line">    <span class="comment">// 计算当前全连接层的偏置更新值</span></div><div class="line">    <span class="comment">// 相比于卷积层的偏置更新值，此处更为简单（卷积层中有专门的偏置更新值计算函数，主要原因是卷积核在图像上做卷积即权值共享增加了复杂度，</span></div><div class="line">    <span class="comment">//而全连接层没有权值共享），只需调用axpy_cpu()函数就可以完成。误差函数对偏置的导数实际就等于以上刚求完的敏感度值，</span></div><div class="line">    <span class="comment">//因为有多张图片，需要将多张图片的效果叠加，故而循环调用axpy_cpu()函数，</span></div><div class="line">    <span class="comment">// 不同于卷积层每个卷积核才有一个偏置参数，全连接层是每个输出元素就对应有一个偏置参数，共有l.outputs个，</span></div><div class="line">    <span class="comment">//每次循环将求完一张图片所有输出的偏置更新值。</span></div><div class="line">    <span class="comment">// l.bias_updates虽然没有明显的初始化操作，但其在make_connected_layer()中是用calloc()动态分配内存的，</span></div><div class="line">    <span class="comment">//因此其已经全部初始化为0值。</span></div><div class="line">    <span class="comment">// 循环结束后，最终会把每一张图的偏置更新值叠加，因此，最终l.bias_updates中每一个元素的值是batch中</span></div><div class="line">    <span class="comment">//所有图片对应输出元素偏置更新值的叠加。</span></div><div class="line">    for(i = <span class="number">0</span>; i &lt; l.batch; ++i)&#123;</div><div class="line">        axpy_cpu(l.outputs, <span class="number">1</span>, l.delta + i*l.outputs, <span class="number">1</span>, l.bias_updates, <span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">    if(l.batch_normalize)&#123;</div><div class="line">        backward_scale_cpu(l.x_norm, l.delta, l.batch, l.outputs, <span class="number">1</span>, l.scale_updates);</div><div class="line"></div><div class="line">        scale_bias(l.delta, l.scales, l.batch, l.outputs, <span class="number">1</span>);</div><div class="line"></div><div class="line">        mean_delta_cpu(l.delta, l.variance, l.batch, l.outputs, <span class="number">1</span>, l.mean_delta);</div><div class="line">        variance_delta_cpu(l.x, l.delta, l.mean, l.variance, l.batch, l.outputs, <span class="number">1</span>, l.variance_delta);</div><div class="line">        normalize_delta_cpu(l.x, l.mean, l.variance, l.mean_delta, l.variance_delta, l.batch, l.outputs, <span class="number">1</span>, l.delta);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 计算当前全连接层的权重更新值</span></div><div class="line">    int m = l.outputs;</div><div class="line">    int k = l.batch;</div><div class="line">    int n = l.inputs;</div><div class="line">    <span class="type">float</span> *a = l.delta;</div><div class="line">    <span class="type">float</span> *b = net.input;</div><div class="line">    <span class="type">float</span> *c = l.weight_updates;</div><div class="line"></div><div class="line">    <span class="comment">// a：当前全连接层敏感度图，维度为l.batch*l.outputs</span></div><div class="line">    <span class="comment">// b：当前全连接层所有输入，维度为l.batch*l.inputs</span></div><div class="line">    <span class="comment">// c：当前全连接层权重更新值，维度为l.outputs*l.inputs（权重个数）</span></div><div class="line">    <span class="comment">// 由行列匹配规则可知，需要将a转置，故而调用gemm_tn()函数，转置a实际上是想把batch中所有图片的影响叠加。</span></div><div class="line">    <span class="comment">// 全连接层的权重更新值的计算也相对简单，简单的矩阵乘法即可完成：当前全连接层的敏感度图乘以当前层的输入即可得到当前全连接层的权重更新值，</span></div><div class="line">    <span class="comment">// （当前层的敏感度是误差函数对于加权输入的导数，所以再乘以对应输入值即可得到权重更新值）</span></div><div class="line">    <span class="comment">// m：a'的行，值为l.outputs，含义为每张图片输出的元素个数</span></div><div class="line">    <span class="comment">// n：b的列数，值为l.inputs，含义为每张输入图片的元素个数</span></div><div class="line">    <span class="comment">// k：a’的列数，值为l.batch，含义为一个batch中含有的图片张数</span></div><div class="line">    <span class="comment">// 最终得到的c维度为l.outputs*l.inputs，对应所有权重的更新值</span></div><div class="line">    gemm(<span class="number">1</span>,<span class="number">0</span>,m,n,k,<span class="number">1</span>,a,m,b,n,<span class="number">1</span>,c,n);</div><div class="line"></div><div class="line">    <span class="comment">// 由当前全连接层计算上一层的敏感度图（完成绝大部分计算：当前全连接层敏感度图乘以当前层还未更新的权重）</span></div><div class="line">    m = l.batch;</div><div class="line">    k = l.outputs;</div><div class="line">    n = l.inputs;</div><div class="line"></div><div class="line">    a = l.delta;</div><div class="line">    b = l.weights;</div><div class="line">    c = net.delta;</div><div class="line"></div><div class="line">    <span class="comment">// 一定注意此时的c等于net.delta，已经在network.c中的backward_network()函数中赋值为上一层的delta</span></div><div class="line">    <span class="comment">// a：当前全连接层敏感度图，维度为l.batch*l.outputs</span></div><div class="line">    <span class="comment">// b：当前层权重（连接当前层与上一层），维度为l.outputs*l.inputs</span></div><div class="line">    <span class="comment">// c：上一层敏感度图（包含整个batch），维度为l.batch*l.inputs</span></div><div class="line">    <span class="comment">// 由行列匹配规则可知，不需要转置。由全连接层敏感度图计算上一层的敏感度图也很简单，直接利用矩阵相乘，</span></div><div class="line">    <span class="comment">//将当前层l.delta与当前层权重相乘就可以了，只需要注意要不要转置，拿捏好就可以，不需要像卷积层一样，需要对权重或者输入重排！</span></div><div class="line">    <span class="comment">// m：a的行，值为l.batch，含义为一个batch中含有的图片张数</span></div><div class="line">    <span class="comment">// n：b的列数，值为l.inputs，含义为每张输入图片的元素个数</span></div><div class="line">    <span class="comment">// k：a的列数，值为l.outputs，含义为每张图片输出的元素个数</span></div><div class="line">    <span class="comment">// 最终得到的c维度为l.bacth*l.inputs（包含所有batch）</span></div><div class="line">    if(c) gemm(<span class="number">0</span>,<span class="number">0</span>,m,n,k,<span class="number">1</span>,a,k,b,n,<span class="number">1</span>,c,n);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="4-卷积层"><a href="#4-卷积层" class="headerlink" title="4. 卷积层"></a>4. 卷积层</h2><h3 id="4-1-卷积运算的加速实现"><a href="#4-1-卷积运算的加速实现" class="headerlink" title="4.1  卷积运算的加速实现"></a>4.1  卷积运算的加速实现</h3><p>将图像平铺成一行，每一段均可以直接与卷积核相乘，根据Stride大小可能有重复元素，因此元素总个数也可能变多，这样做是为了更方便快捷的并行进行卷积运算！<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">**  从输入的多通道数组im（存储图像数据）中获取指定行、列、、通道数处的元素值</span></div><div class="line"><span class="comment">**  输入： im      输入，所有数据存成一个一维数组，例如对于3通道的二维图像而言，</span></div><div class="line"><span class="comment">**                每一通道按行存储（每一通道所有行并成一行），三通道依次再并成一行</span></div><div class="line"><span class="comment">**        height  每一通道的高度（即输入图像的真正的高度，补0之前）</span></div><div class="line"><span class="comment">**        width   每一通道的宽度（即输入图像的宽度，补0之前）</span></div><div class="line"><span class="comment">**        channels 输入im的通道数，比如彩色图为3通道，之后每一卷积层的输入的通道数等于上一卷积层卷积核的个数</span></div><div class="line"><span class="comment">**        row     要提取的元素所在的行（二维图像补0之后的行数）</span></div><div class="line"><span class="comment">**        col     要提取的元素所在的列（二维图像补0之后的列数）</span></div><div class="line"><span class="comment">**        channel 要提取的元素所在的通道</span></div><div class="line"><span class="comment">**        pad     图像左右上下各补0的长度（四边补0的长度一样）</span></div><div class="line"><span class="comment">**  返回： float类型数据，为im中channel通道，row-pad行，col-pad列处的元素值</span></div><div class="line"><span class="comment">而row与col则是补0之后，元素所在的行列，因此，要准确获取在im中的元素值，首先需要减去pad以获取在im中真实的行列数</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="keyword">float</span> im2col_get_pixel(<span class="keyword">float</span> *im, <span class="keyword">int</span> <span class="built_in">height</span>, <span class="keyword">int</span> <span class="built_in">width</span>, <span class="keyword">int</span> channels,<span class="keyword">int</span> row, <span class="keyword">int</span> col, <span class="keyword">int</span> channel, <span class="keyword">int</span> pad)</div><div class="line">&#123;</div><div class="line">    <span class="comment">// 减去补0长度，获取元素真实的行列数</span></div><div class="line">    row -= pad;</div><div class="line">    col -= pad;</div><div class="line"></div><div class="line">    <span class="comment">// 如果行列数小于0,则返回0（刚好是补0的效果）</span></div><div class="line">    <span class="built_in">if</span> (row &lt; <span class="number">0</span> || col &lt; <span class="number">0</span> ||</div><div class="line">        row &gt;= <span class="built_in">height</span> || col &gt;= <span class="built_in">width</span>) <span class="built_in">return</span> <span class="number">0</span>;</div><div class="line">    </div><div class="line">    <span class="comment">// im存储多通道二维图像的数据的格式为：各通道所有行并成一行，再多通道依次并成一行，</span></div><div class="line">    <span class="comment">// 因此width*height*channel首先移位到所在通道的起点位置，加上width*row移位到所在指定通道所在行，再加上col移位到所在列</span></div><div class="line">    <span class="built_in">return</span> im[col + <span class="built_in">width</span>*(row + <span class="built_in">height</span>*channel)];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//From Berkeley Vision's Caffe!</span></div><div class="line"><span class="comment">//https://github.com/BVLC/caffe/blob/master/LICENSE</span></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 将输入图片转为便于计算的数组格式，可以参考https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/</span></div><div class="line"><span class="comment">** 进行辅助理解（但执行方式并不同，只是用于概念上的辅助理解），由作者的注释可知，这是直接从caffe移植过来的</span></div><div class="line"><span class="comment">** 输入： data_im    输入图像</span></div><div class="line"><span class="comment">**       channels   输入图像的通道数（对于第一层，一般是颜色图，3通道，中间层通道数为上一层卷积核个数）</span></div><div class="line"><span class="comment">**       height     输入图像的高度（行）</span></div><div class="line"><span class="comment">**       width      输入图像的宽度（列）</span></div><div class="line"><span class="comment">**       ksize      卷积核尺寸</span></div><div class="line"><span class="comment">**       stride     卷积核跨度</span></div><div class="line"><span class="comment">**       pad        四周补0长度</span></div><div class="line"><span class="comment">**       data_col   相当于输出，为进行格式重排后的输入图像数据</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">** 说明：1）此函数个人感觉在实现上存在不足，传入参数没有必要这么多，只需传入当前卷积层的指针即可，这样函数中的一些代码就会变的多余</span></div><div class="line"><span class="comment">**      2）输出data_col的元素个数与data_im元素个数不相等，一般比data_im的元素个数多，因为stride较小，</span></div><div class="line"><span class="comment">各个卷积核之间有很多重叠，实际data_col中的元素个数为channels*ksize*ksize*height_col* width_col，</span></div><div class="line"><span class="comment">其中channels为data_im的通道数，ksize为卷积核大小，height_col和width_col如下所注。data_col的还是按行排列，只是行数为</span></div><div class="line"><span class="comment">channels*ksize*ksize,列数为height_col*width_col，即一张特征图总的元素个数，每整列包含与某个位置处的卷积核计算的所有通道上的像素</span></div><div class="line"><span class="comment">（比如输入图像通道数为3,卷积核尺寸为3*3，则共有27行，每列有27个元素），不同列对应卷积核在图像上的不同位置做卷积</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="keyword">void</span> im2col_cpu(<span class="keyword">float</span>* data_im,</div><div class="line">     <span class="keyword">int</span> channels,  <span class="keyword">int</span> <span class="built_in">height</span>,  <span class="keyword">int</span> <span class="built_in">width</span>,</div><div class="line">     <span class="keyword">int</span> ksize,  <span class="keyword">int</span> stride, <span class="keyword">int</span> pad, <span class="keyword">float</span>* data_col) </div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> c,h,w;</div><div class="line">    <span class="comment">// 计算该层神经网络的输出图像尺寸（其实没有必要再次计算的，因为在构建卷积层时，make_convolutional_layer()函数已经调用</span></div><div class="line"><span class="comment">//函数参数只要传入该层网络指针就可了，没必要弄这么多参数）</span></div><div class="line">    <span class="keyword">int</span> height_col = (<span class="built_in">height</span> + <span class="number">2</span>*pad - ksize) / stride + <span class="number">1</span>;</div><div class="line">    <span class="keyword">int</span> width_col = (<span class="built_in">width</span> + <span class="number">2</span>*pad - ksize) / stride + <span class="number">1</span>;</div><div class="line"></div><div class="line">    <span class="comment">// 卷积核大小：ksize*ksize是一个卷积核的大小，之所以乘以通道数channels，是因为输入图像有多通道，</span></div><div class="line">    <span class="comment">//每个卷积核在做卷积时，是同时对同一位置处多通道的图像进行卷积运算，这里为了实现这一目的，将三通道上的卷积核并在一起以便进行计算，</span></div><div class="line">    <span class="comment">//实际就是同一个卷积核的复制，比如对于3通道图像，卷积核尺寸为3*3，该卷积核将同时作用于三通道图像上，</span></div><div class="line">    <span class="comment">//这样并起来就得到含有27个元素的卷积核,能不能使得作用在不同通道上的卷积核有不同参数呢？</span></div><div class="line">    <span class="comment">//不知道有没有这样的做法？可以思考下，当然这样做肯定会是参数剧增！！</span></div><div class="line">    <span class="keyword">int</span> channels_col = channels * ksize * ksize;</div><div class="line">    </div><div class="line">    <span class="comment">// 这三层循环之间的逻辑关系，决定了输入图像重排后的格式，更为详细/形象的说明可参考博客</span></div><div class="line">    <span class="comment">// 外循环次数为一个卷积核的尺寸数，循环次数即为最终得到的data_col的总行数</span></div><div class="line">    <span class="built_in">for</span> (c = <span class="number">0</span>; c &lt; channels_col; ++c) &#123;</div><div class="line">        <span class="comment">// 列偏移，卷积核是一个二维矩阵，并按行存储在一维数组中，利用求余运算获取对应在卷积核中的列数，</span></div><div class="line">        <span class="comment">//比如对于3*3的卷积核（3通道），当c=0时，显然在第一列，当c=5时，显然在第2列，</span></div><div class="line">        <span class="comment">//当c=9时，在第二通道上的卷积核的第一列，当c=26时，在第三列（第三通道上）</span></div><div class="line">        <span class="keyword">int</span> w_offset = c % ksize;</div><div class="line">        <span class="comment">// 行偏移，卷积核是一个二维的矩阵，且是按行（卷积核所有行并成一行）存储在一维数组中的，</span></div><div class="line">        <span class="comment">// 比如对于3*3的卷积核，处理3通道的图像，那么一个卷积核具有27个元素，每9个元素对应一个通道上的卷积核（互为一样），</span></div><div class="line">        <span class="comment">// 每当c为3的倍数，就意味着卷积核换了一行，h_offset取值为0,1,2，对应3*3卷积核中的第1, 2, 3行</span></div><div class="line">        <span class="keyword">int</span> h_offset = (c / ksize) % ksize;</div><div class="line">        <span class="comment">// 通道偏移，channels_col是多通道的卷积核并在一起的，比如对于3通道，3*3卷积核，每过9个元素就要换一通道数，</span></div><div class="line">        <span class="comment">// 当c=0~8时，c_im=0;c=9~17时，c_im=1;c=18~26时，c_im=2</span></div><div class="line">        <span class="keyword">int</span> c_im = c / ksize / ksize;</div><div class="line"></div><div class="line">        <span class="comment">// 中循环次数等于该层输出图像行数height_col，说明data_col中的每一行存储了一张特征图，</span></div><div class="line">        <span class="comment">//这张特征图又是按行存储在data_col中的某行中</span></div><div class="line">        <span class="built_in">for</span> (h = <span class="number">0</span>; h &lt; height_col; ++h) &#123;</div><div class="line">            <span class="comment">// 内循环等于该层输出图像列数width_col，说明最终得到的data_col总有channels_col行，height_col*width_col列</span></div><div class="line">            <span class="built_in">for</span> (w = <span class="number">0</span>; w &lt; width_col; ++w) &#123;</div><div class="line">                <span class="comment">// 由上面可知，对于3*3的卷积核，h_offset取值为0,1,2,当h_offset=0时，会提取出所有与卷积核第一行元素进行运算的像素，</span></div><div class="line">                <span class="comment">// 依次类推；加上h*stride是对卷积核进行行移位操作，比如卷积核从图像(0,0)位置开始做卷积，</span></div><div class="line">                <span class="comment">//那么最先开始涉及(0,0)~(3,3)之间的像素值，若stride=2，那么卷积核进行一次行移位时，</span></div><div class="line">                <span class="comment">//下一行的卷积操作是从元素(2,0)（2为图像行号，0为列号）开始</span></div><div class="line">                <span class="keyword">int</span> im_row = h_offset + h * stride;</div><div class="line">                <span class="comment">// 对于3*3的卷积核，w_offset取值也为0,1,2，当w_offset取1时，会提取出所有与卷积核中第2列元素进行运算的像素，</span></div><div class="line">                <span class="comment">// 实际在做卷积操作时，卷积核对图像逐行扫描做卷积，加上w*stride就是为了做列移位，</span></div><div class="line">                <span class="comment">// 比如前一次卷积其实像素元素为(0,0)，若stride=2,那么下次卷积元素起始像素位置为(0,2)（0为行号，2为列号）</span></div><div class="line">                <span class="keyword">int</span> im_col = w_offset + w * stride;</div><div class="line"></div><div class="line">                <span class="comment">// col_index为重排后图像中的像素索引，等于c * height_col * width_col + h * width_col +w（还是按行存储，所有通道再并成一行），</span></div><div class="line">                <span class="comment">// 对应第c通道，h行，w列的元素</span></div><div class="line">                <span class="keyword">int</span> col_index = (c * height_col + h) * width_col + w;</div><div class="line">                </div><div class="line">                <span class="comment">// im2col_get_pixel函数获取输入图像data_im中第c_im通道，im_row,im_col的像素值并赋值给重排后的图像，</span></div><div class="line">                <span class="comment">// height和width为输入图像data_im的真实高、宽，pad为四周补0的长度（注意im_row,im_col是补0之后的行列号，</span></div><div class="line">                <span class="comment">// 不是真实输入图像中的行列号，因此需要减去pad获取真实的行列号）</span></div><div class="line">                data_col[col_index] = im2col_get_pixel(data_im, <span class="built_in">height</span>, <span class="built_in">width</span>, channels,im_row, im_col, c_im, pad);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="4-2-更新卷积核的偏置"><a href="#4-2-更新卷积核的偏置" class="headerlink" title="4.2 更新卷积核的偏置"></a>4.2 更新卷积核的偏置</h3><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 计算每个卷积核的偏置更新值，所谓偏置更新值，就是bias = bias - alpha * bias_update中的bias_update</span></div><div class="line"><span class="comment">** 输入： bias_updates     当前层所有偏置的更新值，维度为l.n（即当前层卷积核的个数）</span></div><div class="line"><span class="comment">**       delta            当前层的敏感度图（即l.delta）</span></div><div class="line"><span class="comment">**       batch            一个batch含有的图片张数（即l.batch）</span></div><div class="line"><span class="comment">**       n                当前层卷积核个数（即l.h）</span></div><div class="line"><span class="comment">**       k                当前层输入特征图尺寸（即l.out_w*l.out_h）</span></div><div class="line"><span class="comment">** 原理：当前层的敏感度图l.delta是误差函数对加权输入的导数，也就是偏置更新值，只是其中每l.out_w*l.out_h个元素都对应同一个</span></div><div class="line"><span class="comment">偏置，因此需要将其加起来，得到的和就是误差函数对当前层各偏置的导数（l.delta的维度为l.batch*l.n*l.out_h*l.out_w,</span></div><div class="line"><span class="comment">可理解成共有l.batch行，每行有l.n*l.out_h*l.out_w列，而这一大行又可以理解成有l.n，l.out_h*l.out_w列，</span></div><div class="line"><span class="comment">这每一小行就对应同一个卷积核也即同一个偏置）</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="keyword">void</span> backward_bias(<span class="keyword">float</span> *bias_updates, <span class="keyword">float</span> *delta, <span class="keyword">int</span> batch, <span class="keyword">int</span> n, <span class="keyword">int</span> <span class="built_in">size</span>)</div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> i,b;</div><div class="line">    <span class="comment">// 遍历batch中每张输入图片</span></div><div class="line">    <span class="comment">// 注意，最后的偏置更新值是所有输入图片的总和（多张图片无非就是重复一张图片的操作，求和即可）。</span></div><div class="line">    <span class="comment">// 总之：一个卷积核对应一个偏置更新值，该偏置更新值等于batch中所有输入图片累积的偏置更新值，</span></div><div class="line">    <span class="comment">// 而每张图片也需要进行偏置更新值求和（因为每个卷积核在每张图片多个位置做了卷积运算，这都对偏置更新值有贡献）</span></div><div class="line">    <span class="comment">//以得到每张图片的总偏置更新值。</span></div><div class="line">    <span class="built_in">for</span>(b = <span class="number">0</span>; b &lt; batch; ++b)&#123;</div><div class="line">        <span class="comment">// 求和得一张输入图片的总偏置更新值</span></div><div class="line">        <span class="built_in">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</div><div class="line">            bias_updates[i] += sum_array(delta+<span class="built_in">size</span>*(i+b*n), <span class="built_in">size</span>);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 将以a为首地址此后n个元素相加，返回总和</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="keyword">float</span> sum_array(<span class="keyword">float</span> *a, <span class="keyword">int</span> n)</div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> i;</div><div class="line">    <span class="keyword">float</span> sum = <span class="number">0</span>;</div><div class="line">    <span class="built_in">for</span>(i = <span class="number">0</span>; i &lt; n; ++i) sum += a[i];</div><div class="line">    <span class="built_in">return</span> sum;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="4-3-卷积层前向传播函数"><a href="#4-3-卷积层前向传播函数" class="headerlink" title="4.3 卷积层前向传播函数"></a>4.3 卷积层前向传播函数</h3><figure class="highlight processing"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">void</span> forward_convolutional_layer(convolutional_layer l, network net)</div><div class="line">&#123;</div><div class="line">    <span class="built_in">int</span> out_h = l.out_h;</div><div class="line">    <span class="built_in">int</span> out_w = l.out_w;</div><div class="line">    <span class="built_in">int</span> i;</div><div class="line">    <span class="comment">/*l.outputs = l.out_h * l.out_w * l.out_c在make各网络层函数中赋值（比如make_convolutional_layer()），</span></div><div class="line"><span class="comment">    对应每张输入图片的所有输出特征图的总元素个数（每张输入图片会得到n也即l.out_c张特征图）</span></div><div class="line"><span class="comment">    初始化输出l.output全为0.0；输入l.outputs*l.batch为输出的总元素个数，其中l.outputs为batch</span></div><div class="line"><span class="comment">    中一个输入对应的输出的所有元素的个数，l.batch为一个batch输入包含的图片张数；0表示初始化所有输出为0；*/</span></div><div class="line">    fill_cpu(l.outputs*l.batch, <span class="number">0</span>, l.output, <span class="number">1</span>);</div><div class="line"></div><div class="line">   <span class="comment">/*是否进行二值化操作（这个操作应该只有第一个卷积层使用吧？因为下面直接对net.input操作，这个理解是错误的，</span></div><div class="line"><span class="comment">   因为在forward_network()含中，每进行一层都会将net.input = l.output，即下一层的输入被设置为当前层的输出）*/</span></div><div class="line">    <span class="keyword">if</span>(l.xnor)&#123;</div><div class="line">        binarize_weights(l.weights, l.n, l.c*l.<span class="built_in">size</span>*l.<span class="built_in">size</span>, l.binary_weights);</div><div class="line">        swap_binary(&amp;l);</div><div class="line">        binarize_cpu(net.input, l.c*l.h*l.w*l.batch, l.binary_input);</div><div class="line">        net.input = l.binary_input;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="built_in">int</span> m = l.n;                <span class="comment">// 该层卷积核个数</span></div><div class="line">    <span class="built_in">int</span> k = l.<span class="built_in">size</span>*l.<span class="built_in">size</span>*l.c;  <span class="comment">// 该层每个卷积核的参数元素个数</span></div><div class="line">    <span class="built_in">int</span> n = out_h*out_w;        <span class="comment">// 该层每个特征图的尺寸（元素个数）</span></div><div class="line"></div><div class="line"></div><div class="line">    <span class="built_in">float</span> *a = l.weights;       <span class="comment">// 所有卷积核（也即权重），元素个数为l.n*l.c*l.size*l.size，按行存储，共有l*n行，l.c*l.size*l.size列</span></div><div class="line">    <span class="built_in">float</span> *b = net.workspace;   <span class="comment">// 对输入图像进行重排之后的图像数据</span></div><div class="line">    <span class="built_in">float</span> *c = l.output;        <span class="comment">// 存储一张输入图片（多通道）所有的输出特征图（输入图片是多通道的，输出图片也是多通道的，有多少个卷积核就有多少个通道，每个卷积核得到一张特征图即为一个通道）</span></div><div class="line"></div><div class="line">    <span class="comment">/*该循环即为卷积计算核心代码：所有卷积核对batch中每张图片进行卷积运算！！</span></div><div class="line"><span class="comment">    可以参考：https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/</span></div><div class="line"><span class="comment">    进行辅助理解（主要是辅助理解，实际执行并不一样）。每次循环处理一张输入图片（所有卷积核对batch中一张图片做卷积运算）*/</span></div><div class="line">    </div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; l.batch; ++i)&#123;</div><div class="line">        <span class="comment">/*将多通道二维图像net.input变成按一定存储规则排列的数组b，以方便、高效地进行矩阵（卷积）计算，</span></div><div class="line"><span class="comment">        详细查看该函数注释（比较复杂).注意net.input包含batch中所有图片的数据，但是每次循环只处理一张</span></div><div class="line"><span class="comment">        （本循环最后一句对net.input进行了移位），因此在im2col_cpu仅会对其中一张图片</span></div><div class="line"><span class="comment">        进行重排，l.c为每张图片的通道数，l.h为每张图片的高度，l.w为每张图片的宽度，l.size为卷积核尺寸，l.stride为跨度</span></div><div class="line"><span class="comment">        得到的b为一张图片重排后的结果，也是按行存储的一维数组（共有l.c*l.size*l.size行，l.out_w*l.out_h列），*/</span></div><div class="line">        im2col_cpu(net.input, l.c, l.h, l.w, </div><div class="line">                l.<span class="built_in">size</span>, l.stride, l.pad, b);</div><div class="line"></div><div class="line">        <span class="comment">/*GEneral Matrix to Matrix Multiplication</span></div><div class="line"><span class="comment">        // 此处在im2col_cpu操作基础上，利用矩阵乘法c=alpha*a*b+beta*c完成对图像卷积的操作</span></div><div class="line"><span class="comment">        // 0,0表示不对输入a,b进行转置，</span></div><div class="line"><span class="comment">        // m是输入a,c的行数，具体含义为每个卷积核的个数，</span></div><div class="line"><span class="comment">        // n是输入b,c的列数，具体含义为每个输出特征图的元素个数(out_h*out_w)，</span></div><div class="line"><span class="comment">        // k是输入a的列数也是b的行数，具体含义为卷积核元素个数乘以输入图像的通道数（l.size*l.size*l.c），</span></div><div class="line"><span class="comment">        // a,b,c即为三个参与运算的矩阵（用一维数组存储）,alpha=beta=1为常系数，</span></div><div class="line"><span class="comment">        // a为所有卷积核集合,元素个数为l.n*l.c*l.size*l.size，按行存储，共有l*n行，l.c*l.size*l.size列，</span></div><div class="line"><span class="comment">        // 即a中每行代表一个可以作用在3通道上的卷积核，</span></div><div class="line"><span class="comment">        // b为一张输入图像经过im2col_cpu重排后的图像数据（共有l.c*l.size*l.size行，l.out_w*l.out_h列），</span></div><div class="line"><span class="comment">        // c为gemm()计算得到的值，包含一张输入图片得到的所有输出特征图（每个卷积核得到一张特征图），c中一行代表一张特征图，</span></div><div class="line"><span class="comment">        // 各特征图铺排开成一行后，再将所有特征图并成一大行，存储在c中，因此c可视作有l.n行，l.out_h*l.out_w列。</span></div><div class="line"><span class="comment">        // 详细查看该函数注释（比较复杂）*/</span></div><div class="line">        gemm(<span class="number">0</span>,<span class="number">0</span>,m,n,k,<span class="number">1</span>,a,k,b,n,<span class="number">1</span>,c,n);</div><div class="line"></div><div class="line">        <span class="comment">/*// 对c进行指针偏移：移到batch中下一张图片对应输出的起始位置（每循环一次，将完成对一张图片的卷积操作，</span></div><div class="line"><span class="comment">        // 产生的所有特征图的元素个数总和为n*m）*/</span></div><div class="line">        c += n*m;</div><div class="line">        <span class="comment">// 同样，输入也进行指针偏移，移动到下一张图片元素的起始位置，以便下一次循环处理</span></div><div class="line">        <span class="comment">// （batch中每张图片的元素个数为通道数*高度*宽度，即l.c*l.h*l.w）</span></div><div class="line">        net.input += l.c*l.h*l.w;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//如需要规范化（BN在非线性激活函数处理之前完成）</span></div><div class="line">    <span class="keyword">if</span>(l.batch_normalize)&#123;</div><div class="line">        forward_batchnorm_layer(l, net);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        add_bias(l.output, l.biases, l.batch, l.n, out_h*out_w);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    activate_array(l.output, m*n*l.batch, l.activation);</div><div class="line">    <span class="keyword">if</span>(l.<span class="built_in">binary</span> || l.xnor) swap_binary(&amp;l);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="4-4-卷积层反向传播函数"><a href="#4-4-卷积层反向传播函数" class="headerlink" title="4.4 卷积层反向传播函数"></a>4.4 卷积层反向传播函数</h3><figure class="highlight mel"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 卷积神经网络反向传播核心函数</span></div><div class="line"><span class="comment">** 主要流程：</span></div><div class="line"><span class="comment">1） 调用gradient_array()计算当前层l所有输出元素关于加权输入的导数值（也即激活函数关于输入的导数值），</span></div><div class="line"><span class="comment">并乘上上一次调用backward_convolutional_layer()还没计算完的l.delta，得到当前层最终的敏感度图；</span></div><div class="line"><span class="comment">2） 如果网络进行了BN，则需要进行BN的梯度计算；</span></div><div class="line"><span class="comment">3） 如果网络没有进行BN，则直接调用 backward_bias()计算当前层所有卷积核的偏置更新值；</span></div><div class="line"><span class="comment">4） 依次调用im2col_cpu()，gemm_nt()函数计算当前层权重系数更新值；</span></div><div class="line"><span class="comment">5） 如果上一层的delta已经动态分配了内存，则依次调用gemm_tn(), </span></div><div class="line"><span class="comment">col2im_cpu()计算上一层的敏感度图（并未完成所有计算，还差一个步骤）；</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">** 强调：每次调用本函数会计算完成当前层的敏感度计算，同时计算当前层的偏置、权重更新值，除此之外，还会计算上一层的敏感度图，</span></div><div class="line"><span class="comment">但是要注意的是，并没有完全计算完，还差一步：乘上激活函数对加权输入的导数值。这一步在下一次调用本函数时完成。</span></div><div class="line"><span class="comment">*/</span></div><div class="line"></div><div class="line">void backward_convolutional_layer(convolutional_layer l, network net)</div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> i;</div><div class="line">    <span class="keyword">int</span> m = l.n;                <span class="comment">// 卷积核个数</span></div><div class="line">    <span class="comment">// 每一个卷积核元素个数（包括l.c（l.c为该层网络接受的输入图片的通道数）个通道上的卷积核元素个数总数，比如卷积核尺寸为3*3,</span></div><div class="line">    <span class="comment">// 输入图片有3个通道，因为要同时作用于3个通道上，所以需要额外复制两次这个卷积核，那么一个卷积核共有27个元素）</span></div><div class="line">    <span class="keyword">int</span> n = l.<span class="keyword">size</span>*l.<span class="keyword">size</span>*l.c;</div><div class="line">    <span class="keyword">int</span> k = l.out_w*l.out_h;    <span class="comment">// 每张输出特征图的元素个数：out_w，out_h是输出特征图的宽高</span></div><div class="line"></div><div class="line">    <span class="comment">// 计算当前层激活函数对加权输入的导数值并乘以l.delta相应元素，从而彻底完成当前层敏感度图的计算，得到当前层的敏感度图l.delta。</span></div><div class="line">    <span class="comment">// l.output存储了该层网络的所有输出：该层网络接受一个batch的输入图片，其中每张图片经卷积处理后得到的特征图尺寸为：l.out_w,l.out_h，</span></div><div class="line">    <span class="comment">// 该层卷积网络共有l.n个卷积核，因此一张输入图片共输出l.n张宽高为l.out_w,l.out_h的特征图（l.outputs为一张图所有输出特征图的总元素个数），</span></div><div class="line">    <span class="comment">// 所以所有输入图片也即l.output中的总元素个数为：l.n*l.out_w*l.out_h*l.batch；</span></div><div class="line">    <span class="comment">// l.activation为该卷积层的激活函数类型，l.delta就是gradient_array()函数计算得到的</span></div><div class="line">   <span class="comment">//l.output中每一个元素关于激活函数函数输入的导数值，</span></div><div class="line">    <span class="comment">// 注意，这里直接利用输出值求得激活函数关于输入的导数值是因为神经网络中所使用的绝大部分激活函数关于输入的导数值</span></div><div class="line">    <span class="comment">//都可以描述为输出值的函数表达式，比如对于Sigmoid激活函数（记作f(x)），其导数值为f(x)'=f(x)*(1-f(x)),</span></div><div class="line">    <span class="comment">//因此如果给出y=f(x)，那么f(x)'=y*(1-y)，只需要输出值y就可以了，不需要输入x的值，</span></div><div class="line">    <span class="comment">// （暂时不确定darknet中有没有使用特殊的激活函数，以致于必须要输入值才能够求出导数值，</span></div><div class="line">    <span class="comment">//在activiation.c文件中，有几个激活函数暂时没看懂，也没在网上查到）。</span></div><div class="line">    <span class="comment">// l.delta是一个一维数组，长度为l.batch * l.outputs（其中l.outputs = l.out_h * l.out_w * l.out_c），</span></div><div class="line">    <span class="comment">//在make_convolutional_layer()动态分配内存；</span></div><div class="line">    <span class="comment">// 再强调一次：gradient_array()不单单是完成激活函数对输入的求导运算，还完成计算当前层敏感度图的最后一步：</span></div><div class="line">    <span class="comment">//l.delta中每个元素乘以激活函数对输入的导数（注意gradient_arry中使用的是*=运算符）。</span></div><div class="line">    <span class="comment">// 每次调用backward_convolutional_laye时，都会完成当前层敏感度图的计算，同时会计算上一层的敏感度图，</span></div><div class="line">    <span class="comment">//但对于上一层，其敏感度图并没有完全计算完成，还差一步，</span></div><div class="line">    <span class="comment">// 需要等到下一次调用backward_convolutional_layer()时来完成，诚如col2im_cpu()中注释一样。</span></div><div class="line">    </div><div class="line">    gradient_array(l.output, m*k*l.batch, l.activation, l.delta);</div><div class="line"></div><div class="line">    <span class="keyword">if</span>(l.batch_normalize)&#123;</div><div class="line">        backward_batchnorm_layer(l, net);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// 计算偏置的更新值：每个卷积核都有一个偏置，偏置的更新值也即误差函数对偏置的导数，这个导数的计算很简单，</span></div><div class="line">        <span class="comment">//实际所有的导数已经求完了，都存储在l.delta中，</span></div><div class="line">        <span class="comment">// 接下来只需把l.delta中对应同一个卷积核的项加起来就可以（卷积核在图像上逐行逐列跨步移动做卷积，</span></div><div class="line">        <span class="comment">//每个位置处都有一个输出，共有l.out_w*l.out_h个，</span></div><div class="line">        <span class="comment">// 这些输出都与同一个偏置关联，因此将l.delta中对应同一个卷积核的项加起来即得误差函数对这个偏置的导数）</span></div><div class="line">        backward_bias(l.bias_updates, l.delta, l.batch, l.n, k);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 遍历batch中的每张照片，对于l.delta来说，每张照片是分开存的，因此其维度会达到：l.batch*l.n*l.out_w*l.out_h，</span></div><div class="line">    <span class="comment">// 对于l.weights,l.weight_updates以及上面提到的l.bias,l.bias_updates，是将所有照片对应元素叠加起来</span></div><div class="line">    <span class="comment">// （循环的过程就是叠加的过程，注意gemm()这系列函数含有叠加效果，不是覆盖输入C的值，而是叠加到之前的C上），</span></div><div class="line">    <span class="comment">// 因此l.weights与l.weight_updates维度为l.n*l.size*l.size，l.bias与</span></div><div class="line">    <span class="comment">//l.bias_updates的维度为l.h，都与l.batch无关</span></div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; l.batch; ++i)&#123;</div><div class="line">        <span class="keyword">float</span> *a = l.delta + i*m*k;</div><div class="line">        <span class="comment">// net.workspace的元素个数为所有层中最大的l.workspace_size（在make_convolutional_layer()</span></div><div class="line">        <span class="comment">//计算得到workspace_size的大小，在parse_network_cfg()中动态分配内存，此值对应未使用gpu时的情况）,</span></div><div class="line">        <span class="comment">// net.workspace充当一个临时工作空间的作用，存储临时所需要的计算参数，比如每层单张图片重排后的结果</span></div><div class="line">        <span class="comment">//（这些参数马上就会参与卷积运算），一旦用完，就会被马上更新（因此该变量的值的更新频率比较大）</span></div><div class="line">        <span class="keyword">float</span> *b = net.<span class="keyword">workspace</span>;</div><div class="line">        <span class="keyword">float</span> *c = l.weight_updates;</div><div class="line"></div><div class="line">        <span class="comment">// 进入本函数之前，在backward_network()函数中，已经将net.input赋值为prev.output，</span></div><div class="line">        <span class="comment">//也即若当前层为第l层，net.input此时已经是第l-1层的输出</span></div><div class="line">        <span class="keyword">float</span> *im = net.input+i*l.c*l.h*l.w;</div><div class="line"></div><div class="line">        <span class="comment">// 下面两步：im2col_cpu()与gemm()是为了计算当前层的权重更新值（其实也就是误差函数对当前成权重的导数）</span></div><div class="line">        <span class="comment">// 将多通道二维图像net.input变成按一定存储规则排列的数组b，以方便、高效地进行矩阵（卷积）计算，详细查看该函数注释（比较复杂），</span></div><div class="line">        <span class="comment">// im2col_cpu每次仅处理net.input（包含整个batch）中的一张输入图片（对于第一层，则就是读入的图片，</span></div><div class="line">        <span class="comment">//对于之后的层，这些图片都是上一层的输出，通道数等于上一层卷积核个数）。</span></div><div class="line">        <span class="comment">// 最终重排的b为l.c * l.size * l.size行，l.out_h * l.out_w列。</span></div><div class="line">        <span class="comment">// 你会发现在前向forward_convolutional_layer()函数中，也为每层的输入进行了重排，</span></div><div class="line">        <span class="comment">//但是很遗憾的是，并没有一个l.workspace把每一层的重排结果保存下来，而是统一存储到net.workspace中，</span></div><div class="line">        <span class="comment">// 并被不断擦除更新，那为什么不保存呢？保存下来不是省掉一大笔额外重复计算开销？原因有两个：</span></div><div class="line">        <span class="comment">//1）net.workspace中只存储了一张输入图片的重排结果，所以重排下张图片时，马上就会被擦除，</span></div><div class="line">        <span class="comment">// 当然你可能会想，那为什么不弄一个l.worspaces将每层所有输入图片的结果保存呢？这引出第二个原因；</span></div><div class="line">        <span class="comment">//2）计算成本是降低了，但存储空间需求急剧增加，想想每一层都有l.batch张图，且每张都是多通道的，</span></div><div class="line">        <span class="comment">// 重排后其元素个数还会增多，这个存储量搁谁都受不了，如果一个batch有128张图，输入图片尺寸为400*400，</span></div><div class="line">        <span class="comment">//3通道，网络有16层（假设每层输入输出尺寸及通道数都一样），那么单单为了存储这些重排结果，</span></div><div class="line">        <span class="comment">// 就需要128*400*400*3*16*4/1024/1024/1024 = 3.66G，所以为了权衡，只能重复计算！</span></div><div class="line">        im2col_cpu(im, l.c, l.h, l.w, </div><div class="line">                l.<span class="keyword">size</span>, l.stride, l.pad, b);</div><div class="line"></div><div class="line">        <span class="comment">// 下面计算当前层的权重更新值，所谓权重更新值就是weight = weight - alpha * weight_update中的weight_update，</span></div><div class="line">        <span class="comment">// 权重更新值等于当前层敏感度图中每个元素乘以相应的像素值，因为一个权重跟当前层多个输出有关联</span></div><div class="line">        <span class="comment">//（权值共享，即卷积核在图像中跨步移动做卷积，每个位置卷积得到的值</span></div><div class="line">        <span class="comment">// 都与该权值相关），所以对每一个权重更新值来说，需要在l.delta中找出所有与之相关的敏感度，</span></div><div class="line">        <span class="comment">//乘以相应像素值，再求和，具体实现的方式依靠im2col_cpu()与gemm_nt()完成。</span></div><div class="line">        <span class="comment">// （backward_convolutional_layer整个函数的代码非常重要，仅靠文字没有公式与图表辅助说明可能很难说清，</span></div><div class="line">        <span class="comment">//所以这部分更为清晰详细的说明，请参考个人博客！）</span></div><div class="line">        <span class="comment">// GEneral Matrix to Matrix Multiplication</span></div><div class="line">        <span class="comment">// 此处在im2col_cpu操作基础上，利用矩阵乘法c=alpha*a*b+beta*c完成对图像卷积的操作；</span></div><div class="line">        <span class="comment">// 0表示不对输入a进行转置，1表示对输入b进行转置；</span></div><div class="line">        <span class="comment">// m是输入a,c的行数，具体含义为卷积核的个数(l.n)；</span></div><div class="line">        <span class="comment">// n是输入b,c的列数，具体含义为每个卷积核元素个数乘以输入图像的通道数(l.size*l.size*l.c)；</span></div><div class="line">        <span class="comment">// k是输入a的列数也是b的行数，具体含义为每个输出特征图的元素个数（l.out_w*l.out_h）；</span></div><div class="line">        <span class="comment">// a,b,c即为三个参与运算的矩阵（用一维数组存储）,alpha=beta=1为常系数；</span></div><div class="line">        <span class="comment">// a为l.delta的一大行。l.delta为本层所有输出元素（包含整个batch中每张图片的所有输出特征图）</span></div><div class="line">        <span class="comment">//关于加权输入的导数（即激活函数的导数值）集合,</span></div><div class="line">        <span class="comment">// 元素个数为l.batch * l.out_h * l.out_w * l.out_c（l.out_c = l.n），</span></div><div class="line">        <span class="comment">//按行存储，共有l.batch行，l.out_c * l.out_h * l.out_w列，</span></div><div class="line">        <span class="comment">// 即l.delta中每行包含一张图的所有输出图，故这么一大行，又可以视作有l.out_c（l.out_c=l.n）小行，</span></div><div class="line">        <span class="comment">//l.out_h*l*out_w小列，而一次循环就是处理l.delta的一大行，</span></div><div class="line">        <span class="comment">// 故可以将a视作l.out_c行，l.out_h*l*out_w列的矩阵；</span></div><div class="line">        <span class="comment">// b为单张输入图像经过im2col_cpu重排后的图像数据；</span></div><div class="line">        <span class="comment">// c为输出，按行存储，可视作有l.n行，l.c*l.size*l.size列（l.c是输入图像的通道数，l.n是卷积核个数），</span></div><div class="line">	        <span class="comment">// 即c就是所谓的误差项（输出关于加权输入的导数），或者敏感度（强烈推荐：https://www.zybuluo.com/hanbingtao/note/485480）</span></div><div class="line">	        <span class="comment">//（一个核有l.c*l.size*l.size个权重，共有l.n个核）。</span></div><div class="line">        <span class="comment">// 由上可知：</span></div><div class="line">        <span class="comment">// a: (l.out_c) * (l.out_h*l*out_w)</span></div><div class="line">        <span class="comment">// b: (l.c * l.size * l.size) * (l.out_h * l.out_w)</span></div><div class="line">        <span class="comment">// c: (l.n) * (l.c*l.size*l.size)（注意：l.n = l.out_c）</span></div><div class="line">        <span class="comment">// 故要进行a * b + c计算，必须对b进行转置（否则行列不匹配），因故调用gemm_nt()函数</span></div><div class="line">        gemm(<span class="number">0</span>,<span class="number">1</span>,m,n,k,<span class="number">1</span>,a,k,b,k,<span class="number">1</span>,c,n);</div><div class="line"></div><div class="line">        <span class="comment">// 接下来，用当前层的敏感度图l.delta以及权重l.weights（还未更新）来获取上一层网络的敏感度图，</span></div><div class="line">        <span class="comment">//BP算法的主要流程就是依靠这种层与层之间敏感度反向递推传播关系来实现。</span></div><div class="line">      <span class="comment">//而每次开始遍历某一层网络之前，都会更新net.input为这一层网络前一层的输出，即prev.output,</span></div><div class="line">        <span class="comment">// 同时更新net.delta为prev.delta，因此，这里的net.delta是当前层前一层的敏感度图。</span></div><div class="line">        <span class="comment">// 已经强调很多次了，再说一次：下面得到的上一层的敏感度并不完整，完整的敏感度图是损失函数对上一层的加权输入的导数，</span></div><div class="line">        <span class="comment">// 而这里得到的敏感度图是损失函数对上一层输出值的导数，还差乘以一个输出值也即激活函数对加权输入的导数。</span></div><div class="line">        <span class="keyword">if</span>(net.delta)&#123;</div><div class="line">            <span class="comment">// 当前层还未更新的权重</span></div><div class="line">            a = l.weights;</div><div class="line"></div><div class="line">            <span class="comment">// 每次循环仅处理一张输入图，注意移位（l.delta的维度为l.batch * l.out_c * l.out_w * l.out_h）（注意l.n = l.out_c，另外提一下，对整个网络来说，每一层的l.batch其实都是一样的）</span></div><div class="line">            b = l.delta + i*m*k;</div><div class="line"></div><div class="line">            <span class="comment">// net.workspace和上面一样，还是一张输入图片的重排，不同的是，此处我们只需要这个容器，</span></div><div class="line">            <span class="comment">//而里面存储的值我们并不需要，在后面的处理过程中，</span></div><div class="line">            <span class="comment">// 会将其中存储的值一一覆盖掉（尺寸维持不变，还是(l.c * l.size * l.size) * (l.out_h * l.out_w）</span></div><div class="line">            c = net.<span class="keyword">workspace</span>;</div><div class="line"></div><div class="line">            <span class="comment">// 相比上一个gemm，此处的a对应上一个的c,b对应上一个的a，c对应上一个的b，即此处a,b,c的行列分别为：</span></div><div class="line">            <span class="comment">// a: (l.n) * (l.c*l.size*l.size)，表示当前层所有权重系数</span></div><div class="line">            <span class="comment">// b: (l.out_c) * (l.out_h*l*out_w)（注意：l.n = l.out_c），表示当前层的敏感度图</span></div><div class="line">            <span class="comment">// c: (l.c * l.size * l.size) * (l.out_h * l.out_w)，表示上一层的敏感度图</span></div><div class="line">            <span class="comment">//（其元素个数等于上一层网络单张输入图片的所有输出元素个数），</span></div><div class="line">            <span class="comment">// 此时要完成a * b + c计算，必须对a进行转置（否则行列不匹配），因故调用gemm_tn()函数。</span></div><div class="line">            <span class="comment">// 此操作含义是用：用当前层还未更新的权重值对敏感度图做卷积，得到包含上一层所有敏感度信息的矩阵，</span></div><div class="line">            <span class="comment">//但这不是上一层最终的敏感度图，因为此时的c，也即net.workspace的尺寸为</span></div><div class="line">            <span class="comment">//(l.c * l.size * l.size)*(l.out_h * l.out_w)，明显不是上一层的输出尺寸l.c*l.w*l.h，</span></div><div class="line">            <span class="comment">// 接下来还需要调用col2im_cpu()函数将其恢复至l.c*l.w*l.h（可视为l.c行，l.w*l.h列），</span></div><div class="line">            <span class="comment">//这才是上一层的敏感度图（实际还差一个环节，这个环节需要等到下一次调用backward_convolutional_layer()才完成：</span></div><div class="line">            <span class="comment">//将net.delta中每个元素乘以激活函数对加权输入的导数值）。完成gemm这一步，如col2im_cpu()中注释，</span></div><div class="line">            <span class="comment">//是考虑了多个卷积核导致的一对多关系（上一层的一个输出元素会流入到下一层多个输出元素中），</span></div><div class="line">            <span class="comment">// 接下来调用col2im_cpu()则是考虑卷积核重叠（步长较小）导致的一对多关系。</span></div><div class="line">            gemm(<span class="number">1</span>,<span class="number">0</span>,n,k,m,<span class="number">1</span>,a,n,b,k,<span class="number">0</span>,c,k);</div><div class="line"></div><div class="line">            <span class="comment">// 对c也即net.workspace进行重排，得到的结果存储在net.delta中，每次循环只会处理一张输入图片，</span></div><div class="line">            <span class="comment">//因此，此处只会得到一张输入图产生的敏感图（注意net.delta的移位）,</span></div><div class="line">            <span class="comment">// 整个循环结束后，net.delta的总尺寸为l.batch * l.h * l.w * l.c，这就是上一层网络整个batch的敏感度图，</span></div><div class="line">            <span class="comment">//可视为有l.batch行，l.h*l.w*l.c列，每行存储了一张输入图片所有输出特征图的敏感度</span></div><div class="line">            <span class="comment">// col2im_cpu()函数中会调用col2im_add_pixel()函数，该函数中使用了+=运算符，</span></div><div class="line">            <span class="comment">//也即该函数要求输入的net.delta的初始值为0,而在gradient_array()中注释到l.delta的元素是不为0（也不能为0）的，</span></div><div class="line">            <span class="comment">// 看上去是矛盾的，实则不然，gradient_array()使用的l.delta是当前层的敏感度图，</span></div><div class="line">            <span class="comment">//而在col2im_cpu()使用的net.delta是上一层的敏感度图，正如gradient_array()中所注释的，</span></div><div class="line">            <span class="comment">// 当前层l.delta之所以不为0,是因为从后面层反向传播过来的，对于上一层，显然还没有反向传播到那，</span></div><div class="line">            <span class="comment">//因此net.delta的初始值都是为0的（注意，每一层在构建时，就为其delta动态分配了内存，</span></div><div class="line">            <span class="comment">// 且在前向传播时，为每一层的delta都赋值为0,可以参考network.c中forward_network()函数）</span></div><div class="line">            col2im_cpu(net.<span class="keyword">workspace</span>, l.c,  l.h,  l.w,  l.<span class="keyword">size</span>,  l.stride, l.pad, net.delta+i*l.c*l.h*l.w);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="5-Dropout层"><a href="#5-Dropout层" class="headerlink" title="5. Dropout层"></a>5. Dropout层</h2><h3 id="5-1-前向传播"><a href="#5-1-前向传播" class="headerlink" title="5.1 前向传播"></a>5.1 前向传播</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** dropout层前向传播函数</span></div><div class="line"><span class="comment">** 输入： l    当前dropout层网络</span></div><div class="line"><span class="comment">**       net  整个网络</span></div><div class="line"><span class="comment">** 说明：dropout层同样没有训练参数，因此前向传播比较简单，只完成一个事：按指定概率l.probability，</span></div><div class="line"><span class="comment">**      丢弃输入元素，并将保留下来的输入元素乘以比例因子（采用的是inverted dropout，这种方式实现更为方便，</span></div><div class="line"><span class="comment">**      且代码接口比较统一，想想看，如果采用标准的droput，则测试阶段还需要进入forward_dropout_layer()，</span></div><div class="line"><span class="comment">**      使每个输入乘以保留概率，而使用inverted dropout，测试阶段根本就不需要进入到forward_dropout_layer）。</span></div><div class="line"><span class="comment">** 说明2：dropout层输入与输出元素个数相同（即l.intputs=l.outputs）</span></div><div class="line"><span class="comment">** 说明3：关于inverted dropout，在网上随便搜索关于dropout的博客，都会讲到，这里给一个博客链接：https://yq.aliyun.com/articles/68901</span></div><div class="line"><span class="comment">*/</span></div><div class="line">void forward_dropout_layer(dropout_layer <span class="keyword">l</span>, network <span class="keyword">net</span>)</div><div class="line">&#123;</div><div class="line">    int i;</div><div class="line">    <span class="comment">// 如果当前网络不是处于训练阶段而处于测试阶段，则直接返回（使用inverted dropout带来的方便）</span></div><div class="line">    <span class="keyword">if</span> (!<span class="keyword">net</span>.train) <span class="keyword">return</span>;</div><div class="line"></div><div class="line">    <span class="comment">// 遍历dropout层的每一个输入元素（包含整个batch的），按照指定的概率l.probability置为0或者按l.scale缩放</span></div><div class="line">    <span class="keyword">for</span>(i = 0; i &lt; <span class="keyword">l</span>.batch * <span class="keyword">l</span>.inputs; ++i)&#123;</div><div class="line">        <span class="comment">// 产生一个0~1之间均匀分布的随机数</span></div><div class="line">        float r = rand_uniform(0, 1);</div><div class="line"></div><div class="line">        <span class="comment">// 每个输入元素都对应一个随机数，保存在l.rand中</span></div><div class="line">        <span class="keyword">l</span>.rand[i] = r;</div><div class="line"></div><div class="line">        <span class="comment">// 如果r小于l.probability（l.probability是舍弃概率），则舍弃该输入元素，注意，舍弃并不是删除，</span></div><div class="line">        <span class="comment">// 而是将其值置为0,所以输入元素个数总数没变（因故输出元素个数l.outputs等于l.inputs）</span></div><div class="line">        <span class="keyword">if</span>(r &lt; <span class="keyword">l</span>.probability) <span class="keyword">net</span>.<span class="keyword">input</span>[i] = 0;</div><div class="line">        <span class="comment">// 否则保留该输入元素，并乘以比例因子</span></div><div class="line">        <span class="keyword">else</span> <span class="keyword">net</span>.<span class="keyword">input</span>[i] *= <span class="keyword">l</span>.scale;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="5-2-反向传播"><a href="#5-2-反向传播" class="headerlink" title="5.2 反向传播"></a>5.2 反向传播</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** dropout层反向传播函数</span></div><div class="line"><span class="comment">** 输入： l    当前dropout层网络</span></div><div class="line"><span class="comment">**       net  整个网络</span></div><div class="line"><span class="comment">** 说明：dropout层的反向传播相对简单，因为其本身没有训练参数，也没有激活函数，或者说激活函数就为f(x) = x，也</span></div><div class="line"><span class="comment">**      也就是激活函数关于加权输入的导数值为1,因此其自身的敏感度值已经由其下一层网络反向传播时计算完了，</span></div><div class="line"><span class="comment">**      没有必要再乘以激活函数关于加权输入的导数了。剩下要做的就是计算上一层的敏感度图net.delta，这个计算也很简单，详见下面注释。</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">backward_dropout_layer</span><span class="params">(dropout_layer l, network net)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="comment">// 如果进入backward_dropout_layer()函数，那没得说，一定是训练阶段，因为测试阶段压根就没有反向过程，只有前向过程，</span></div><div class="line">    <span class="comment">// 所以就不再需要像forward_dropout_layer()函数中一样判断net.train是不是处于训练阶段了</span></div><div class="line">    <span class="keyword">int</span> i;</div><div class="line"></div><div class="line">    <span class="comment">// 如果net.delta为空，则返回（net.delta为空则说明已经反向到第一层了，此处所指第一层，是net.layers[0]，</span></div><div class="line">    <span class="comment">// 也是与输入层直接相连的第一层隐含层，详细参见：network.c中的forward_network()函数）</span></div><div class="line">    <span class="keyword">if</span>(!net.delta) <span class="keyword">return</span>;</div><div class="line"></div><div class="line">    <span class="comment">// 因为dropout层的输入输出元素个数相等，所以dropout层的敏感度图的维度就为l.batch*l.inputs（每一层的敏感度值与该层的输出维度一致），</span></div><div class="line">    <span class="comment">// 以下循环遍历当前层的敏感度图，并根据l.rand的指示反向计算上一层的敏感度值，由于当前dropout层与上一层之间的连接没有权重，</span></div><div class="line">    <span class="comment">// 或者说连接权重为0（对于舍弃的输入）或固定的l.scale（保留的输入，这个比例因子是固定的，不需要训练），所以计算过程比较简单，</span></div><div class="line">    <span class="comment">// 只需让保留输入对应输出的敏感度值乘以l.scale，其他输入（输入是针对当前dropout层而言，实际为上一层的输出）的敏感度值直接置为0即可</span></div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; l.batch * l.inputs; ++i)&#123;</div><div class="line">        <span class="keyword">float</span> r = l.rand[i];</div><div class="line">        <span class="comment">// 与前向过程forward_dropout_layer照应，根据l.rand指示，如果r小于l.probability，说明是舍弃的输入，其敏感度值为0；</span></div><div class="line">        <span class="comment">// 反之是保留下来的输入元素，其敏感度值为当前层对应输出的敏感度值乘以l.scale</span></div><div class="line">        <span class="keyword">if</span>(r &lt; l.probability) net.delta[i] = <span class="number">0</span>;</div><div class="line">        <span class="keyword">else</span> net.delta[i] *= l.scale;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="6-Maxpool层"><a href="#6-Maxpool层" class="headerlink" title="6. Maxpool层"></a>6. Maxpool层</h2><h3 id="6-1-前向传播"><a href="#6-1-前向传播" class="headerlink" title="6.1 前向传播"></a>6.1 前向传播</h3><figure class="highlight glsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 最大池化层前向传播函数：计算l层的输出</span></div><div class="line"><span class="comment">** 输入： l    当前层（最大池化层）</span></div><div class="line"><span class="comment">**       net  整个网络结构</span></div><div class="line"><span class="comment">** 说明：最大池化层处理图像的方式与卷积层类似，也是将最大池化核在图像平面上按照指定的跨度移动，</span></div><div class="line"><span class="comment">**      并取对应池化核区域中最大元素值为对应输出元素。最大池化层没有训练参数（没有权重以及偏置），</span></div><div class="line"><span class="comment">**      因此，相对与卷积来说，其前向（以及下面的反向）过程比较简单，实现上也是非常直接，不需要什么技巧。</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="type">void</span> forward_maxpool_layer(<span class="keyword">const</span> maxpool_layer l, network net)</div><div class="line">&#123;</div><div class="line">    <span class="type">int</span> b,i,j,k,m,n;</div><div class="line">    <span class="comment">// 初始偏移设定为四周补0长度的负值</span></div><div class="line">    <span class="type">int</span> w_offset = -l.pad;</div><div class="line">    <span class="type">int</span> h_offset = -l.pad;</div><div class="line"></div><div class="line">    <span class="comment">// 获取当前层的输出尺寸</span></div><div class="line">    <span class="type">int</span> h = l.out_h;</div><div class="line">    <span class="type">int</span> w = l.out_w;</div><div class="line"></div><div class="line">    <span class="comment">// 获取当前层输入图像的通道数，为什么是输入通道数？不应该为输出通道数吗？</span></div><div class="line">    <span class="comment">//实际二者没有区别，对于最大池化层来说，输入有多少通道，输出就有多少通道！</span></div><div class="line">    <span class="type">int</span> c = l.c;</div><div class="line"></div><div class="line">    <span class="comment">// 遍历batch中每一张输入图片，计算得到与每一张输入图片具有相同通道数的输出图</span></div><div class="line">    <span class="keyword">for</span>(b = <span class="number">0</span>; b &lt; l.batch; ++b)&#123;</div><div class="line">        <span class="comment">// 对于每张输入图片，将得到通道数一样的输出图，以输出图为基准，按输出图通道，行，列依次遍历</span></div><div class="line">        <span class="comment">// （这对应图像在l.output的存储方式，每张图片按行铺排成一大行，然后图片与图片之间再并成一行）。</span></div><div class="line">        <span class="comment">// 以输出图为基准进行遍历，最终循环的总次数刚好覆盖池化核在输入图片不同位置进行池化操作。</span></div><div class="line">        <span class="keyword">for</span>(k = <span class="number">0</span>; k &lt; c; ++k)&#123;</div><div class="line">            <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; h; ++i)&#123;</div><div class="line">                <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; w; ++j)&#123;</div><div class="line"></div><div class="line">                    <span class="comment">// out_index为输出图中的索引：out_index = b * c * w * h + k * w * h + h * w + w，</span></div><div class="line">                    <span class="comment">//展开写可能更为清晰些</span></div><div class="line">                    <span class="type">int</span> out_index = j + w*(i + h*(k + c*b));</div><div class="line"></div><div class="line">                    <span class="type">float</span> <span class="built_in">max</span> = -FLT_MAX;   </div><div class="line">                    <span class="comment">// FLT_MAX为c语言中float.h定义的对大浮点数，</span></div><div class="line">                    <span class="comment">//此处初始化最大元素值为最小浮点数</span></div><div class="line">					<span class="comment">// 最大元素值的索引初始化为-1</span></div><div class="line"></div><div class="line">                    <span class="comment">// 下面两个循环回到了输入图片，计算得到的cur_h以及cur_w都是在当前层所有输入元素的索引，</span></div><div class="line">                    <span class="comment">//内外循环的目的是找寻输入图像中，</span></div><div class="line">                    <span class="comment">// 以(h_offset + i*l.stride, w_offset + j*l.stride)为左上起点，</span></div><div class="line">                    <span class="comment">//尺寸为l.size池化区域中的最大元素值max及其在所有输入元素中的索引max_i</span></div><div class="line">                    <span class="keyword">for</span>(n = <span class="number">0</span>; n &lt; l.size; ++n)&#123;</div><div class="line">                        <span class="keyword">for</span>(m = <span class="number">0</span>; m &lt; l.size; ++m)&#123;</div><div class="line">                            <span class="comment">// cur_h，cur_w是在所有输入图像中第k通道中的cur_h行与cur_w列，</span></div><div class="line">                            <span class="comment">//index是在所有输入图像元素中的总索引。</span></div><div class="line">                            <span class="comment">// 为什么这里少一层对输入通道数的遍历循环呢？因为对于最大池化层来说</span></div><div class="line">                            <span class="comment">//输入与输出通道数是一样的，并在上面的通道数循环了！</span></div><div class="line">                            <span class="type">int</span> cur_h = h_offset + i*l.stride + n;</div><div class="line">                            <span class="type">int</span> cur_w = w_offset + j*l.stride + m;</div><div class="line">                            <span class="type">int</span> <span class="keyword">index</span> = cur_w + l.w*(cur_h + l.h*(k + b*l.c));</div><div class="line"></div><div class="line">                            <span class="comment">// 边界检查：正常情况下，是不会越界的，但是如果有补0操作，就会越界了，</span></div><div class="line">                            <span class="comment">//这里的处理方式是直接让这些元素值为-FLT_MAX</span></div><div class="line">                            <span class="comment">// （注意虽然称之为补0操作，但实际不是补0），总之，这些补的元素永远不会充当最大元素值。</span></div><div class="line">                            <span class="type">int</span> valid = (cur_h &gt;= <span class="number">0</span> &amp;&amp; cur_h &lt; l.h &amp;&amp;</div><div class="line">                                         cur_w &gt;= <span class="number">0</span> &amp;&amp; cur_w &lt; l.w);</div><div class="line">                            <span class="type">float</span> val = (valid != <span class="number">0</span>) ? net.input[<span class="keyword">index</span>] : -FLT_MAX;</div><div class="line"></div><div class="line">                            <span class="comment">// 记录这个池化区域中的最大的元素值及其在所有输入元素中的总索引</span></div><div class="line">                            max_i = (val &gt; <span class="built_in">max</span>) ? <span class="keyword">index</span> : max_i;</div><div class="line">                            <span class="built_in">max</span>   = (val &gt; <span class="built_in">max</span>) ? val   : <span class="built_in">max</span>;</div><div class="line">                        &#125;</div><div class="line">                    &#125;</div><div class="line">                    <span class="comment">// 由此得到最大池化层每一个输出元素值及其在所有输入元素中的总索引。</span></div><div class="line">                    <span class="comment">// 为什么需要记录每个输出元素值对应在输入元素中的总索引呢？因为在下面的反向过程中需要用到，</span></div><div class="line">                    <span class="comment">//在计算当前最大池化层上一层网络的敏感度时，</span></div><div class="line">                    <span class="comment">// 需要该索引明确当前层的每个元素究竟是取上一层输出（也即上前层输入）的哪一个元素的值，</span></div><div class="line">                    <span class="comment">//具体见下面backward_maxpool_layer()函数的注释。</span></div><div class="line">                    l.output[out_index] = <span class="built_in">max</span>;</div><div class="line">                    l.indexes[out_index] = max_i;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="6-2-反向传播"><a href="#6-2-反向传播" class="headerlink" title="6.2 反向传播"></a>6.2 反向传播</h3><figure class="highlight glsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 最大池化层反向传播传播函数</span></div><div class="line"><span class="comment">** 输入： l     当前最大池化层</span></div><div class="line"><span class="comment">**       net   整个网络</span></div><div class="line"><span class="comment">** 说明：这个函数看上去很简单，比起backward_convolutional_layer()少了很多，这都是有原因的。实际上，在darknet中，不管是什么层，</span></div><div class="line"><span class="comment">**      其反向传播函数都会先后做两件事：</span></div><div class="line"><span class="comment">**      1）计算当前层的敏感度图l.delta、权重更新值以及偏置更新值；</span></div><div class="line"><span class="comment">**      2）计算上一层的敏感度图net.delta（部分计算，要完成计算得等到真正到了这一层再说）。</span></div><div class="line"><span class="comment">**      而这里，显然没有第一步，只有第二步，而且很简单，这是为什么呢？</span></div><div class="line"><span class="comment">**      首先回答为什么没有第一步。注意当前层l是最大池化层，最大池化层没有训练参数，</span></div><div class="line"><span class="comment">**      说的再直白一点就是没有激活函数，或者认为激活函数就是f(x)=x，所以激活函数对于加权输入的导数其实就是1,</span></div><div class="line"><span class="comment">**      正如在backward_convolutional_layer()注释的那样，每一层的反向传播函数的第一步是将之前</span></div><div class="line"><span class="comment">**      （就是下一层计算得到的，注意过程是反向的）未计算完得到的l.delta乘以激活函数对加权输入的导数，</span></div><div class="line"><span class="comment">**      以最终得到当前层的敏感度图，而对于最大池化层来说，每一个输出对于加权输入的导数值都是1,</span></div><div class="line"><span class="comment">**      同时并没有权重及偏置这些需要训练的参数，自然不再需要第一步；对于第二步为什么会如此简单，可以参考：</span></div><div class="line"><span class="comment">**      https://www.zybuluo.com/hanbingtao/note/485480，最大池化层它就是这么简单，剩下的参考下面的注释。</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="type">void</span> backward_maxpool_layer(<span class="keyword">const</span> maxpool_layer l, network net)</div><div class="line">&#123;</div><div class="line">    <span class="type">int</span> i;</div><div class="line">    <span class="comment">// 获取当前最大池化层l的输出尺寸h,w</span></div><div class="line">    <span class="type">int</span> h = l.out_h;</div><div class="line">    <span class="type">int</span> w = l.out_w;</div><div class="line"></div><div class="line">    <span class="comment">// 获取当前层输入的通道数，为什么是输入通道数？不应该为输出通道数吗？实际二者没有区别，</span></div><div class="line">    <span class="comment">//对于最大池化层来说，输入有多少通道，输出就有多少通道！</span></div><div class="line">    <span class="type">int</span> c = l.c;</div><div class="line"></div><div class="line">    <span class="comment">// 计算上一层的敏感度图（未计算完全，还差一个环节，这个环节等真正反向到了那层再执行）</span></div><div class="line">    <span class="comment">// 这个循环很有意思，循环总次数为当前层输出总元素个数（包含所有输入图片的输出，</span></div><div class="line">    <span class="comment">//即维度为l.out_h * l.out_w * l.c * l.batch，注意此处l.c==l.out_c）,</span></div><div class="line">    <span class="comment">// 而不是上一层输出总元素个数，为什么呢？是因为对于最大池化层而言，</span></div><div class="line">    <span class="comment">//其每个输出元素对仅受上一层输出对应池化核区域中最大值元素的影响，所以当前池化层每个输出元素</span></div><div class="line">    <span class="comment">// 对于上一层输出中的很多元素的导数值为0,而对最大值元素，其导数值为1；再乘以当前层的敏感度图，</span></div><div class="line">    <span class="comment">//导数值为0的还是为0,导数值为1则就等于当前层的敏感度值。</span></div><div class="line">    <span class="comment">// 以输出图总元素个数进行遍历，刚好可以找出上一层输出中所有真正起作用（在某个池化区域中充当了最大元素值）</span></div><div class="line">    <span class="comment">//也即敏感度值不为0的元素，而那些没有起作用的元素，</span></div><div class="line">    <span class="comment">// 可以不用理会，保持其初始值0就可以了。</span></div><div class="line">    <span class="comment">// 详细原理推导可以参见：https://www.zybuluo.com/hanbingtao/note/485480</span></div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; h*w*c*l.batch; ++i)&#123;</div><div class="line">        <span class="comment">// 遍历的基准是以当前层的输出元素为基准的，l.indexes记录了当前层每一个输出元素与</span></div><div class="line">        <span class="comment">//上一层哪一个输出元素有真正联系（也即上一层对应池化核区域中最大值元素的索引），</span></div><div class="line">        <span class="comment">// 所以index是上一层中所有输出元素的索引，且该元素在当前层某个池化域中充当了最大值元素，</span></div><div class="line">        <span class="comment">//这个元素的敏感度值将直接传承当前层对应元素的敏感度值。</span></div><div class="line">        <span class="comment">// 而net.delta中，剩下没有被index按索引访问到的元素，就是那些没有真正起到作用的元素，</span></div><div class="line">        <span class="comment">//这些元素的敏感度值为0（net.delta已经在前向时将所有元素值初始化为0）</span></div><div class="line">        <span class="comment">// 至于为什么要用+=运算符，原因有两个，和卷积类似：一是池化核由于跨度较小，导致有重叠区域；</span></div><div class="line">        <span class="comment">//二是batch中有多张图片，需要将所有图片的影响加起来。</span></div><div class="line">        <span class="type">int</span> <span class="keyword">index</span> = l.indexes[i];</div><div class="line">        net.delta[<span class="keyword">index</span>] += l.delta[i];</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="7-RNN"><a href="#7-RNN" class="headerlink" title="7. RNN"></a>7. RNN</h2><p>darknet中的RNN是vanilla RNN，RNN层本质上是三个全连接层构成的，具体结构可以参考 <a href="https://pjreddie.com/darknet/rnns-in-darknet/" target="_blank" rel="external">https://pjreddie.com/darknet/rnns-in-darknet/</a></p>
<h3 id="7-1-前向传播层"><a href="#7-1-前向传播层" class="headerlink" title="7.1 前向传播层"></a>7.1 前向传播层</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** RNN层前向传播</span></div><div class="line"><span class="comment">** 输入    l      当前的RNN层</span></div><div class="line"><span class="comment">**        net     当前网络</span></div><div class="line"><span class="comment">**</span></div><div class="line"><span class="comment">** RNN层前向传播与其他网络不同，RNN中的全连接层的当前状态与上一个时间的状态有关，</span></div><div class="line"><span class="comment">** 所以要在每次传播后记录上一个时刻的状态</span></div><div class="line"><span class="comment">*/</span></div><div class="line"></div><div class="line">void forward_rnn_layer(layer l, network net)</div><div class="line">&#123;</div><div class="line">    network s = net;</div><div class="line">    s.train = net.train;</div><div class="line">    int i;</div><div class="line">    layer input_layer = *(l.input_layer);</div><div class="line">    layer self_layer = *(l.self_layer);</div><div class="line">    layer output_layer = *(l.output_layer);</div><div class="line"></div><div class="line">    <span class="comment">/* 开始训练前要将三个全连接层的错误值设置为0 */</span></div><div class="line">    fill_cpu(l.outputs * l.batch * l.steps, <span class="number">0</span>, output_layer.delta, <span class="number">1</span>);</div><div class="line">    fill_cpu(l.hidden * l.batch * l.steps, <span class="number">0</span>, self_layer.delta, <span class="number">1</span>);</div><div class="line">    fill_cpu(l.hidden * l.batch * l.steps, <span class="number">0</span>, input_layer.delta, <span class="number">1</span>);</div><div class="line">    <span class="comment">/* 如果网络处于训练状态，要将state设置为0，因为初始状态的上一时刻状态是不存在的，我们只能假设它存在，并把它赋值为0 */</span></div><div class="line">    if(net.train) fill_cpu(l.hidden * l.batch, <span class="number">0</span>, l.<span class="section">state</span>, <span class="number">1</span>);</div><div class="line"></div><div class="line">    <span class="comment">/*</span></div><div class="line"><span class="comment">    ** 以下是RNN层前向传播的主要过程，该层总共要传播steps次，每次的输入batch个字符。</span></div><div class="line"><span class="comment">    ** Vanilla RNN具体结构参考 https://pjreddie.com/darknet/rnns-in-darknet/，</span></div><div class="line"><span class="comment">    ** 这里我只简单的说明一下。</span></div><div class="line"><span class="comment">    ** Vanilla RNN的RNN层虽然包含三个全连接层，但是只有中间一层（也就是self_layer)与传统的RNN的隐含层一致。</span></div><div class="line"><span class="comment">    **</span></div><div class="line"><span class="comment">    ** 第一层input_layer可以理解为embedding层，它将input编码为一个hidden维的向量，</span></div><div class="line"><span class="comment">    ** 以darknet的字符预测问题为例，网络的input是英文字母</span></div><div class="line"><span class="comment">    ** 采用one-hot编码，是一个256维的向量，embedding后成为了hidden维的向量。</span></div><div class="line"><span class="comment">    **</span></div><div class="line"><span class="comment">    ** 第二层self_layer与普通RNN的隐含层功能相同，它接收输入层和上一时刻的状态作为输入。</span></div><div class="line"><span class="comment">    **</span></div><div class="line"><span class="comment">    ** 第三层output_layer，接收self_layer的输出为输入，需要注意的是这一层的输出并不是最终结果，</span></div><div class="line"><span class="comment">    ** 还需要做进一步处理，还是以darknet的字符预测为例，第三层的输出要进一步转化为一个256维的向量，</span></div><div class="line"><span class="comment">    ** 然后进行归一化，找到概率最大的字符作为预测结果</span></div><div class="line"><span class="comment">    */</span></div><div class="line">    </div><div class="line">    for (i = <span class="number">0</span>; i &lt; l.steps; ++i) &#123;</div><div class="line">        s.input = net.input;</div><div class="line">        forward_connected_layer(input_layer, s);</div><div class="line"></div><div class="line">        s.input = l.<span class="section">state</span>;</div><div class="line">        forward_connected_layer(self_layer, s);</div><div class="line"></div><div class="line">        <span class="type">float</span> *old_state = l.<span class="section">state</span>;                </div><div class="line">         <span class="comment">// 将当前状态存入上一时刻状态</span></div><div class="line">        if(net.train) l.<span class="section">state</span> += l.hidden*l.batch;  </div><div class="line">        <span class="comment">// 如果网络处于训练状态，注意的是上一时刻的状态包含一个batch</span></div><div class="line">        <span class="comment">// 如何设置当前状态，由shortcut的值决定</span></div><div class="line">            copy_cpu(l.hidden * l.batch, old_state, <span class="number">1</span>, l.<span class="section">state</span>, <span class="number">1</span>);</div><div class="line">        &#125;else&#123;</div><div class="line">            fill_cpu(l.hidden * l.batch, <span class="number">0</span>, l.<span class="section">state</span>, <span class="number">1</span>);</div><div class="line">        &#125;</div><div class="line">        axpy_cpu(l.hidden * l.batch, <span class="number">1</span>, input_layer.output, <span class="number">1</span>, l.<span class="section">state</span>, <span class="number">1</span>);</div><div class="line">        axpy_cpu(l.hidden * l.batch, <span class="number">1</span>, self_layer.output, <span class="number">1</span>, l.<span class="section">state</span>, <span class="number">1</span>);</div><div class="line"></div><div class="line">        s.input = l.<span class="section">state</span>;</div><div class="line">        forward_connected_layer(output_layer, s);</div><div class="line"></div><div class="line">        <span class="comment">/* 一次传播结束，将三个层同时向前推移一步 */</span></div><div class="line">        net.input += l.inputs*l.batch;</div><div class="line">        increment_layer(&amp;input_layer, <span class="number">1</span>);</div><div class="line">        increment_layer(&amp;self_layer, <span class="number">1</span>);</div><div class="line">        increment_layer(&amp;output_layer, <span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="7-1-后向传播层"><a href="#7-1-后向传播层" class="headerlink" title="7.1 后向传播层"></a>7.1 后向传播层</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 误差后向传播</span></div><div class="line"><span class="comment">** 输入     l       当前RNN层</span></div><div class="line"><span class="comment">**          net     当前网络</span></div><div class="line"><span class="comment">*/</span></div><div class="line">void <span class="keyword">backward_rnn_layer(layer </span>l, network net)</div><div class="line">&#123;</div><div class="line">    network s = net<span class="comment">;</span></div><div class="line">    s.train = net.train<span class="comment">;</span></div><div class="line">    int i<span class="comment">;</span></div><div class="line">    layer input_layer = *(l.input_layer)<span class="comment">;</span></div><div class="line">    layer self_layer = *(l.self_layer)<span class="comment">;</span></div><div class="line">    layer output_layer = *(l.output_layer)<span class="comment">;</span></div><div class="line"></div><div class="line">    <span class="comment">/* 误差传播从网络中最后一步开始 */</span></div><div class="line">    increment_layer(&amp;input_layer, l.steps-1)<span class="comment">;</span></div><div class="line">    increment_layer(&amp;self_layer, l.steps-1)<span class="comment">;</span></div><div class="line">    increment_layer(&amp;output_layer, l.steps-1)<span class="comment">;</span></div><div class="line"></div><div class="line">    l.state += l.hidden*l.<span class="keyword">batch*l.steps;</span></div><div class="line"><span class="keyword"> </span>   for (i = l.steps-1<span class="comment">; i &gt;= 0; --i) &#123;</span></div><div class="line">        copy_cpu(l.hidden * l.<span class="keyword">batch, </span>input_layer.output, <span class="number">1</span>, l.state, <span class="number">1</span>)<span class="comment">;</span></div><div class="line">        axpy_cpu(l.hidden * l.<span class="keyword">batch, </span><span class="number">1</span>, self_layer.output, <span class="number">1</span>, l.state, <span class="number">1</span>)<span class="comment">;</span></div><div class="line"></div><div class="line">        <span class="comment">/* 计算output_layer层的误差 */</span></div><div class="line">        s.input = l.state<span class="comment">;</span></div><div class="line">        s.delta = self_layer.delta<span class="comment">;</span></div><div class="line">        <span class="keyword">backward_connected_layer(output_layer, </span>s)<span class="comment">;</span></div><div class="line"></div><div class="line">        l.state -= l.hidden*l.<span class="keyword">batch;</span></div><div class="line"><span class="keyword"> </span>       <span class="comment">/*</span></div><div class="line"><span class="comment">           if(i &gt; 0)&#123;</span></div><div class="line"><span class="comment">           copy_cpu(l.hidden * l.batch, input_layer.output - l.hidden*l.batch, 1, l.state, 1);</span></div><div class="line"><span class="comment">           axpy_cpu(l.hidden * l.batch, 1, self_layer.output - l.hidden*l.batch, 1, l.state, 1);</span></div><div class="line"><span class="comment">           &#125;else&#123;</span></div><div class="line"><span class="comment">           fill_cpu(l.hidden * l.batch, 0, l.state, 1);</span></div><div class="line"><span class="comment">           &#125;</span></div><div class="line"><span class="comment">         */</span></div><div class="line"></div><div class="line">        <span class="comment">/* 计算self_layer层的误差 */</span></div><div class="line">        s.input = l.state<span class="comment">;</span></div><div class="line">        s.delta = self_layer.delta - l.hidden*l.<span class="keyword">batch;</span></div><div class="line"><span class="keyword"> </span>       if (i == <span class="number">0</span>) s.delta = <span class="number">0</span><span class="comment">;</span></div><div class="line">        <span class="keyword">backward_connected_layer(self_layer, </span>s)<span class="comment">;</span></div><div class="line"></div><div class="line">        copy_cpu(l.hidden*l.<span class="keyword">batch, </span>self_layer.delta, <span class="number">1</span>, input_layer.delta, <span class="number">1</span>)<span class="comment">;</span></div><div class="line">        if (i &gt; <span class="number">0</span> &amp;&amp; l.<span class="keyword">shortcut) </span>axpy_cpu(l.hidden*l.<span class="keyword">batch, </span><span class="number">1</span>, self_layer.delta,</div><div class="line">         <span class="number">1</span>, self_layer.delta -l.hidden*l.<span class="keyword">batch, </span><span class="number">1</span>)<span class="comment">;</span></div><div class="line">        s.input = net.input + i*l.inputs*l.<span class="keyword">batch;</span></div><div class="line"><span class="keyword"> </span>       if(net.delta) s.delta = net.delta + i*l.inputs*l.<span class="keyword">batch;</span></div><div class="line"><span class="keyword"> </span>       else s.delta = <span class="number">0</span><span class="comment">;</span></div><div class="line">        <span class="comment">/* 计算input_layer层的误差 */</span></div><div class="line">        <span class="keyword">backward_connected_layer(input_layer, </span>s)<span class="comment">;</span></div><div class="line"></div><div class="line">        <span class="comment">/* 误差传播一步之后，需要重新调整各个连接层， 向后移动一步 */</span></div><div class="line">        increment_layer(&amp;input_layer, -<span class="number">1</span>)<span class="comment">;</span></div><div class="line">        increment_layer(&amp;self_layer, -<span class="number">1</span>)<span class="comment">;</span></div><div class="line">        increment_layer(&amp;output_layer, -<span class="number">1</span>)<span class="comment">;</span></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>工程地址：</strong><br><a href="https://github.com/hgpvision/darknet" target="_blank" rel="external">https://github.com/hgpvision/darknet</a></p>
]]></content>
      
        <categories>
            
            <category> Deep Learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CS231n学习笔记--Assignment2/3]]></title>
      <url>/2017/10/27/CS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--Assignment2&amp;3/</url>
      <content type="html"><![CDATA[<h2 id="1-Assignment2"><a href="#1-Assignment2" class="headerlink" title="1. Assignment2"></a>1. Assignment2</h2><h3 id="1-1-全连接神经网络"><a href="#1-1-全连接神经网络" class="headerlink" title="1.1 全连接神经网络"></a>1.1 全连接神经网络</h3><p><a href="http://blog.csdn.net/margretwg/article/details/70761543" target="_blank" rel="external">深度学习小白——CS231n Assignment2(FC)</a></p>
<p><a href="http://blog.csdn.net/l691899397/article/details/52291909" target="_blank" rel="external"> 深度学习笔记8：softmax层的实现</a></p>
<h3 id="1-2-卷积神经网络"><a href="#1-2-卷积神经网络" class="headerlink" title="1.2 卷积神经网络"></a>1.2 卷积神经网络</h3><p><a href="http://blog.csdn.net/margretwg/article/details/70761543" target="_blank" rel="external">深度学习小白——CS231n Assignment2（CNN）</a></p>
<p><strong>pooling mean max 前向和反向传播</strong></p>
<p>对于mean pooling，真的是好简单：假设pooling的窗大小是2x2, 在forward的时候啊，就是在前面卷积完的输出上依次不重合的取2x2的窗平均，得到一个值就是当前mean pooling之后的值。backward的时候，把一个值分成四等分放到前面2x2的格子里面就好了。如下</p>
<p>forward: [1 3; 2 2] -&gt; [2]<br>backward: [2] -&gt; [0.5 0.5; 0.5 0.5]</p>
<p>max pooling就稍微复杂一点，forward的时候你只需要把2x2窗子里面那个最大的拿走就好了，backward的时候你要把当前的值放到之前那个最大的位置，其他的三个位置都弄成0。如下</p>
<p>forward: [1 3; 2 2] -&gt; 3<br>backward: [3] -&gt; [0 3; 0 0]</p>
<h2 id="2-Assignment3"><a href="#2-Assignment3" class="headerlink" title="2. Assignment3"></a>2. Assignment3</h2><p><a href="https://zhuanlan.zhihu.com/p/25356438" target="_blank" rel="external">CS231n Assignment3</a></p>
<p><a href="http://www.jianshu.com/p/e46b1aa48886" target="_blank" rel="external">CS231n (winter 2016) : Assignment3</a></p>
]]></content>
      
        <categories>
            
            <category> CS231n </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CS231n学习笔记--16. Adversarial Examples and Adversarial Training]]></title>
      <url>/2017/10/23/CS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--16.%20Adversarial%20Examples%20and%20Adversarial%20Training/</url>
      <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ul>
<li>What are adversarial examples?</li>
<li>Why do they happen?</li>
<li>How can they be used to compromise machine learning<br>systems?</li>
<li>What are the defenses?</li>
<li>How to use adversarial examples to improve machine<br>learning, even when there is no adversary</li>
</ul>
<h2 id="1-Adversarial-Examples"><a href="#1-Adversarial-Examples" class="headerlink" title="1. Adversarial Examples"></a>1. Adversarial Examples</h2><p>Fool  neural nets from Panda to Gibbon</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111180523437?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"> <div align="left"></div></div></p>
<p><strong>Turning Objects into “Airplanes”</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111180752444?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>Attacking a Linear Model</strong></p>
<p>黄框内的数字被神经网络误识别！</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111180834679?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"> <div align="left"></div></div></p>
<p>以下类型的分类器也存在这样的问题：</p>
<ul>
<li>Linear models：Logistic regression，Softmax regression，SVMs</li>
<li>Decision trees</li>
<li>Nearest neighbors</li>
</ul>
<h2 id="2-Reason"><a href="#2-Reason" class="headerlink" title="2. Reason"></a>2. Reason</h2><p><strong>原因猜测：Adversarial Examples from Overfitting</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111181230937?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%"> <div align="left"></div></div></p>
<p><strong>Adversarial Examples from Excessive Linearity：</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111181327718?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%"> <div align="left"></div></div></p>
<p><strong>Modern deep nets are very piecewise linear</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111181544790?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"> <div align="left"></div></div></p>
<p><strong>Small inter-class distances</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111181714777?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>High-Dimensional Linear Models</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111182440932?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>Linear Models of ImageNet</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111182516598?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<h2 id="3-compromise-machine-learning-systems"><a href="#3-compromise-machine-learning-systems" class="headerlink" title="3. compromise machine learning systems"></a>3. compromise machine learning systems</h2><p><strong>Cross-model, cross-dataset generalization</strong></p>
<p>不同的模型使用同样的数据产生的权重几乎相同！</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111182736670?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>Cross-technique transferability</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111182824016?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>Transferability Attack</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111183029038?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>Cross-Training Data Transferability</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111183135444?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>Adversarial Examples in the Human Brain</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111183348309?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%"> <div align="left"></div></div></p>
<p><strong>Practical Attacks</strong></p>
<ul>
<li><p>Fool real classifiers trained by remotely hosted API(MetaMind, Amazon, Google)</p>
</li>
<li><p>Fool malware detector networks</p>
</li>
<li><p>Display adversarial examples in the physical world and fool machine learning systems that perceive them through a camera</p>
</li>
</ul>
<p><strong>Failed defenses</strong></p>
<p>以下方法均解决不了：</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111183555975?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<h2 id="4-Use-adversarial-examples"><a href="#4-Use-adversarial-examples" class="headerlink" title="4. Use adversarial examples"></a>4. Use adversarial examples</h2><p>Training on Adversarial Examples</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111183812727?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>Adversarial Training of other Models</strong></p>
<ul>
<li><p>Linear models: SVM / linear regression cannot learn a step function, so adversarial training is less useful, very similar to weight decay</p>
</li>
<li><p>k-NN: adversarial training is prone to overfitting.</p>
</li>
<li><p>Takeway: neural nets can actually become more secure than other models. Adversarially trained neural nets have the best empirical success rate on adversarial examples of any machine learning model.</p>
</li>
</ul>
<p><strong>Adversarial Training</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111184122821?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>Virtual Adversarial Training</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111184234221?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>Text Classification with VAT</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111184359347?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171111184529095?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul>
<li><p>Attacking is easy</p>
</li>
<li><p>Defending is difficult</p>
</li>
<li><p>Adversarial training provides regularization and semi-supervised learning</p>
</li>
<li><p>The out-of-domain input problem is a bottleneck for model-based optimization generally</p>
</li>
</ul>
]]></content>
      
        <categories>
            
            <category> CS231n </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[YOLO算法学习及训练]]></title>
      <url>/2017/10/20/YOLO%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E5%8F%8A%E8%AE%AD%E7%BB%83/</url>
      <content type="html"><![CDATA[<h2 id="1-YOLO2代码"><a href="#1-YOLO2代码" class="headerlink" title="1. YOLO2代码"></a>1. YOLO2代码</h2><p>在window下的训练代码：<br><a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="external">https://github.com/AlexeyAB/darknet</a></p>
<p>原始代码：<br><a href="https://pjreddie.com/darknet/" target="_blank" rel="external">https://pjreddie.com/darknet/</a></p>
<p><strong>Tips:</strong></p>
<ol>
<li>虽然要求OPENCV版本为2.4.13或2.4.3以上，VS2015，但实际上改一下代码中opencv和VS的配置信息，低版本也可以，本人版本opencv2.4.10 + VS2013。</li>
<li>VS运行代码时会出现大量的找不到定义错误，这是因为C和C++代码风格差距导致的，本代码为C风格，而在C风格下需要把变量的声明反正其生命周期开始的地方！</li>
</ol>
<h2 id="2-YOLO网络参数"><a href="#2-YOLO网络参数" class="headerlink" title="2. YOLO网络参数"></a>2. YOLO网络参数</h2><p><strong>batch:</strong> </p>
<p>每一次迭代送到网络的图片数量，也叫批数量。增大这个可以让网络在较少的迭代次数内完成一个epoch。在固定最大迭代次数的前提下，增加batch会延长训练时间，但会更好的寻找到梯度下降的方向。如果你显存够大，可以适当增大这个值来提高内存利用率。这个值是需要大家不断尝试选取的，过小的话会让训练不够收敛，过大会陷入局部最优。</p>
<p><strong>subdivision：</strong></p>
<p>这个参数很有意思的，它会让你的每一个batch不是一下子都丢到网络里。而是分成subdivision对应数字的份数，一份一份的跑完后，在一起打包算作完成一次iteration。这样会降低对显存的占用情况。如果设置这个参数为1的话就是一次性把所有batch的图片都丢到网络里，如果为2的话就是一次丢一半。</p>
<p><strong>angle：</strong></p>
<p>图片旋转角度，这个用来增强训练效果的。从本质上来说，就是通过旋转图片来变相的增加训练样本集。</p>
<p><strong>saturation，exposure，hue：</strong></p>
<p>饱和度，曝光度，色调，这些都是为了增强训练效果用的。</p>
<p><strong>learning_rate：</strong></p>
<p>学习率，训练发散的话可以降低学习率。学习遇到瓶颈，loss不变的话也减低学习率。</p>
<p><strong>max_batches：</strong></p>
<p> 最大迭代次数。</p>
<p>  <strong>policy：</strong></p>
<p>学习策略，可以设置成以下方式：</p>
<ul>
<li>fixed:　　 保持base_lr不变.</li>
<li>step: 　　 如果设置为step,则还需要设置一个stepsize,  返回 base_lr * gamma ^ (floor(iter / stepsize)),其中iter表示当前的迭代次数</li>
<li>exp:   　　返回base_lr * gamma ^ iter， iter为当前迭代次数</li>
<li>inv:　　    如果设置为inv,还需要设置一个power, 返回base_lr <em> (1 + gamma </em> iter) ^ (- power)</li>
<li>multistep: 如果设置为multistep,则还需要设置一个stepvalue。这个参数和step很相似，step是均匀等间隔变化，而multistep则是根据                                 stepvalue值变化</li>
<li>poly: 　　  学习率进行多项式误差, 返回 base_lr (1 - iter/max_iter) ^ (power)</li>
<li>sigmoid:　学习率进行sigmod衰减，返回 base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))</li>
</ul>
<p><strong>step，scales：</strong></p>
<p>这两个是组合一起的，举个例子：learn_rate: 0.001, step:100,25000,35000   scales: 10, .1, .1 这组数据的意思就是在0-100次iteration期间learning rate为原始0.001，在100-25000次iteration期间learning rate为原始的10倍0.01，在25000-35000次iteration期间learning rate为当前值的0.1倍，就是0.001， 在35000到最大iteration期间使用learning rate为当前值的0.1倍，就是0.0001。随着iteration增加，降低学习率可以是模型更有效的学习，也就是更好的降低train loss。</p>
<p>最后一层卷积层中filters数值是 5×（类别数 + 1*5）。具体原因就不多说了，知道就好哈。<br>region里需要把classes改成你的类别数。</p>
<p><strong>random</strong></p>
<p>如果设置为1的话，就是在训练的时候每一batch图片会随便改成320-640（32整倍数）大小的图片。目的和上面的色度，曝光度等一样。如果设置为0的话，所有图片就只修改成默认的大小 416*416。</p>
<p>参考博客：</p>
<p><a href="http://blog.csdn.net/renhanchi/article/details/71077830?locationNum=11&amp;fps=1" target="_blank" rel="external">【Darknet】【yolo v2】训练自己数据集的一些心得—-VOC格式</a></p>
<p><strong>网络参数文件cfg解析：</strong></p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div></pre></td><td class="code"><pre><div class="line">[net]</div><div class="line"><span class="attribute">batch</span>=64                           每batch个样本更新一次参数。</div><div class="line"><span class="attribute">subdivisions</span>=8                     如果内存不够大，将batch分割为subdivisions个子batch，每个子batch的大小为batch/subdivisions。</div><div class="line">                                   在darknet代码中，会将batch/subdivisions命名为batch。</div><div class="line"><span class="attribute">height</span>=416                         input图像的高</div><div class="line"><span class="attribute">width</span>=416                          Input图像的宽</div><div class="line"><span class="attribute">channels</span>=3                         Input图像的通道数</div><div class="line"><span class="attribute">momentum</span>=0.9                       动量</div><div class="line"><span class="attribute">decay</span>=0.0005                       权重衰减正则项，防止过拟合</div><div class="line"><span class="attribute">angle</span>=0                            通过旋转角度来生成更多训练样本</div><div class="line">saturation = 1.5                   通过调整饱和度来生成更多训练样本</div><div class="line">exposure = 1.5                     通过调整曝光量来生成更多训练样本</div><div class="line"><span class="attribute">hue</span>=.1                             通过调整色调来生成更多训练样本</div><div class="line"></div><div class="line"><span class="attribute">learning_rate</span>=0.0001               初始学习率</div><div class="line">max_batches = 45000                训练达到max_batches后停止学习</div><div class="line"><span class="attribute">policy</span>=steps                       调整学习率的policy，有如下policy：CONSTANT, <span class="keyword">STEP</span>, EXP, POLY, STEPS, SIG, RANDOM</div><div class="line"><span class="attribute">steps</span>=100,25000,35000              根据batch_num调整学习率</div><div class="line"><span class="attribute">scales</span>=10,.1,.1                    学习率变化的比例，累计相乘</div><div class="line"></div><div class="line">[convolutional]</div><div class="line"><span class="attribute">batch_normalize</span>=1                  是否做BN</div><div class="line"><span class="attribute">filters</span>=32                         输出多少个特征图</div><div class="line"><span class="attribute">size</span>=3                             卷积核的尺寸</div><div class="line"><span class="attribute">stride</span>=1                           做卷积运算的步长</div><div class="line"><span class="attribute">pad</span>=1                              如果pad为0,padding由 padding参数指定。如果pad为1，padding大小为size/2</div><div class="line"><span class="attribute">activation</span>=leaky                   激活函数：</div><div class="line">                                   logistic，loggy，relu，elu，relie，plse，hardtan，lhtan，linear，ramp，leaky，tanh，stair</div><div class="line"></div><div class="line">[maxpool]</div><div class="line"><span class="attribute">size</span>=2                             池化层尺寸</div><div class="line"><span class="attribute">stride</span>=2                           池化步进</div><div class="line"></div><div class="line">[convolutional]</div><div class="line"><span class="attribute">batch_normalize</span>=1</div><div class="line"><span class="attribute">filters</span>=64</div><div class="line"><span class="attribute">size</span>=3</div><div class="line"><span class="attribute">stride</span>=1</div><div class="line"><span class="attribute">pad</span>=1</div><div class="line"><span class="attribute">activation</span>=leaky</div><div class="line"></div><div class="line">[maxpool]</div><div class="line"><span class="attribute">size</span>=2</div><div class="line"><span class="attribute">stride</span>=2</div><div class="line"></div><div class="line"><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span></div><div class="line"><span class="built_in">..</span><span class="built_in">..</span><span class="built_in">..</span></div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">#######</span></div><div class="line"></div><div class="line">[convolutional]</div><div class="line"><span class="attribute">batch_normalize</span>=1</div><div class="line"><span class="attribute">size</span>=3</div><div class="line"><span class="attribute">stride</span>=1</div><div class="line"><span class="attribute">pad</span>=1</div><div class="line"><span class="attribute">filters</span>=1024</div><div class="line"><span class="attribute">activation</span>=leaky</div><div class="line"></div><div class="line">[convolutional]</div><div class="line"><span class="attribute">batch_normalize</span>=1</div><div class="line"><span class="attribute">size</span>=3</div><div class="line"><span class="attribute">stride</span>=1</div><div class="line"><span class="attribute">pad</span>=1</div><div class="line"><span class="attribute">filters</span>=1024</div><div class="line"><span class="attribute">activation</span>=leaky</div><div class="line"></div><div class="line">[route]                            the<span class="built_in"> route </span>layer is <span class="keyword">to</span> bring finer grained features <span class="keyword">in</span> <span class="keyword">from</span> earlier <span class="keyword">in</span> the network</div><div class="line"><span class="attribute">layers</span>=-9</div><div class="line"></div><div class="line">[reorg]                            the reorg layer is <span class="keyword">to</span> make these features match the feature map size at the later layer. </div><div class="line">                                   The end feature map is 13x13, the feature map <span class="keyword">from</span> earlier is 26x26x512. </div><div class="line">                                   The reorg layer maps the 26x26x512 feature map onto a 13x13x2048 feature map </div><div class="line">                                   so that it can be concatenated with the feature maps at 13x13 resolution.</div><div class="line"><span class="attribute">stride</span>=2</div><div class="line"></div><div class="line">[route]</div><div class="line"><span class="attribute">layers</span>=-1,-3</div><div class="line"></div><div class="line">[convolutional]</div><div class="line"><span class="attribute">batch_normalize</span>=1</div><div class="line"><span class="attribute">size</span>=3</div><div class="line"><span class="attribute">stride</span>=1</div><div class="line"><span class="attribute">pad</span>=1</div><div class="line"><span class="attribute">filters</span>=1024</div><div class="line"><span class="attribute">activation</span>=leaky</div><div class="line"></div><div class="line">[convolutional]</div><div class="line"><span class="attribute">size</span>=1</div><div class="line"><span class="attribute">stride</span>=1</div><div class="line"><span class="attribute">pad</span>=1</div><div class="line"><span class="attribute">filters</span>=125                        region前最后一个卷积层的filters数是特定的，计算公式为<span class="attribute">filter</span>=num*(classes+5) </div><div class="line">                                   5的意义是5个坐标，论文中的tx,ty,tw,th,<span class="keyword">to</span></div><div class="line"><span class="attribute">activation</span>=linear</div><div class="line"></div><div class="line">[region]</div><div class="line">anchors = 1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52          预选框，可以手工挑选，</div><div class="line">                                                                     也可以通过k means 从训练样本中学出</div><div class="line"><span class="attribute">bias_match</span>=1</div><div class="line"><span class="attribute">classes</span>=20                         网络需要识别的物体种类数</div><div class="line"><span class="attribute">coords</span>=4                           每个box的4个坐标tx,ty,tw,th</div><div class="line"><span class="attribute">num</span>=5                              每个grid cell预测几个box</div><div class="line"><span class="attribute">softmax</span>=1                          使用softmax做激活函数</div><div class="line"><span class="attribute">jitter</span>=.2                          通过抖动增加噪声来抑制过拟合</div><div class="line"><span class="attribute">rescore</span>=1                          暂理解为一个开关，非0时通过重打分来调整l.delta（预测值与真实值的差）</div><div class="line"></div><div class="line"><span class="attribute">object_scale</span>=5                     暂理解为计算损失时预测框中有物体时的权重</div><div class="line"><span class="attribute">noobject_scale</span>=1                   暂理解为计算损失时预测框中无物体时的权重</div><div class="line"><span class="attribute">class_scale</span>=1                      暂理解为计算类别损失时的权重                      </div><div class="line"><span class="attribute">coord_scale</span>=1                      暂理解为计算损失时坐标偏差的权重</div><div class="line"></div><div class="line"><span class="attribute">absolute</span>=1</div><div class="line">thresh = .6</div><div class="line"><span class="attribute">random</span>=0</div></pre></td></tr></table></figure>
<p>具体的参数代码解析，可以参考：</p>
<p><a href="https://xmfbit.github.io/2017/03/06/yolo-cfg-parser/" target="_blank" rel="external">YOLO网络参数的解析与存储</a></p>
<h2 id="3-YOLO的算法原理"><a href="#3-YOLO的算法原理" class="headerlink" title="3. YOLO的算法原理"></a>3. YOLO的算法原理</h2><p><strong>参考博客：</strong></p>
<p><a href="https://xmfbit.github.io/2017/02/04/yolo-paper/" target="_blank" rel="external">YOLO 论文阅读</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/25167153" target="_blank" rel="external">YOLO2</a></p>
<p><a href="http://blog.csdn.net/hrsstudy/article/details/71173305" target="_blank" rel="external">K-means 计算 anchor boxes</a></p>
<p><a href="http://blog.csdn.net/mounty_fsc/article/details/51746111" target="_blank" rel="external">（Paper）Network in Network网络分析</a></p>
<p><a href="http://blog.csdn.net/malefactor/article/details/51476961#0-tsina-1-62851-397232819ff9a47a7b7e80a40613cfe1" target="_blank" rel="external">Batch Normalization导读</a></p>
<p><strong>global average pooling :</strong></p>
<p>主要是用来解决全连接的问题，其主要是是将最后一层的特征图进行整张图的一个均值池化，形成一个特征点，将这些特征点组成最后的特征向量进行softmax中进行计算。</p>
<p><a href="http://blog.csdn.net/chaojichaoachao/article/details/50994237" target="_blank" rel="external">论文心得：BatchNorm及其变体</a></p>
]]></content>
      
        <categories>
            
            <category> Deep Learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kalman Filter--理解卡尔曼滤波的三重境界]]></title>
      <url>/2017/10/18/Kalman%20Filter--%E7%90%86%E8%A7%A3%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E7%9A%84%E4%B8%89%E9%87%8D%E5%A2%83%E7%95%8C/</url>
      <content type="html"><![CDATA[<h2 id="第一重：初见Kalman"><a href="#第一重：初见Kalman" class="headerlink" title="第一重：初见Kalman"></a>第一重：初见Kalman</h2><p>假设我养了一只猪：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171020094252439?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="15%" align="center"> <div align="left"> </div></div></p>
<p>一周前，这只猪的体重是46±0.5kg。注意，在这里我用了±0.5，表示其实我对这只猪一周前的体重并不是那么确定的，也就是说，46kg这个体重有0.5kg的误差。现在，我又养了这只猪一个星期。那么我想要知道它一个星期之后多重，又大概有多少的误差？</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171020094422975?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="40%" align="center"> <div align="left"> </div></div></p>
<p>为了得到一周后的体重，我有两种方法：一是根据我多年的养猪经验得到的猪体重公式推求出一个大概的值，另一个就是直接去称它的体重。当然，两种方法都有一定的误差。假设经验公式得到的体重是48kg，误差2kg；直接称体重得到的是49kg，误差1kg：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171020094542419?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="40%" align="center"> <div align="left"> </div></div></p>
<p>可是，我是一个处女座的人，不管是经验公式得到的值，还是直接称量得到的值，我都觉得不够准。我希望有一种方法，可以同时结合这只猪一周前的体重、用经验公式估计的值以及直接称量得到的值，综合考虑，得出一个最接近猪真实体重的，误差最小的值。这就是卡尔曼滤波要完成的任务。现在我们来把养猪的模型抽象成数学公式：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171020094640594?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="40%" align="center"> <div align="left"> </div></div></p>
<p>上图的左边，上一周的猪的体重，可以抽象为也k-1时刻的状态值，用k-1时刻的最优估计值加上一个误差项来表示，右边同理。其中,</p>
<p>$$P_k=E[e_ke_k^T]$$</p>
<p>这一项表示的是估计值的协方差。这里要说明两点：</p>
<ol>
<li>上图中所有的变量都是用粗体，表示这是一个向量或者一个矩阵；</li>
<li>之所以用（列）向量而非一个数来表示状态值，是因为，虽然一只猪的体重可以用一个值来表示，但是在实际的应用中很多状态并不是一个数就能表示的（比如导弹在空间中的位置，同时有x、y、z三个坐标）。<br>图中右边表示k时刻的状态值，这个值可以通过预测模块（也就是根据经验公式估计猪的体重）和纠错模块（也就是直接去称量猪的体重值）来估计。同样，预测模块和纠错模块都有着对应的误差和误差协方差矩阵。卡尔曼滤波要做的，就是根据贝叶斯估计的相关理论，同时考虑预测模块和纠错模块的协方差，对误差小的项赋予较大的权重，对误差大的项赋予较小的权重，并使预测的误差最小。</li>
</ol>
<p>具体的实现过程如下：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171020095019777?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p><strong>参考：</strong><br><a href="https://www.zhihu.com/question/23971601/answer/137325095" target="_blank" rel="external">https://www.zhihu.com/question/23971601/answer/137325095</a></p>
<h2 id="第二重：Kalman的数学原理"><a href="#第二重：Kalman的数学原理" class="headerlink" title="第二重：Kalman的数学原理"></a>第二重：Kalman的数学原理</h2><p>首先，我们先要引入一个离散控制过程的系统。该系统可用一个线性随机微分方程（Linear Stochastic Difference equation）来描述：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171019220829166?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"> <div align="left"> </div></div></p>
<p>上两式子中，x(k)是k时刻的系统状态，u(k)是k时刻对系统的控制量。A和B是系统参数，对于多模型系统，他们为矩阵。y(k)是k时刻的测量值，H是测量系统的参数，对于多测量系统，H为矩阵。q(k)和r(k)分别表示过程和测量的噪声。他们被假设成高斯白噪声(White Gaussian Noise)，他们的covariance分别是Q，R（这里我们假设他们不随系统状态变化而变化）。</p>
<p>对于满足上面的条件(线性随机微分系统，过程和测量都是高斯白噪声)，卡尔曼滤波器是最优的信息处理器。下图给出KF算法的流程和五个核心更新方程如下：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171019221157628?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>五个更新方程为：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171019221256550?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><strong>举个栗子：</strong></p>
<p>假设我们要研究的对象是一个房间的温度。根据你的经验判断，这个房间的温度是恒定的。（这里的假设相当于状态方程的系数A为1）假设你对你的经验不是100%的相信，可能会有上下偏差几度，我们把这些偏差看成是高斯白噪声（这里就是W(k)）。另外，我们在房间里放一个温度计，但是这个温度计也不准确的，测量值会比实际值偏差。我们也把这些偏差看成是高斯白噪声。（温度计的测量值就是Z(k),而由于温度测到的温度就是温度，不用再换算，所以系数H就是1，偏差就是V(k)）。好了，现在对于某一分钟我们有两个有关于该房间的温度值：你根据经验的预测值（系统的预测值X(k|k-1)）和温度计的值（测量值Z(k)）。下面我们要用这两个值结合他们各自的噪声来估算出房间的实际温度值。</p>
<p>假如我们要估算k时刻的是实际温度值。首先你要根据k-1时刻的温度值，来预测k时刻的温度。因为你相信温度是恒定的，所以你会得到k时刻的温度预测值是跟k-1时刻一样的，假设是23度，同时该值的高斯噪声的偏差是5度（<strong>5是这样得到的：如果k-1时刻估算出的最优温度值的偏差（p(k-1|k-1)就是上一时刻的p(k|k)</strong>）是3，你对自己预测的不确定度是4度，他们平方相加再开方，就是5（<strong>算出来的就是P(k|k-1)</strong>））。然后，你从温度计那里得到了k时刻的温度值（<strong>测量值Z(k)</strong>），假设是25度，同时该值的偏差是4度。由于我们用于估算k时刻的实际温度有两个温度值，分别是23 度和25度。究竟实际温度是多少呢？相信自己还是相信温度计呢？究竟相信谁多一点，我们可以用他们的covariance（协方差）来判断。因为 Kg^2=5^2/(5^2+4^2)所以Kg=0.78，我们可以估算出k时刻的实际温度值是：23+0.78*(25-23)=24.56度。可以看出，因为温度计的covariance比较小（比较相信温度计），所以估算出的最优温度值偏向温度计的值。</p>
<p>现在我们已经得到k时刻的最优温度值了，下一步就是要进入 k+1时刻，进行新的最优估算。到现在为止，好像还没看到什么自回归的东西出现。对了，在进入k+1时刻之前，我们还要算出k时刻那个最优值（24.56 度）的偏差。算法如下：((1-Kg)*5^2)^0.5=2.35。这里的5就是上面的k时刻你预测的那个23度温度值的偏差，得出的2.35就是进入 k+1时刻以后k时刻估算出的最优温度值的偏差。就是这样，卡尔曼滤波器就不断的把 covariance递归，从而估算出最优的温度值。他运行的很快，而且它只保留了上一时刻的covariance。上面的Kg，就是卡尔曼增益（Kalman Gain）。他可以随不同的时刻而改变他自己的值，是不是很神奇！</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171019221351867?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<h2 id="第三重：Kalman的数学推导"><a href="#第三重：Kalman的数学推导" class="headerlink" title="第三重：Kalman的数学推导"></a>第三重：Kalman的数学推导</h2><p>首先要计算预测值、预测值和真实值之间误差协方差矩阵：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171019221608954?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="20%" align="center"> <div align="left"> </div></div></p>
<p>有了这两个就能计算卡尔曼增益K，再然后得到估计值：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171019221654298?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="25%" align="center"> <div align="left"> </div></div></p>
<p>最后还要计算估计值和真实值之间的误差协方差矩阵，为下次递推做准备：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171019221743267?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="20%" align="center"> <div align="left"> </div></div></p>
<p>具体原理推导过程参考：</p>
<p>《图像处理、分析与机器视觉》第四版16.6.1 卡尔曼滤波器</p>
<p><a href="http://blog.csdn.net/heyijia0327/article/details/17487467" target="_blank" rel="external">卡尔曼滤波 – 从推导到应用(一)</a></p>
]]></content>
      
        <categories>
            
            <category> Machine Vision </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Algorithm Optimization </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[MeanShift算法及其在目标跟踪上的运用]]></title>
      <url>/2017/10/16/MeanShift%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E5%9C%A8%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E4%B8%8A%E7%9A%84%E8%BF%90%E7%94%A8/</url>
      <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h2 id="1-MeanShift算法"><a href="#1-MeanShift算法" class="headerlink" title="1. MeanShift算法"></a>1. MeanShift算法</h2><p>在d维空间中，任选一个点，然后以这个点为圆心，h为半径做一个高维球，因为有d维，d可能大于2，所以是高维球。落在这个球内的所有点和圆心都会产生一个向量，向量是以圆心为起点落在球内的点位终点。然后把这些向量都相加。相加的结果就是Meanshift向量。</p>
<p>如图所示，其中黄色箭头就是Mh（meanshift向量）：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171012215949079?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"> <div align="left"> </div></div></p>
<p>再以meanshift向量的终点为圆心，再做一个高维的球。如下图所以，重复以上步骤，就可得到一个meanshift向量。如此重复下去，meanshift算法可以收敛到概率密度最大得地方。也就是最稠密的地方。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171012220108288?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"> <div align="left"> </div></div></p>
<p>最终的结果如下：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171012220151838?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"> <div align="left"> </div></div></p>
<p>从上述过程可以看出，在Mean Shift算法中，最关键的就是计算每个点的偏移均值，然后根据新计算的偏移均值更新点的位置。</p>
<p>详细计算过程见参考博客2</p>
<p>参考：</p>
<ol>
<li><p><a href="http://www.cnblogs.com/liqizhou/archive/2012/05/12/2497220.html" target="_blank" rel="external">Meanshift，聚类算法</a></p>
</li>
<li><p><a href="http://blog.csdn.net/google19890102/article/details/51030884" target="_blank" rel="external"> 简单易学的机器学习算法——Mean Shift聚类算法</a></p>
</li>
</ol>
<h2 id="2-基于MeanShift的目标跟踪算法"><a href="#2-基于MeanShift的目标跟踪算法" class="headerlink" title="2. 基于MeanShift的目标跟踪算法"></a>2. 基于MeanShift的目标跟踪算法</h2><p>基于均值漂移的目标跟踪算法通过分别计算目标区域和候选区域内像素的特征值概率得到关于目标模型和候选模型的描述，然后利用相似函数度量初始帧目标模型和当前帧的候选模版的相似性，选择使相似函数最大的候选模型并得到关于目标模型的Meanshift向量，这个向量正是目标由初始位置向正确位置移动的向量。由于均值漂移算法的快速收敛性，通过不断迭代计算Meanshift向量，算法最终将收敛到目标的真实位置，达到跟踪的目的。</p>
<p>下面通过图示直观的说明MeanShift跟踪算法的基本原理。如下图所示：目标跟踪开始于数据点xi0（空心圆点xi0，xi1，…，xiN表示的是中心点，上标表示的是的迭代次数，周围的黑色圆点表示不断移动中的窗口样本点，虚线圆圈代表的是密度估计窗口的大小）。箭头表示样本点相对于核函数中心点的漂移向量，平均的漂移向量会指向样本点最密集的方向，也就是梯度方向。因为 Meanshift 算法是收敛的，因此在当前帧中通过反复迭代搜索特征空间中样本点最密集的区域，搜索点沿着样本点密度增加的方向“漂移”到局部密度极大点点xiN，也就是被认为的目标位置，从而达到跟踪的目的，MeanShift 跟踪过程结束。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171012215226239?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/100/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"> <div align="left"> </div></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171103224031577?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171103224107091?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171103224603679?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171012215455652?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>   参考博客：</p>
<ol>
<li><a href="http://blog.csdn.net/jinshengtao/article/details/30258833" target="_blank" rel="external">基于MeanShift的目标跟踪算法及实现</a></li>
<li><a href="http://blog.csdn.net/yuanxing14/article/details/41971101" target="_blank" rel="external">基于核函数的目标跟踪算法(上)</a></li>
</ol>
]]></content>
      
        <categories>
            
            <category> Machine Vision </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Algorithm Optimization </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CS231n学习笔记--15. Efficient Methods and Hardware for Deep Learning]]></title>
      <url>/2017/10/12/CS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--15.%20Efficient%20Methods%20and%20Hardware%20for%20Deep%20Learning/</url>
      <content type="html"><![CDATA[<h2 id="Agenda"><a href="#Agenda" class="headerlink" title="Agenda"></a>Agenda</h2><p><div align="center"> <img src="http://img.blog.csdn.net/20171108230711331?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"> </div></div></p>
<p><strong>Hardware 101: the Family</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108230816417?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"> </div></div></p>
<p><strong>Hardware 101: Number Representation</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108231033037?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"> </div></div></p>
<p><strong>Hardware 101: Number Representation</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108231107741?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<h2 id="1-Algorithms-for-Efficient-Inference"><a href="#1-Algorithms-for-Efficient-Inference" class="headerlink" title="1. Algorithms for Efficient Inference"></a>1. Algorithms for Efficient Inference</h2><h3 id="1-1-Pruning-Neural-Networks"><a href="#1-1-Pruning-Neural-Networks" class="headerlink" title="1.1 Pruning Neural Networks"></a>1.1 Pruning Neural Networks</h3><p><div align="center"> <img src="http://img.blog.csdn.net/20171108231327347?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"> </div></div></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108231404123?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"> </div></div></p>
<p><strong>Iteratively Retrain to Recover Accuracy</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108231541226?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Pruning RNN and LSTM</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108231951830?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p>pruning之后准确率有所提升：</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108231930548?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"> </div></div></p>
<p><strong>Pruning Changes Weight Distribution</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108232106170?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<h3 id="1-2-Weight-Sharing"><a href="#1-2-Weight-Sharing" class="headerlink" title="1.2  Weight Sharing"></a>1.2  Weight Sharing</h3><p><strong>Trained Quantization</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108232245841?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"> </div></div></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108232316328?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>How Many Bits do We Need?</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108232634946?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Pruning + Trained Quantization Work Together</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108232721524?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Huffman Coding</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108232932769?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Summary of Deep Compression</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108233030647?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Results: Compression Ratio</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108233138983?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>SqueezeNet</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108233330666?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="40%"><div align="left"> </div></div></p>
<p><strong>Compressing SqueezeNet</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171108233400593?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"> </div></div></p>
<h3 id="1-3-Quantization"><a href="#1-3-Quantization" class="headerlink" title="1.3  Quantization"></a>1.3  Quantization</h3><p><strong>Quantizing the Weight and Activation</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109213848595?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Quantization Result</strong>：选择8bit</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109213955500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<h3 id="1-4-Low-Rank-Approximation"><a href="#1-4-Low-Rank-Approximation" class="headerlink" title="1.4  Low Rank Approximation"></a>1.4  Low Rank Approximation</h3><p><strong>Low Rank Approximation for Conv</strong>：类似Inception Module</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109214145323?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Low Rank Approximation for FC</strong> :矩阵分解</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109214255451?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<h3 id="1-5-Binary-Ternary-Net"><a href="#1-5-Binary-Ternary-Net" class="headerlink" title="1.5  Binary/Ternary Net"></a>1.5  Binary/Ternary Net</h3><p><strong>Trained Ternary（三元） Quantization</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109214501350?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Weight Evolution during Training</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109214707154?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Error Rate on ImageNet</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109214836127?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"> </div></div></p>
<h3 id="1-6-Winograd-Transformation"><a href="#1-6-Winograd-Transformation" class="headerlink" title="1.6 Winograd Transformation"></a>1.6 Winograd Transformation</h3><p><strong>3x3 DIRECT Convolutions</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109215139917?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p>Direct convolution: we need 9xCx4 = 36xC FMAs for 4 outputs</p>
<p><strong>3x3 WINOGRAD Convolutions</strong>：</p>
<p>Transform Data to Reduce Math Intensity</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109215237660?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p>Direct convolution: we need 9xCx4 = 36xC FMAs for 4 outputs<br>Winograd convolution: we need 16xC FMAs for 4 outputs: 2.25x fewer FMAs</p>
<h2 id="2-Hardware-for-Efficient-Inference"><a href="#2-Hardware-for-Efficient-Inference" class="headerlink" title="2. Hardware for Efficient Inference"></a>2. Hardware for Efficient Inference</h2><p><strong>Hardware for Efficient Inference：</strong></p>
<p>a common goal: minimize memory access</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109215644820?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Google TPU</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109215551131?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109215842498?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Roofline Model: Identify Performance Bottleneck</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109220034310?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"> </div></div></p>
<p><strong>Log Rooflines for CPU, GPU, TPU</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109220218548?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>EIE: the First DNN Accelerator for Sparse, Compressed Model</strong>：<br>不保存、计算0值</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109220406089?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109220525022?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>EIE Architecture</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109220745393?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Micro Architecture for each PE</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109220847590?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Comparison: Throughput</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109221444976?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Comparison: Energy Efficiency</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109221519722?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<h2 id="3-Algorithms-for-Efficient-Training"><a href="#3-Algorithms-for-Efficient-Training" class="headerlink" title="3. Algorithms for Efficient Training"></a>3. Algorithms for Efficient Training</h2><h3 id="3-1-Parallelization"><a href="#3-1-Parallelization" class="headerlink" title="3.1  Parallelization"></a>3.1  Parallelization</h3><p><strong>Data Parallel – Run multiple inputs in parallel</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109221919820?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Parameter Update</strong></p>
<p>参数共享更新</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109222116923?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Model-Parallel Convolution – by output region (x,y)</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109222258401?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Model Parallel Fully-Connected Layer (M x V)</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109222705795?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109222427431?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Summary of Parallelism</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109222808253?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<h3 id="3-2-Mixed-Precision-with-FP16-and-FP32"><a href="#3-2-Mixed-Precision-with-FP16-and-FP32" class="headerlink" title="3.2 Mixed Precision with FP16 and FP32"></a>3.2 Mixed Precision with FP16 and FP32</h3><p><div align="center"> <img src="http://img.blog.csdn.net/20171109222909730?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Mixed Precision Training</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109223040672?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p>结果对比：</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109223128088?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"> </div></div></p>
<h3 id="3-3-Model-Distillation"><a href="#3-3-Model-Distillation" class="headerlink" title="3.3  Model Distillation"></a>3.3  Model Distillation</h3><p>student model has much smaller model size</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109223238654?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Softened outputs reveal the dark knowledge</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109223330302?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"> </div></div></p>
<p><strong>Softened outputs reveal the dark knowledge</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109223437926?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"> </div></div></p>
<h3 id="3-4-DSD-Dense-Sparse-Dense-Training"><a href="#3-4-DSD-Dense-Sparse-Dense-Training" class="headerlink" title="3.4 DSD: Dense-Sparse-Dense Training"></a>3.4 DSD: Dense-Sparse-Dense Training</h3><p><div align="center"> <img src="http://img.blog.csdn.net/20171109223535645?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p>DSD produces same model architecture but can find better optimization solution, arrives at better local minima, and achieves higher prediction accuracy across a wide range of deep neural networks on CNNs / RNNs / LSTMs.</p>
<p><strong>DSD: Intuition</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109223655342?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%"><div align="left"> </div></div></p>
<p><strong>DSD is General Purpose: Vision, Speech, Natural Language</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109223812788?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>DSD on Caption Generation</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109223913510?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<h2 id="4-Hardware-for-Efficient-Training"><a href="#4-Hardware-for-Efficient-Training" class="headerlink" title="4. Hardware for Efficient Training"></a>4. Hardware for Efficient Training</h2><p><strong>GPU / TPU</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109224243342?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"> </div></div></p>
<p><strong>Google Cloud TPU</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109224506393?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<h2 id="Future"><a href="#Future" class="headerlink" title="Future"></a>Future</h2><p><div align="center"> <img src="http://img.blog.csdn.net/20171109224610279?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"> </div></div></p>
<p><strong>Outlook: the Focus for Computation</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171109224655472?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"> </div></div></p>
]]></content>
      
        <categories>
            
            <category> CS231n </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[数据处理——One-Hot Encoding]]></title>
      <url>/2017/10/10/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E2%80%94%E2%80%94One-Hot%20Encoding/</url>
      <content type="html"><![CDATA[<div class="github-widget" data-repo="JoelSutherland/GitHub-jQuery-Repo-Widget"></div>

<script type="text/javascript" src="js/jquery.githubRepoWidget.min.js"></script>

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h2 id="一、One-Hot-Encoding"><a href="#一、One-Hot-Encoding" class="headerlink" title="一、One-Hot Encoding"></a>一、One-Hot Encoding</h2><p>One-Hot编码，又称为一位有效编码，主要是采用位状态寄存器来对个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候只有一位有效。<br>在实际的机器学习的应用任务中，特征有时候并不总是连续值，有可能是一些分类值，如性别可分为“male”和“female”。在机器学习任务中，对于这样的特征，通<br>常我们需要对其进行特征数字化，如下面的例子：<br>有如下三个特征属性：<br>性别：[“male”，”female”]<br>地区：[“Europe”，”US”，”Asia”]<br>浏览器：[“Firefox”，”Chrome”，”Safari”，”Internet Explorer”]<br>对于某一个样本，如[“male”，”US”，”Internet Explorer”]，我们需要将这个分类值的特征数字化，最直接的方法，我们可以采用序列化的方<br>式：[0,1,3]。但是这样的特征处理并不能直接放入机器学习算法中。</p>
<h2 id="二、One-Hot-Encoding的处理方法"><a href="#二、One-Hot-Encoding的处理方法" class="headerlink" title="二、One-Hot Encoding的处理方法"></a>二、One-Hot Encoding的处理方法</h2><p>对于上述的问题，性别的属性是二维的，同理，地区是三维的，浏览器则是思维的，这样，我们可以采用One-Hot编码的方式对上述的样本“[“male”，”US”，”Internet Explorer”]”编码，“male”则对应着[1，0]，同理“US”对应着[0，1，0]，“Internet Explorer”对应着[0,0,0,1]。则完整的特征数字化的结果为：[1,0,0,1,0,0,0,0,1]。这样导致的一个结果就是数据会变得非常的稀疏。</p>
<h2 id="三、实际的Python代码"><a href="#三、实际的Python代码" class="headerlink" title="三、实际的Python代码"></a>三、实际的Python代码</h2><pre><code>[python] view plain copy
from sklearn import preprocessing  

enc = preprocessing.OneHotEncoder()  
enc.fit([[0,0,3],[1,1,0],[0,2,1],[1,0,2]])  //测试样本

array = enc.transform([[0,1,3]]).toarray()  

print array  
</code></pre><p>结果：[[ 1. 0. 0. 1. 0. 0. 0. 0. 1.]]</p>
<p>转载自：</p>
<p><a href="http://blog.csdn.net/google19890102/article/details/44039761" target="_blank" rel="external"> 数据处理——One-Hot Encoding
</a></p>
]]></content>
      
        <categories>
            
            <category> Machine Learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CS231n学习笔记--14. Reinforcement Learning]]></title>
      <url>/2017/10/07/CS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--14.%20Reinforcement%20Learning/</url>
      <content type="html"><![CDATA[<h2 id="1-What-is-Reinforcement-Learning"><a href="#1-What-is-Reinforcement-Learning" class="headerlink" title="1. What is Reinforcement Learning"></a>1. What is Reinforcement Learning</h2><p>概述：</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106213804400?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"> <div align="left"></div></div></p>
<p>举个栗子：</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106213941094?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"> <div align="left"></div></div></p>
<p>再举一个：</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106214010744?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"> <div align="left"></div></div></p>
<h2 id="2-Markov-Decision-Process"><a href="#2-Markov-Decision-Process" class="headerlink" title="2. Markov Decision Process"></a>2. Markov Decision Process</h2><ul>
<li>Mathematical formulation of the RL problem</li>
<li><strong>Markov property</strong>: Current state completely characterises the state of the world</li>
</ul>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106214247108?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>处理流程：</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106214412283?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p>The optimal policy π*</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106214724725?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<h2 id="3-Q-learning"><a href="#3-Q-learning" class="headerlink" title="3. Q-learning"></a>3. Q-learning</h2><p>Definitions: Value function and Q-value function：</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106214951418?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"> <div align="left"></div></div></p>
<p>Bellman equation：</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106220153278?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"> <div align="left"></div></div></p>
<p>优化策略：</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106220520502?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106220743538?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>Solving for the optimal policy: Q-learning</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106221715449?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"> <div align="left"></div></div></p>
<p>举个栗子：Playing Atari Games</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106221803301?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>Q-network Architecture</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106221935504?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>Training the Q-network: Experience Replay</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106222820079?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p>Deep Q-Learning with Experience Replay</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106223228947?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"> <div align="left"></div></div></p>
<h2 id="4-Policy-Gradients"><a href="#4-Policy-Gradients" class="headerlink" title="4. Policy Gradients"></a>4. Policy Gradients</h2><p><div align="center"> <img src="http://img.blog.csdn.net/20171106223426251?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106223637933?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106223812232?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p>Intuition：</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106223900710?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p>Variance reduction：</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106223944772?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p>Variance reduction: Baseline</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106224050287?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p>How to choose the baseline?</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106224133084?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p>A better baseline: Want to push up the probability of an action from a state, if this action was better than the <strong>expected value of what we should get from that state</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106224258262?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106224722426?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><strong>Actor-Critic Algorithm</strong></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106224839928?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106224946577?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="40%"> <div align="left"></div></div></p>
<h2 id="5-REINFORCE-的运用"><a href="#5-REINFORCE-的运用" class="headerlink" title="5. REINFORCE 的运用"></a>5. REINFORCE 的运用</h2><h3 id="5-1-Recurrent-Attention-Model-RAM"><a href="#5-1-Recurrent-Attention-Model-RAM" class="headerlink" title="5.1 Recurrent Attention Model (RAM)"></a>5.1 Recurrent Attention Model (RAM)</h3><p><div align="center"> <img src="http://img.blog.csdn.net/20171106225036441?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"> <div align="left"></div></div></p>
<p>效果示意图：</p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106225124851?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<p><div align="center"> <img src="http://img.blog.csdn.net/20171106225229610?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<h3 id="5-2-AlphaGo"><a href="#5-2-AlphaGo" class="headerlink" title="5.2 AlphaGo"></a>5.2 AlphaGo</h3><p><div align="center"> <img src="http://img.blog.csdn.net/20171106225424714?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/22/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"> <div align="left"></div></div></p>
<h2 id="6-Summary"><a href="#6-Summary" class="headerlink" title="6. Summary"></a>6. Summary</h2><ul>
<li><strong>Policy gradients:</strong> very general but suffer from high variance so requires a lot of samples.<br><strong>Challenge:</strong> sample-efficiency</li>
<li><strong>Q-learning:</strong> does not always work but when it works, usually more sample-efficient. <strong>Challenge:</strong> exploration</li>
<li><strong>Guarantees:</strong><br> <strong>Policy Gradients:</strong> Converges to a local minima of J(θ),      often good enough!<br> <strong>Q-learning:</strong> Zero guarantees since you are approximating      Bellman equation with a complicated function approximator</li>
</ul>
]]></content>
      
        <categories>
            
            <category> CS231n </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CS231n学习笔记--13. Generative Models]]></title>
      <url>/2017/10/06/CS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--13.%20Generative%20Models/</url>
      <content type="html"><![CDATA[<h1 id="1-Unsupervised-Learning"><a href="#1-Unsupervised-Learning" class="headerlink" title="1. Unsupervised Learning"></a>1. Unsupervised Learning</h1><p><strong>Supervised vs Unsupervised Learning:</strong></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104103705970?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<h1 id="2-Generative-Models"><a href="#2-Generative-Models" class="headerlink" title="2. Generative Models"></a>2. Generative Models</h1><p><strong>概述：</strong></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104104427775?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"></div></div></p>
<p><strong>Generative Models的作用：</strong></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104104459704?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"></div></div></p>
<p><strong>Generative Models的分类：</strong></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104104635315?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<h1 id="3-PixelRNN-and-PixelCNN"><a href="#3-PixelRNN-and-PixelCNN" class="headerlink" title="3. PixelRNN and PixelCNN"></a>3. PixelRNN and PixelCNN</h1><p><strong>基本原理：</strong></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104104943325?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p><strong>PixelRNN：</strong></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104110548366?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p><strong>PixelCNN：</strong></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104110710379?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p>Training is faster than PixelRNN (can parallelize convolutions since context region values known from training images)<br>Generation must still proceed sequentially=&gt; still slow</p>
<p><strong>Generation Samples：</strong></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104110944809?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"></div></div></p>
<p><strong>PixelRNN and PixelCNN</strong></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104111111772?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%"><div align="left"></div></div></p>
<h1 id="4-Variational-Autoencoders-VAE"><a href="#4-Variational-Autoencoders-VAE" class="headerlink" title="4. Variational Autoencoders (VAE)"></a>4. Variational Autoencoders (VAE)</h1><h2 id="4-1-与PixelRNN-PixelCNN的比较："><a href="#4-1-与PixelRNN-PixelCNN的比较：" class="headerlink" title="4.1 与PixelRNN/PixelCNN的比较："></a>4.1 与PixelRNN/PixelCNN的比较：</h2><p><div align="center"><img src="http://img.blog.csdn.net/20171104111730877?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<h2 id="4-2-Some-background-first-Autoencoders："><a href="#4-2-Some-background-first-Autoencoders：" class="headerlink" title="4.2 Some background first: Autoencoders："></a>4.2 Some background first: Autoencoders：</h2><p><div align="center"><img src="http://img.blog.csdn.net/20171104111939948?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"></div></div></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104112108563?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"></div></div></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104112220835?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="40%"><div align="left"></div></div></p>
<p><strong>Tips:</strong></p>
<p>如果将其用于特征提取，则在训练之后，将decoder部分丢弃！</p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104112456920?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p>Autoencoders can reconstruct data, and can learn features to initialize a supervised model！</p>
<h2 id="4-3-Variational-Autoencoders"><a href="#4-3-Variational-Autoencoders" class="headerlink" title="4.3 Variational Autoencoders"></a>4.3 Variational Autoencoders</h2><p><div align="center"><img src="http://img.blog.csdn.net/20171104151834765?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"></div></div></p>
<p>利用高斯分布随机生成特征Z：</p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104152032103?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104152232036?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="30%"><div align="left"></div></div></p>
<p><strong>Variational Autoencoders: Intractability</strong></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104155620741?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p>$$p<em>θ(z)$$ 跟据高斯分布随机获得， $$p</em>θ(x|z)$$ 根据decoder net获得，而为每个z计算 $$p<em>θ(x|z)$$ 并最终积分得到 $$p</em>θ(x)$$ 是不可能的！</p>
<p>解决办法：</p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104160258564?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"></div></div></p>
<p>如何进行优化：</p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104160431898?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="90%"><div align="left"></div></div></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104160602026?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="90%"><div align="left"></div></div></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104160651993?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"></div></div></p>
<h2 id="4-4-Generating-Data"><a href="#4-4-Generating-Data" class="headerlink" title="4.4 Generating Data"></a>4.4 Generating Data</h2><p><div align="center"><img src="http://img.blog.csdn.net/20171104160759653?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104160904623?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<h2 id="4-5-性能分析："><a href="#4-5-性能分析：" class="headerlink" title="4.5 性能分析："></a>4.5 性能分析：</h2><p><div align="center"><img src="http://img.blog.csdn.net/20171104155340465?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<h1 id="5-Generative-Adversarial-Networks-GAN"><a href="#5-Generative-Adversarial-Networks-GAN" class="headerlink" title="5. Generative Adversarial Networks (GAN)"></a>5. Generative Adversarial Networks (GAN)</h1><p><strong>回顾：</strong></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104161112759?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"></div></div></p>
<h2 id="5-1-Training-GANs-Two-player-game"><a href="#5-1-Training-GANs-Two-player-game" class="headerlink" title="5.1 Training GANs: Two-player game"></a>5.1 Training GANs: Two-player game</h2><p><strong>Generator network:</strong> try to fool the discriminator by generating real-looking images<br><strong>Discriminator network:</strong> try to distinguish between real and fake images</p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104161425809?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="40%"><div align="left"></div></div></p>
<p>网络优化：</p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104161720914?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"></div></div></p>
<p>优化存在的问题：</p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104161844066?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"></div></div></p>
<p>解决办法：</p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104162032863?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"></div></div></p>
<p>GAN training algorithm：</p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104162213448?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"></div></div></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104162325762?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<h2 id="5-2-Generative-Adversarial-Nets"><a href="#5-2-Generative-Adversarial-Nets" class="headerlink" title="5.2 Generative Adversarial Nets"></a>5.2 Generative Adversarial Nets</h2><p>Generated samples:</p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104162455660?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104162628890?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"><div align="left"></div></div></p>
<p><strong>Generative Adversarial Nets: Convolutional Architectures</strong></p>
<p>Generator is an upsampling network with fractionally-strided convolutions<br>Discriminator is a convolutional network</p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104163059349?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p>Generator网络结构：</p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104163210367?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p>Samples from the model look amazing!</p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104163405077?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p><strong>Generative Adversarial Nets: Interpretable Vector Math</strong></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104163612491?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104163645915?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104163809574?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p><strong>GANs的优缺点：</strong></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171104163911789?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<h1 id="6-回顾："><a href="#6-回顾：" class="headerlink" title="6. 回顾："></a>6. 回顾：</h1><p><div align="center"><img src="http://img.blog.csdn.net/20171104164014035?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
]]></content>
      
        <categories>
            
            <category> CS231n </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CS231n学习笔记--12.Visualizing and Understanding]]></title>
      <url>/2017/10/05/CS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--12.Visualizing%20and%20Understanding/</url>
      <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<p><strong>What’s s going on inside ConvNets?</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015215533156?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<h2 id="1-First-Layer-Visualize-Filters"><a href="#1-First-Layer-Visualize-Filters" class="headerlink" title="1. First Layer: Visualize Filters"></a>1. First Layer: Visualize Filters</h2><p>卷积网络第一层特征以原图像的特征清晰的展现出来：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015215638819?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>但是后续层的输出无法直接理解：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015215915962?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<h2 id="2-Last-Layer"><a href="#2-Last-Layer" class="headerlink" title="2. Last Layer"></a>2. Last Layer</h2><p>最后一层（全连接层）以最邻近算法得出物体类别：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015220221293?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>用降维算法得出结果：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015220327797?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>人脸检测特征激活值示例：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015220620841?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>遮挡实验，用于检测图像某一部分影响识别结果的程度，右图中，像素越红影响越小，越白影响越大：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015220945164?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><strong>Saliency Maps</strong></p>
<p>用类别得分梯度（最后一层）得到Saliency Maps，也可以看出像素层次的不同影响：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015221301222?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>进一步，该图可用于图像分割</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015221806177?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<h2 id="3-中间层"><a href="#3-中间层" class="headerlink" title="3. 中间层"></a>3. 中间层</h2><p><strong>3.1 Visualizing CNN features: Gradient Ascent</strong></p>
<p>找到网络中某一神经元的意义：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015222828521?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"> <div align="left"> </div></div></p>
<p>构建最大神经元响应图步骤：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015222951269?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>改进算法以更好显示：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015223442911?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>中间层的最大神经元响应图：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015223401361?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>多目标：Adding “multi-faceted” visualization gives even nicer results:(Plus more careful regularization, center-bias)</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015223622361?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<h2 id="4-DeepDream-Amplify-existing-features"><a href="#4-DeepDream-Amplify-existing-features" class="headerlink" title="4. DeepDream: Amplify existing features"></a>4. DeepDream: Amplify existing features</h2><p>Rather than synthesizing an image to maximize a specific neuron, instead try to amplify the neuron activations at some layer in the network：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015224026731?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>code:</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015224107845?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>结果图（最后一层）：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015224629614?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<h2 id="5-Feature-Inversion"><a href="#5-Feature-Inversion" class="headerlink" title="5. Feature Inversion"></a>5. Feature Inversion</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20171015224929000?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>利用不同层进行图像重建：Reconstructing from different layers of VGG-16 </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015225040266?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<h2 id="6-Neural-Texture-Synthesis"><a href="#6-Neural-Texture-Synthesis" class="headerlink" title="6. Neural Texture Synthesis"></a>6. Neural Texture Synthesis</h2><p>算法步骤（没看懂。。。）：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015225740000?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>Reconstructing texture from higher layers recovers larger features from the input texture：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015225852124?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<h2 id="7-Neural-Style-Transfer"><a href="#7-Neural-Style-Transfer" class="headerlink" title="7. Neural Style Transfer"></a>7. Neural Style Transfer</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20171015230033442?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>合成流程图：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015230111796?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>效果图：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015230156454?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015230300408?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>Resizing style image before running style transfer algorithm can transfer different types of features：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015230342086?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p><strong>Problem:</strong> Style transfer requires many forward / backward passes through VGG; very slow!</p>
<p><strong>Solution:</strong> Train another neural network to perform style transfer for us!</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015230528532?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>效果图：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015230934609?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><strong>One Network, Many Styles:</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171015230812075?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
]]></content>
      
        <categories>
            
            <category> CS231n </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CS231n学习笔记--11.Detection and Segmentation]]></title>
      <url>/2017/10/04/CS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--11.Detection%20and%20Segmentation/</url>
      <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h2 id="1-Computer-Vision-Task"><a href="#1-Computer-Vision-Task" class="headerlink" title="1. Computer Vision Task"></a>1. Computer Vision Task</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20171014174459140?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<h2 id="2-Semantic-Segmentation"><a href="#2-Semantic-Segmentation" class="headerlink" title="2. Semantic Segmentation"></a>2. Semantic Segmentation</h2><h3 id="2-1-特点："><a href="#2-1-特点：" class="headerlink" title="2.1 特点："></a>2.1 特点：</h3><p>a. Label each pixel in the image with a category label<br>b. Don’t differentiate instances, only care about pixels</p>
<h3 id="2-2-步骤："><a href="#2-2-步骤：" class="headerlink" title="2.2 步骤："></a>2.2 步骤：</h3><p>a. Semantic Segmentation Idea: <strong>Sliding Window</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014181632906?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>b. Semantic Segmentation Idea: <strong>Fully Convolutional</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014183129965?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<h3 id="2-3-upsampling"><a href="#2-3-upsampling" class="headerlink" title="2.3 upsampling:"></a>2.3 upsampling:</h3><p><strong>Max Unpooling</strong>”</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014181855427?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>这样的upsamle有效的原因在于算法不要求得到一张好看的超分辨率图片，而是为了尽可能的保留像素的结构分布特征！</p>
<p><strong>Transpose Convolution</strong></p>
<p>算法原理图：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014182421885?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>1D Example：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014182530379?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<h2 id="3-Classification-Localization"><a href="#3-Classification-Localization" class="headerlink" title="3. Classification + Localization"></a>3. Classification + Localization</h2><p>原理图：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014184419221?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><strong>Human Pose Estimation</strong></p>
<p><strong>目标：</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014184650754?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><strong>原理图：</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014184605837?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<h2 id="4-Object-Detection-as-Classification"><a href="#4-Object-Detection-as-Classification" class="headerlink" title="4. Object Detection as Classification"></a>4. Object Detection as Classification</h2><p>搜索算法Sliding Window存在的问题：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014185035464?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>Region Proposals：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014185114172?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p><strong>RNN</strong>算法原理：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014185411461?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>R-CNN: Problems</p>
<ol>
<li>Ad hoc training objectives</li>
</ol>
<ul>
<li>Fine-tune network with softmax classifier (log loss)</li>
<li>Train post-hoc linear SVMs (hinge loss)</li>
<li>Train post-hoc bounding-box regressions (least squares)</li>
<li>Training is slow (84h), takes a lot of disk space</li>
<li>Inference (detection) is slow</li>
<li>47s / image with VGG16 [Simonyan &amp; Zisserman. ICLR15]</li>
<li>Fixed by SPP-net [He et al. ECCV14]</li>
</ul>
<p><strong>Fast R-CNN</strong>：</p>
<p>检测ROI区域在得到图像特征图之后，从而减少大量的重复特征计算。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014185742438?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> “ </div></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014190129592?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> “ </div></div></p>
<p><strong>Faster R-CNN</strong>: RoI Pooling</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014190053694?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> “ </div></div></p>
<p>在卷积层中设置RPN层用于检测ROI：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014190309725?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> “ </div></div></p>
<p><strong>Detection without Proposals: YOLO / SSD</strong></p>
<p>扫描一次图片时同时进行区域定位与物体识别：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014190538600?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> “ </div></div></p>
<p><strong>Object Detection: Lots of variables …</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014190942365?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> “ </div></div></p>
<p><strong>Aside: Object Detection + Captioning = Dense Captioning</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014191036029?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="90%" align="center"> <div align="left"> “ </div></div></p>
<p>算法架构：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014191130131?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> “ </div></div></p>
<p><strong>Mask R-CNN</strong></p>
<p>加入一个掩摸：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014191313227?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> “ </div></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014191438313?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="90%" align="center"> <div align="left"> “ </div></div></p>
<p><strong>Mask R-CNN Also does pose</strong>：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014191519421?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> “ </div></div></p>
<p>效果图：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171014191547647?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> “ </div></div></p>
]]></content>
      
        <categories>
            
            <category> CS231n </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CS231n学习笔记--10.Recurrent Neural Networks]]></title>
      <url>/2017/10/03/CS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--10.Recurrent%20Neural%20Networks/</url>
      <content type="html"><![CDATA[<p><script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script></p>
<h2 id="1-Recurrent-Neural-Network"><a href="#1-Recurrent-Neural-Network" class="headerlink" title="1. Recurrent Neural Network"></a>1. Recurrent Neural Network</h2><p>RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。<br>有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。</p>
<p>Recurrent Neural Network 框架：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007215326949?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p><strong>Tips：</strong><br>the same function and the same set of parameters are used at every time step.</p>
<p>普通Recurrent Neural Network：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007215514950?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"> <div align="left"> </div></div></p>
<p>多输入输出的RNN架构：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007215644365?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>多输入单一输出的RNN架构：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007215726513?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>单一输入多输出的RNN架构：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007215814020?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>应用于编解码场景：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007215911840?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>单词构建场景训练阶段：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007220048380?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>单词构建场景测试阶段：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007220127803?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>RNN前后向传输计算t梯度策略：</p>
<p>1.前后向传输均使用完整序列</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007220610954?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"> <div align="left"> </div></div></p>
<ol>
<li>前后向传输均使用分块序列</li>
</ol>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007220749020?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"> <div align="left"> </div></div></p>
<p>3.前使用完整序列, 后向传输使用分块序列</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007220831322?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"> <div align="left"> </div></div></p>
<p>CNN+RNN进行图像内容识别：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007221113103?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"> <div align="left"> </div></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007221212884?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="30%" align="center"> <div align="left"> </div></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007221236543?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="30%" align="center"> <div align="left"> </div></div></p>
<p><strong>Image Captioning with Attention：</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007221347502?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"> <div align="left"> </div></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007221759533?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"> <div align="left"> </div></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007221833125?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="15%" align="center"> <div align="left"> </div></div></p>
<p><strong>Tips：</strong></p>
<p>vi指的是CNN之后的图像分块特征，pi指的是该图像块包含特征词汇的概率。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007222325927?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>Soft attention：指的是该词汇参考全图，不过每块有权重大小区别。<br>Hard attention：指的是该词汇参考某块图像块。</p>
<p><strong>RNN用于问题解答：</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007222720764?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007223112884?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<h2 id="2-LSTM"><a href="#2-LSTM" class="headerlink" title="2. LSTM"></a>2. LSTM</h2><p>会有一些更加复杂的场景。假设我们试着去预测“I grew up in France… I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。<br>不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。<br>LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007223732907?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>多个传统RNN级联存在的问题，梯度爆炸/消失：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007224109590?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>LSTM架构：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007224258039?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>特别要注意的因为激活函数是sigmoid的，所以i,f,0均二值化的数值，所以可以缓解传统RNN存在的梯度问题！</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007224347184?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>LSTM与ResNet的相似之处：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007225220654?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p><strong>其他模型：</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171007225803715?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>进一步理解，可参考博客：<a href="http://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="external">理解 LSTM 网络</a></p>
<h1 id="3-Summary"><a href="#3-Summary" class="headerlink" title="3. Summary"></a>3. Summary</h1><p> RNNs allow a lot of flexibility in architecture design</p>
<ul>
<li>Vanilla RNNs are simple but don’t work very well</li>
<li>Common to use LSTM or GRU: their additive interactions improve gradient flow</li>
<li>Backward flow of gradients in RNN can explode or vanish.         Exploding is controlled with gradient clipping. Vanishing is     controlled with additive interactions (LSTM)</li>
<li>Better/simpler architectures are a hot topic of current research</li>
<li>Better understanding (both theoretical and empirical) is needed.</li>
</ul>
]]></content>
      
        <categories>
            
            <category> CS231n </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CS231n学习笔记--9.CNN Architectures]]></title>
      <url>/2017/09/30/CS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--9.CNN%20Architectures/</url>
      <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h2 id="1-AlexNet"><a href="#1-AlexNet" class="headerlink" title="1. AlexNet"></a>1. AlexNet</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20171006102227749?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>Tips：</strong></p>
<ol>
<li><p>Trained on GTX 580 GPU with only 3 GB of memory.Network spread across 2 GPUs, half the neurons (feature maps) on each<br>GPU.所以在CONV1中分为两部分，每部分输出大小为55X55X48！</p>
</li>
<li><p>CONV1, CONV2, CONV4, CONV5: Connections only with feature maps on same GPU</p>
</li>
<li><p>CONV3, FC6, FC7, FC8: Connections with all feature maps in<br>preceding layer, communication across GPUs</p>
</li>
<li><p>heavy data augmentation：</p>
<p> a. 增大训练样本：通过对于图像的变换实现了对于数据集合的enlarge。首先对于输入的图像（size 256<em>256）随机提取224</em>224的图像集合，并对他们做一个horizontal reflections。变换后图像和原图像相差了32个像素，因此主体部分应该都包含在训练集合中，相当于在位置这个维度上丰富了训练数据。对horizontal reflections来说，相当于相机在主轴方向做了镜像，丰富了反方向的图像。数据集合增大了2048倍，直接结果就是降低了overfitting同时降低了网络结构设计的复杂层度。</p>
<p> 在测试阶段，取每一个测试样本四个角以及中间区域，一共5个patch然后再镜像后得到10个样本输入到网络中，最后将10个softmax输出平均后作为最后的输出。</p>
<p> b.使用PCA对于训练数据进行增强：对于每一个RGB图像进行一个PCA的变换，完成去噪功能，同时为了保证图像的多样性，在eigenvalue上加了一个随机的尺度因子，每一轮重新生成一个尺度因子，这样保证了同一副图像中在显著特征上有一定范围的变换，降低了overfitting的概率。</p>
</li>
<li><p>Norm layers:</p>
<p>更常用的是Local Response Normalization：<br>使用ReLU f(x)=max(0,x)后，你会发现激活函数之后的值没有了tanh，sigmoid函数那样有一个值域区间，所以一般在ReLU之后会做一个normalization，LRU就是其中一种方法，在神经科学中有个概念叫“Lateral inhibition”，讲的是活跃的神经元对它周边神经元的影响。</p>
</li>
</ol>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006104608122?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>从这个公式中可以看出，原来的激活值a被加一个归一化权重（分母部分）生成了新的激活b，相当于在同一个位置（x，y），不同的map上的激活进行了平滑，平滑操作大概可以将识别率提高1-2个百分点。<br>之后，这一层已经被其它种的Regularization技术，如drop out, batch normalization取代了。知道了这些，似乎也可以不那么纠结这个LRN了。</p>
<h2 id="2-ZFNet"><a href="#2-ZFNet" class="headerlink" title="2. ZFNet"></a>2. ZFNet</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20171006105228083?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006105341267?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<h2 id="3-VGGNet"><a href="#3-VGGNet" class="headerlink" title="3. VGGNet"></a>3. VGGNet</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20171006105627204?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>Tips：</strong></p>
<ol>
<li>用更小的filters的原因是可以大大减少参数的数量，并通过增加depth，达到与较大filters相同的效果（从参考的邻域点考虑Stack of three 3x3 conv (stride 1) layers has same effective receptive field as<br>one 7x7 conv layer）！</li>
<li>No Local Response Normalisation (LRN)</li>
</ol>
<p>VGGNet消耗资源表：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006110142088?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<h2 id="4-GoogLeNet"><a href="#4-GoogLeNet" class="headerlink" title="4. GoogLeNet"></a>4. GoogLeNet</h2><p>GoogLeNet架构：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006110524369?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>Tips：</strong></p>
<p>“Inception module”: design a good local network topology (network within a network) and then stack these modules on top of each other</p>
<p>初始的Inception module及其存在的问题：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006110936604?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>Solution: “bottleneck” layers that use 1x1 convolutions to reduce feature depth:</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006111154381?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>Tips:</strong></p>
<p>Add 1x1 CONV with 32 filters, preserves spatial dimensions, reduces depth! Projects depth to lower dimension (combination of feature maps).</p>
<p>改进的Inception module：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006112916918?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>Full GoogLeNet architecture：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006113103546?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>Auxiliary classification outputs可以缓解深度过深导致权重梯度过小而无法优化的现象！</p>
<h2 id="5-ResNet"><a href="#5-ResNet" class="headerlink" title="5. ResNet"></a>5. ResNet</h2><p>受到深度的意义的驱使，出现了这样一个问题：是不是更多的堆叠层就一定能学习出更好的网络？这个问题的一大障碍就是臭名昭著的梯度消失/爆炸问题，它从一开始就阻碍了收敛，然而梯度消失/爆炸的问题，很大程度上可以通过标准的初始化和正则化层来基本解决，确保几十层的网络能够收敛（用SGD+反向传播）。</p>
<p>　　然而当开始考虑更深层的网络的收敛问题时，退化问题就暴露了：随着神经网络深度的增加，精确度开始饱和（这是不足为奇的），然后会迅速的变差。出人意料的，这样一种退化，并不是过拟合导致的，并且增加更多的层匹配深度模型，会导致更多的训练误差。如下图所示：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006155054290?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>　　我们通过引入一个深度残差学习框架，解决了这个退化问题。我们不期望每一层能直接吻合一个映射，我们明确的让这些层去吻合残差映射。形式上看，就是用H(X)来表示最优解映射，但我们让堆叠的非线性层去拟合另一个映射F（X）:=H(X) - X, 此时原最优解映射H（X）就可以改写成F(X)+X，我们假设残差映射跟原映射相比更容易被优化。极端情况下，如果一个映射是可优化的，那也会很容易将残差推至0，<strong>把残差推至0和把此映射逼近另一个非线性层相比要容易的多。</strong></p>
<p>　　F(X)+X的公式可以通过在前馈网络中做一个“快捷连接”来实现 ，快捷连接跳过一个或多个层。在我们的用例中，快捷连接简单的执行自身映射，它们的输出被添加到叠加层的输出中。自身快捷连接既不会添加额外的参数也不会增加计算复杂度。整个网络依然可以用SGD+反向传播来做端到端的训练，并且可以很容易用大众框架来实现（比如Caffe）不用修改slover配置（slover是caffe中的核心slover.prototxt）</p>
<p>ResNet结构图：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006154318957?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>ResNet结构特点：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006155302440?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>更进一步的改进：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006155417500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>Training ResNet in practice:</strong></p>
<ul>
<li>Batch Normalization after every CONV layer</li>
<li>Xavier/2 initialization from He et al.</li>
<li>SGD + Momentum (0.9)</li>
<li>Learning rate: 0.1, divided by 10 when validation error plateaus</li>
<li>Mini-batch size 256</li>
<li>Weight decay of 1e-5</li>
<li>No dropout used</li>
</ul>
<p>各网络架构性能（准确率，耗时，占用内存）比较：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006155812382?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>Tips：</strong></p>
<ol>
<li>Inception-v4: Resnet + Inception!</li>
<li>VGG: Highest memory, most operations</li>
<li>GoogLeNet: most efficient</li>
<li>AlexNet: Smaller compute, still memory heavy, lower accuracy</li>
<li>ResNet: Moderate efficiency depending on model, highest accuracy</li>
</ol>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006160056485?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<h2 id="6-Other-architectures"><a href="#6-Other-architectures" class="headerlink" title="6. Other architectures"></a>6. Other architectures</h2><p><strong>Network in Network (NiN)：</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006160409516?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>Improving ResNets：</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006160734579?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>更宽的残差网络：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006161004901?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006161050225?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>加入随机概念：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006161416263?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>Fractal architecture with both shallow and deep paths to output：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006161558698?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><strong>Beyond ResNets</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006161729409?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><strong>Efficient networks</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006161816291?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><strong>Summary: CNN Architectures</strong></p>
<ul>
<li>VGG, GoogLeNet, ResNet all in wide use, available in model zoos</li>
<li>ResNet current best default</li>
<li>Trend towards extremely deep networks</li>
<li>Significant research centers around design of layer / skip connections and improving gradient flow</li>
<li>Even more recent trend towards examining necessity of depth vs.<br>width and residual connections</li>
</ul>
]]></content>
      
        <categories>
            
            <category> CS231n </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CS231n学习笔记--8.Deep Learning Software]]></title>
      <url>/2017/09/27/CS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--8.Deep%20Learning%20Software/</url>
      <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h2 id="1-CPU-amp-GPU"><a href="#1-CPU-amp-GPU" class="headerlink" title="1.CPU &amp; GPU"></a>1.CPU &amp; GPU</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20171004210746371?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"></div></p>
<div align="left"> 

<p>GPU更擅长做并行化简单任务处理，如矩阵运算等！</p>
<p>GPU开源库：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004211115965?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"></div></p>
<p>CPU vs GPU:</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004211258839?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"></div></p>
<p>CPU/GPU通信：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004211344408?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"></div></p>
<div align="left"> 

<h2 id="2-Deep-Learning-Frameworks"><a href="#2-Deep-Learning-Frameworks" class="headerlink" title="2. Deep Learning Frameworks"></a>2. Deep Learning Frameworks</h2><p>利用Numpy手动构建神经网络：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004211822673?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"></div></p>
<p><div align="left"><br>利用TensorFlow构建神经网络：</div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004211935224?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"></div></p>
<p><div align="left"><br>利用PyTorch构建神经网络：</div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004212238217?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"></div></p>
<p><div align="left"> </div></p>
<h2 id="3-TensorFlow"><a href="#3-TensorFlow" class="headerlink" title="3. TensorFlow"></a>3. TensorFlow</h2><p>利用TensorFlow构建神经网络容易出现以下问题：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004213718594?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"></div></p>
<p><div align="left"><br><strong>解决方法：</strong></div></p>
<ol>
<li>Change w1 and w2 from placeholder (fed on each call) to Variable(persists in the graph between calls)</li>
<li>Add assign operations to update w1 and w2 as part of the graph!</li>
<li>Run graph once to initialize w1 and w2</li>
<li>Add dummy graph node that depends on updates</li>
<li>Tell graph to compute dummy node</li>
</ol>
<p><strong>Tips：</strong><br>实质上是通过构建假的权重网络结点，将权重值保留在内存中，而该网络结点实际上是没有输出，故不会运行！</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004214550191?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004215049370?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004215310918?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004215533230?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004215833091?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004215947719?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"></div></p>
<div align="left"> 

<h2 id="4-PyTorch"><a href="#4-PyTorch" class="headerlink" title="4. PyTorch"></a>4. PyTorch</h2><p>PyTorch与TensorFlow的区别：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004220802421?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"><br> <div align="left"><br> 利用PyTorch构建网络：</div></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004220941552?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/10/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"></div></p>
<p><div align="left"><br>如何在GPU上运行PyTorch代码：</div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004221036399?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"></div></p>
<p><div align="left"><br>PyTorch可自动计算梯度（x,y不需要）：</div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004221249749?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"></div></p>
<p><div align="left"><br>利用PyTorch可自定义梯度计算公式：</div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004221827008?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"></div></p>
<p>PyTorch的nn模型：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004222120729?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"></div></p>
<p>利用PyTorch对训练数据进行minbath：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004222303436?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"></div></p>
<p><div align="left"><br>PyTorch的预训练模型库：</div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004222540597?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"></div></p>
<p><div align="left"><br>Static vs Dynamic Graphs：</div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004223037786?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"></div></p>
<div align="left"> 

<p>Static 比 Dynamic Graphs更有利用优化模型：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004223202301?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"></div></p>
<div align="left"> 

<p>因为Static Graphs在正式训练前模型架构已经确定，所以第一次训练后不需要用指定的代码，可以执行C代码，这样运行效率更高！</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004223337742?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"></div></p>
<p><div align="left"><br>在条件分支选择中，dynamic模型更加简洁：</div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004223640158?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"></div></p>
<p><div align="left"><br>在循环网络中，dynamic模型更加简洁：<br>（TensorFlow Fold make dynamic graphs easier in TensorFlow through dynamic batching）</div></p>
<p><div align="left"> </div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004223837289?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"></div></p>
<p><div align="left"><br>Dynamic Graph Applications：</div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004224413042?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004224424473?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004224435128?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"></div></p>
<p><div align="left"> </div></p>
<h2 id="5-Caffe"><a href="#5-Caffe" class="headerlink" title="5. Caffe"></a>5. Caffe</h2><p><strong>特点：</strong></p>
<p>●  Core written in C++<br>●  Has Python and MATLAB bindings<br>●  Good for training or finetuning feedforward classification models<br>●  Often <strong>no need to write code</strong>!<br>●  Not used as much in research anymore, still popular for deploying models</p>
<p><strong>使用步骤：</strong></p>
<p><strong>1.  Convert data (run a script)</strong></p>
<pre><code>●  DataLayer reading from LMDB is the easiest
●  Create LMDB using convert_imageset
●  Need text file where each line is
    ○ “[path/to/image.jpeg] [label]”
●  Create HDF5 file yourself using h5py
●  ImageDataLayer: Read from image files
●  WindowDataLayer: For detection
●  HDF5Layer: Read from HDF5 file
●  From memory, using Python interface
●  All of these are harder to use (except Python)
</code></pre><p><strong>2.  Define net (edit prototxt)</strong><br>     <div align="center"><br><img src="http://img.blog.csdn.net/20171004225034281?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"></div></p>
<p><div align="left"><br><strong>3.  Define solver (edit prototxt)</strong></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004225137723?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"></div></p>
<p><div align="left"><br><strong>4.  Train (with pretrained weights) (run a script)</strong></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004225235393?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"></div></p>
<div align="left"> 

<p>Caffe的相关资料：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004225333216?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"></div></p>
<p><div align="left"><br><strong>Caffe的优缺点：</strong></div></p>
<p>● Interfacing with numpy<br>● Extract features: Run net forward<br>● Compute gradients: Run net backward (DeepDream, etc)<br>● Define layers in Python with numpy (CPU only)<br>● (+) Good for feedforward networks<br>● (+) Good for finetuning existing networks<br>● (+) Train models without writing any code!<br>● (+) Python interface is pretty useful!<br>● (+) Can deploy without Python<br>● (-) Need to write C++ / CUDA for new GPU layers<br>● (-) Not good for recurrent networks<br>● (-) Cumbersome for big networks (GoogLeNet, ResNet)</p>
<p><strong>Caffe2 Overview</strong><br>● Static graphs, somewhat similar to TensorFlow<br>● Core written in C++<br>● Nice Python interface<br>● Can train model in Python, then serialize and deploy<br>without Python<br>● Works on iOS / Android, etc</p>
<p><strong>模型使用建议：</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171004225626076?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"></div></p>
</div></div></div></div></div></div>]]></content>
      
        <categories>
            
            <category> CS231n </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CS231n学习笔记--5.CNN&&6-7. Training Neural Networks]]></title>
      <url>/2017/09/23/CS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--5.CNN&amp;&6-7.%20Training%20Neural%20Networks/</url>
      <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h1 id="1-CNN"><a href="#1-CNN" class="headerlink" title="1.CNN"></a>1.CNN</h1><h2 id="1-1-原理"><a href="#1-1-原理" class="headerlink" title="1.1 原理"></a>1.1 原理</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170913183725651?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="80%"></div></p>
<div align="left"> 

<p><strong>Tips：</strong><br>1.每个卷积层由K个卷积模板生成，例如：第一层一般为边缘检测的卷积模板。<br>2.为了使卷积前后的数据维度一致，可以将原始数据进行边拓展，拓展的数量为P。</p>
<h2 id="1-2-CNN流程"><a href="#1-2-CNN流程" class="headerlink" title="1.2.CNN流程"></a>1.2.CNN流程</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170913184322933?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left"> 

<p><strong>Tips：</strong>  </p>
<p>1.每个卷积层后会有Relu操作，起将矩阵稀疏化的作用。<br>2.若干个卷积层后会有POOL操作，其实就是降采样，一般取采样区域中的最大值为采样结果。<br>3.最后是FC（full connected）layers(第三章所讲的神经网络)，对数据进行分类识别，计算样本在各分类下的分值，最后使用softmax激活函数。</p>
<h1 id="2-Training-Neural-Networks-1"><a href="#2-Training-Neural-Networks-1" class="headerlink" title="2. Training Neural Networks 1"></a>2. Training Neural Networks 1</h1><h2 id="2-1-Activation-Functions"><a href="#2-1-Activation-Functions" class="headerlink" title="2.1 Activation Functions"></a>2.1 Activation Functions</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170918094222944?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">



<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918094408664?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">  





<p><strong> Tips:  </strong></p>
<p>Sigmoid激活函数的导数大于0，如果输入数据全是正或全是负，那么dL/dw符号始终不变，这意味着w的更新方向均相同，这必然对寻找合适的w参数不利，这也是为什么一般要求数据关于0对称的原因，也解释了问题2。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918095000584?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left"> 



<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918095153335?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">





<p><strong> Tips：  </strong></p>
<p>当输入x小于0时，Relu的输出为0，导数也为0，这使得所在的神经元权重w始终得不到更新，因此称此神经元为dead ReLU。</p>
<p><strong>ReLU的改进：</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918095530367?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">  





<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918095731733?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">  







<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918095825876?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="50%"></div></p>
<div align="left"> 




<p><strong>总结与建议：</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918095932140?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="40%"></div></p>
<div align="left">  







<h2 id="2-2-Date-Preprocessing"><a href="#2-2-Date-Preprocessing" class="headerlink" title="2.2 Date Preprocessing"></a>2.2 Date Preprocessing</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170918100757243?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="80%"></div></p>
<div align="left"> 





<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918100838972?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="80%"></div></p>
<div align="left">  



<p><strong> Tips:  </strong></p>
<p>Whitening的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。数据的whitening必须满足两个条件：一是不同特征间相关性最小，接近0；二是所有特征的方差相等（不一定为1）。常见的白化操作有PCA whitening和ZCA whitening。</p>
<p>PCA whitening是指将数据x经过PCA降维为z后，可以看出z中每一维是独立的，满足whitening白化的第一个条件，这是只需要将z中的每一维都除以标准差就得到了每一维的方差为1，也就是说方差相等。公式为：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918102059563?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="30%"></div></p>
<div align="left"> 



<p>ZCA whitening是指数据x先经过PCA变换为z，但是并不降维，因为这里是把所有的成分都选进去了。这是也同样满足whtienning的第一个条件，特征间相互独立。然后同样进行方差为1的操作，最后将得到的矩阵左乘一个特征向量矩阵U即可。</p>
<p>ZCA whitening公式为：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918102237937?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="30%"></div></p>
<div align="left"> 

<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923173136443?watermark/2/text/aHR0cDovL2J
sb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==
/dissolve/70/gravity/SouthEast" width="30%"></div></p>
<div align="left">  







<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918102340856?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left"> 



<h2 id="2-3-Weight-Initialiation"><a href="#2-3-Weight-Initialiation" class="headerlink" title="2.3 Weight Initialiation"></a>2.3 Weight Initialiation</h2><p><strong> 当参数w初始化为0时：  </strong></p>
<p>很多神经元的输出可能趋于一致，梯度值也一致，因此w的更新也一致，这并不是我们希望看到的！</p>
<p><strong> 当参数初始化为一个小数：  </strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918104055176?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">   


<p><strong> 图片依次为：  </strong></p>
<p>每层网络的输出均值，输出方差，输出数据直方图</p>
<p><strong> 当初始化为1附近的值时：  </strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923172046057?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/Center" width="60%"></div></p>
<div align="left">







<p>目前比较理想的初始化算法：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918104544346?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="80%"></div></p>
<div align="left">  



<p><strong> Tips:  </strong></p>
<p>1. fan_in与fan_out指的是输入与输出的数据的数量。</p>
<p>2.以上神经网络采用的激活函数为tanh，当采用ReLU时，初始化公式应变为：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918105137548?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">  

<p>因为一般数据是关于0中心化，而ReLU对小于0的数据设为0，所以要除以2！</p>
<h2 id="2-4-Bath-Normalization"><a href="#2-4-Bath-Normalization" class="headerlink" title="2.4 Bath Normalization"></a>2.4 Bath Normalization</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170918105734430?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="80%"></div></p>
<div align="left">  





<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918105812903?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="40%"></div></p>
<div align="left">  





<p><strong> Tips:  </strong></p>
<p>在训练阶段，β与γ是需要进行训练的参数，而μ与σ是根据训练数据计算得到，两者值不一样！  </p>
<h2 id="2-5-Baby-Setting-the-Learning-Process"><a href="#2-5-Baby-Setting-the-Learning-Process" class="headerlink" title="2.5 Baby Setting the Learning Process"></a>2.5 Baby Setting the Learning Process</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170918111603360?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="80%"></div></p>
<div align="left">







<h2 id="2-6-Hyperparameter-Optimzation"><a href="#2-6-Hyperparameter-Optimzation" class="headerlink" title=" 2.6 Hyperparameter Optimzation  "></a><strong> 2.6 Hyperparameter Optimzation  </strong></h2><p><strong>   
</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918111927295?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%"></div></p>
<p><div align="left"><br>**</div></p>
<p><strong>   
</strong></p>
<p><strong>   
</strong></p>
<p><strong>   
</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918112759495?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"></div></p>
<p><div align="left"><br>**</div></p>
<p><strong>   
</strong></p>
<p><strong>   
</strong></p>
<p><strong> Tips:  </strong></p>
<p>learning rate与regularization系数以log形式搜索，而不是直接按值搜索，这是因为它们更新权重时时乘数！</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918113209290?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="80%"></div></p>
<div align="left">  





<p><strong> Tips：  </strong></p>
<p>这是因为所选的参数搜索范围太小！</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918113403911?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">










<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918113618643?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">  













<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918113653005?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="40%"></div></p>
<div align="left">  





<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918113725864?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="80%"></div></p>
<div align="left">  







<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918113911185?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="80%"></div></p>
<div align="left">  









<p><div align="center"><br><img src="http://img.blog.csdn.net/20170918113843634?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="40%"></div></p>
<div align="left">  







<h1 id="3-Training-Neural-Networks-2"><a href="#3-Training-Neural-Networks-2" class="headerlink" title="3. Training Neural Networks 2"></a>3. Training Neural Networks 2</h1><h2 id="3-1-Fancier-optimization"><a href="#3-1-Fancier-optimization" class="headerlink" title="3.1 Fancier optimization"></a>3.1 Fancier optimization</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170923174503443?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">







<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923174412703?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="80%"></div></p>
<div align="left">

<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923174114024?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">

























<p>初始的SGD算法容易出现锯齿状反复计算的情况，收敛太慢，容易收敛到局部极小值和鞍点（高维出现几率比局部极小值更加频繁），抗噪声能力差！</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923173608231?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast) ![](http://img.blog.csdn.net/20170923174733842?wa
termark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/font
size/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">



<p>Momentum,即动量，可以理解为保留之前的梯度方向和强度作为此次迭代更新的依据！因此，可以有效的避免锯齿状反复计算的情况！</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923175143280?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">

<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923175229949?wa
termark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/font
size/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">  





<p>Nesterov动量法，相比于动量法，可以矫正  之前的梯度对当前参数更新的影响！</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923175551930?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">    







<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923175806546?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<p><div align="left">  </div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923175856203?wa
termark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/font
size/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">    





<p>RMSprop相对于AdaGrad而言，减少了之前梯度对当前参数更新的影响！</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923180226407?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast) ![](http://img.blog.csdn.net/20170923180303038?wa
termark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/font
size/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">    





<p>Adam,融合了  RMSprop和动量法！</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923180857360?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">    



<p>在神经网络的超参数中，学习率最为重要，其次为学习衰减率，因为训练阶段越到后面搜索步长理应越小！右图中的拐点及学习率衰减时出现的情况。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923181032094?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">  

<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923181048654?wa
termark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/font
size/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">    



<p>注：一阶优化和二阶优化的区别示意图</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923181605015?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">  

<p> <div align="center"><br><img src="http://img.blog.csdn.net/20170923181741067?wa
termark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/font
size/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">  





<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923181821253?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">   







<p>二阶牛顿优化法具有没有学习率等超参数等优点，但是要求Hession矩阵的逆矩阵，应用于大型网络计算量太大，一般使用其改进版本：L-BFGS！</p>
<p>总结：</p>
<p>1. Adam是不错的默认参数优化算法！</p>
<p>2. If you can afford to do full batch updates then try out L-BFGS(do not<br>forget to disable all sources of noise).</p>
<h2 id="3-2-Regularization"><a href="#3-2-Regularization" class="headerlink" title="3.2 Regularization"></a>3.2 Regularization</h2><h3 id="3-2-1-Dropout"><a href="#3-2-1-Dropout" class="headerlink" title="3.2.1 Dropout"></a>3.2.1 Dropout</h3><p><strong>   
</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923182844208?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">  



<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923182749197?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">  

<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923182815309?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">  





<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923183019050?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">  





<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923183244839?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">    


<h3 id="3-2-2-训练数据增强"><a href="#3-2-2-训练数据增强" class="headerlink" title="3.2.2 训练数据增强"></a>3.2.2 训练数据增强</h3><p><div align="center"><br><img src="http://img.blog.csdn.net/20170923183606723?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">  

<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923183619617?wa
termark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/font
size/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">  



<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923183631954?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">    







<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923183721697?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">      





<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923184008961?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast) ![](http://img.blog.csdn.net/20170923184024855?wa
termark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/font
size/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">    





<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923183854008?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">     



<p>DropConnect: 随机的让某些神经元的参数w为0</p>
<p>Fractional Max Pooling: 在CNN中的Pooling阶段，随机的对图像中的某些小块进行下采样</p>
<p>Stochastic Depth:<br>对神经网络中的某些神经元添加直接跳转到几级之后的神经元的路径，训练时中间的神经元可能不进行训练，但测试时全部神经元均参加！</p>
<h2 id="3-3-Transfer-Learning"><a href="#3-3-Transfer-Learning" class="headerlink" title="3.3 Transfer Learning"></a>3.3 Transfer Learning</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170923184654076?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">    



<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923184750098?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">      



<p>通常情况，我们可以利用已经经过大量样本（如ImageNet）训练过的部分网络（如CNN层）作为我们的训练网络一部分，在此基础上根据需求进行特化，这就是所谓的迁移学习！</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170923185109436?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">      




















































</div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>]]></content>
      
        <categories>
            
            <category> CS231n </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[CS231n学习笔记--4.Backpropagation and Neural Networks]]></title>
      <url>/2017/09/16/CS231n%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0--4.Backpropagation%20and%20Neural%20Networks/</url>
      <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h1 id="1-损失函数"><a href="#1-损失函数" class="headerlink" title="1.损失函数"></a>1.损失函数</h1><p><div align="center"><br><img src="http://img.blog.csdn.net/20170910110827816?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">

<p>其中SVM损失函数计算的是不正确分类的得分惩罚，即Syi是正确分类结果的得分，Sj是错误分类结果的得分，超参数（1）度量正确分类得分的优越性。</p>
<h2 id="2-简单损失函数流程图："><a href="#2-简单损失函数流程图：" class="headerlink" title="2.简单损失函数流程图："></a>2.简单损失函数流程图：</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170910113644822?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">

<h2 id="3-求解损失函数的梯度矩阵"><a href="#3-求解损失函数的梯度矩阵" class="headerlink" title="3.求解损失函数的梯度矩阵"></a>3.求解损失函数的梯度矩阵</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170910113927597?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="80%"></div></p>
<div align="left">

<p>其中，绿色数值代表前向网络中的实际值，红色数值代表反向神经网络得到的梯度值。</p>
<h2 id="4-常见的激活函数"><a href="#4-常见的激活函数" class="headerlink" title="4.常见的激活函数"></a>4.常见的激活函数</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170910113457935?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">

<p><strong> sigmoid </strong></p>
<p>有点落伍了（fallen out of favor and it is rarely ever used）<br>原因 1：梯度饱和问题（sigmoid saturate and kill gradients），即如果神经元的激活值很大，返回的梯度几乎为零，因此反向传播的时候，也会阻断（or kill）从此处流动的梯度。此外初始化的时候，也要注意，如果梯度很大的话，也很容易造成梯度饱和。<br>原因 2：sigmoid outputs are not zero-centered，因为输出结果在 [0,1]之间，都是整数，所以造成了某些维度一直更新正的梯度，某些则相反。就会造成 zig-zagging形状的参数更新。不过利用批随机梯度下降法就会缓解这个问题，没有第一个问题严重。</p>
<p><strong> tanh </strong></p>
<p>很明显，tanh（non-linearity ）虽然也有梯度饱和问题，但是起码是 zero-centered，因此实际中比 sigmoid 效果更好。</p>
<p><strong> ReLU </strong></p>
<p>好处：能够加速收敛速度，据说是因为线性，非饱和（non-saturating）的形式；运算（oprations）很实现都很简单，不用指数（exponentials）操作。<br>坏处：很脆弱（fragile），容易死掉（die），即如果很大的梯度经过神经元，那么就会造成此神经元不会再对任何数据点有激活。不过学习率设置小一点就不会有太大的问题。</p>
<p><strong> Leaky ReLU </strong></p>
<p>不是单纯的把负数置零，而是加一个很小的 slope，比如 0.01<br>f(x)=1(x&lt;0)(αx)+1(x≥0)(x)。这样做是为了修复 “dying ReLU的 问题。<br>如果 α 作为参数，即每个神经元的 slope 都不一样，如果可以自学习的话，称为 PReLU。</p>
<p><strong> Maxout </strong></p>
<p>每个神经元会有两个权重，激活函数为 max(wT1x+b1,wT2+b2)<br>当 w1=b1=0 时，就是 ReLU .<br>虽然和 ReLU 一样没有梯度饱和问题，也没有 dying ReLU 问题，但是参数确实原来的两倍</p>
<p><strong> 实践中，用 ReLU 较多 </strong> ，学习率要调小一点，如果 dead units 很多的话，用 PReLU 或者 Maxout 试一下。 </p>
<h2 id="5-minbath"><a href="#5-minbath" class="headerlink" title="5.minbath"></a>5.minbath</h2><p>深度学习的优化算法，说白了就是梯度下降。每次的参数更新有两种方式。<br>第一种，遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢<br>，不支持在线学习，这称为Batch gradient descent，批梯度下降。<br>另一种，每看一个数据就算一下损失函数，然后求梯度更新参数，这个称为随机梯度下降，stochastic gradient descent。这个方法速度比较快，<br>但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。<br>为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批<br>来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不<br>是很大。</p>
<h2 id="6-Assinment-1"><a href="#6-Assinment-1" class="headerlink" title="6.Assinment 1:"></a>6.Assinment 1:</h2><p>参考博客： <a href="http://blog.csdn.net/zhangxb35/article/details/55223825" target="_blank" rel="external"> cs231n 课程作业 Assignment 1
</a></p>
<p><strong> KNN分类器 </strong></p>
<pre><code>import numpy as np

class KNearestNeighbor(object):
  &quot;&quot;&quot; a kNN classifier with L2 distance &quot;&quot;&quot;

  def __init__(self):
    pass

  def train(self, X, y):
    &quot;&quot;&quot;
    Train the classifier. For k-nearest neighbors this is just
    memorizing the training data.
    Inputs:
    - X: A numpy array of shape (num_train, D) containing the training data
      consisting of num_train samples each of dimension D.
    - y: A numpy array of shape (N,) containing the training labels, where
         y[i] is the label for X[i].
    &quot;&quot;&quot;
    self.X_train = X
    self.y_train = y

  def predict(self, X, k=1, num_loops=0):
    &quot;&quot;&quot;
    Predict labels for test data using this classifier.
    Inputs:
    - X: A numpy array of shape (num_test, D) containing test data consisting
         of num_test samples each of dimension D.
    - k: The number of nearest neighbors that vote for the predicted labels.
    - num_loops: Determines which implementation to use to compute distances
      between training points and testing points.
    Returns:
    - y: A numpy array of shape (num_test,) containing predicted labels for the
      test data, where y[i] is the predicted label for the test point X[i].
    &quot;&quot;&quot;
    if num_loops == 0:
      dists = self.compute_distances_no_loops(X)
    elif num_loops == 1:
      dists = self.compute_distances_one_loop(X)
    elif num_loops == 2:
      dists = self.compute_distances_two_loops(X)
    else:
      raise ValueError(&apos;Invalid value %d for num_loops&apos; % num_loops)

    return self.predict_labels(dists, k=k)

  def compute_distances_two_loops(self, X):
    &quot;&quot;&quot;
    Compute the distance between each test point in X and each training point
    in self.X_train using a nested loop over both the training data and the
    test data.
    Inputs:
    - X: A numpy array of shape (num_test, D) containing test data.
    Returns:
    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]
      is the Euclidean distance between the ith test point and the jth training
      point.
    &quot;&quot;&quot;
    num_test = X.shape[0]
    num_train = self.X_train.shape[0]
    dists = np.zeros((num_test, num_train))
    for i in xrange(num_test):
      for j in xrange(num_train):
        #####################################################################
        # TODO:                                                             #
        # Compute the l2 distance between the ith test point and the jth    #
        # training point, and store the result in dists[i, j]. You should   #
        # not use a loop over dimension.                                    #
        #####################################################################
        pass
        dists[i][j] = np.sum((X[i] - self.X_train[j]) ** 2)
        #####################################################################
        #                       END OF YOUR CODE                            #
        #####################################################################
    return dists

  def compute_distances_one_loop(self, X):
    &quot;&quot;&quot;
    Compute the distance between each test point in X and each training point
    in self.X_train using a single loop over the test data.
    Input / Output: Same as compute_distances_two_loops
    &quot;&quot;&quot;
    num_test = X.shape[0]
    num_train = self.X_train.shape[0]
    dists = np.zeros((num_test, num_train))
    for i in xrange(num_test):
      #######################################################################
      # TODO:                                                               #
      # Compute the l2 distance between the ith test point and all training #
      # points, and store the result in dists[i, :].                        #
      #######################################################################
      pass
      dists[i] = np.sum((self.X_train - X[i]) ** 2, 1)
      #######################################################################
      #                         END OF YOUR CODE                            #
      #######################################################################
    return dists

  def compute_distances_no_loops(self, X):
    &quot;&quot;&quot;
    Compute the distance between each test point in X and each training point
    in self.X_train using no explicit loops.
    Input / Output: Same as compute_distances_two_loops
    &quot;&quot;&quot;
    num_test = X.shape[0]
    num_train = self.X_train.shape[0]
    dists = np.zeros((num_test, num_train))
    #########################################################################
    # TODO:                                                                 #
    # Compute the l2 distance between all test points and all training      #
    # points without using any explicit loops, and store the result in      #
    # dists.                                                                #
    #                                                                       #
    # You should implement this function using only basic array operations; #
    # in particular you should not use functions from scipy.                #
    #                                                                       #
    # HINT: Try to formulate the l2 distance using matrix multiplication    #
    #       and two broadcast sums.                                         #
    #########################################################################
    pass
    dists += np.sum(self.X_train ** 2, axis=1).reshape(1, num_train)
    dists += np.sum(X ** 2, axis=1).reshape(num_test, 1) # reshape for broadcasting
    dists -= 2 * np.dot(X, self.X_train.T)
    #########################################################################
    #                         END OF YOUR CODE                              #
    #########################################################################
    return dists

  def predict_labels(self, dists, k=1):
    &quot;&quot;&quot;
    Given a matrix of distances between test points and training points,
    predict a label for each test point.
    Inputs:
    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]
      gives the distance betwen the ith test point and the jth training point.
    Returns:
    - y: A numpy array of shape (num_test,) containing predicted labels for the
      test data, where y[i] is the predicted label for the test point X[i].
    &quot;&quot;&quot;
    num_test = dists.shape[0]
    y_pred = np.zeros(num_test)
    for i in xrange(num_test):
      # A list of length k storing the labels of the k nearest neighbors to
      # the ith test point.
      closest_y = []
      #########################################################################
      # TODO:                                                                 #
      # Use the distance matrix to find the k nearest neighbors of the ith    #
      # testing point, and use self.y_train to find the labels of these       #
      # neighbors. Store these labels in closest_y.                           #
      # Hint: Look up the function numpy.argsort.                             #
      #########################################################################
      pass
      closest_y = self.y_train[np.argsort(dists[i])[0:k]]
      #########################################################################
      # TODO:                                                                 #
      # Now that you have found the labels of the k nearest neighbors, you    #
      # need to find the most common label in the list closest_y of labels.   #
      # Store this label in y_pred[i]. Break ties by choosing the smaller     #
      # label.                                                                #
      #########################################################################
      pass
      # to find the most common element in list, you can use np.bincount
      y_pred[i] = np.bincount(closest_y).argmax()
      #########################################################################
      #                           END OF YOUR CODE                            #
      #########################################################################

    return y_pred
</code></pre><p><strong> 前向/后向神经网络 </strong></p>
<p><strong> a.损失函数为SVM： </strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170910164448168?watermark/2/text/aHR0
cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQ
kFCMA==/dissolve/70/gravity/SouthEast" width="80%"></div></p>
<div align="left">

<p><strong> Python代码： </strong></p>
<pre><code>import numpy as np
#from random import shuffle

def svm_loss_naive(W, X, y, reg):
  &quot;&quot;&quot;
  Structured SVM loss function, naive implementation (with loops).
  Inputs have dimension D, there are C classes, and we operate on minibatches
  of N examples.
  Inputs:
  - W: A numpy array of shape (D, C) containing weights.
  - X: A numpy array of shape (N, D) containing a minibatch of data.
  - y: A numpy array of shape (N,) containing training labels; y[i] = c means
    that X[i] has label c, where 0 &lt;= c &lt; C.
  - reg: (float) regularization strength
  Returns a tuple of:
  - loss as single float
  - gradient with respect to weights W; an array of same shape as W
  &quot;&quot;&quot;
  dW = np.zeros(W.shape) # initialize the gradient as zero

  # compute the loss and the gradient
  num_classes = W.shape[1]
  num_train = X.shape[0]
  loss = 0.0
  for i in xrange(num_train):
    scores = X[i].dot(W)
    correct_class_score = scores[y[i]]
    for j in xrange(num_classes):
      if j == y[i]:
        continue
      margin = scores[j] - correct_class_score + 1 # note delta = 1
      if margin &gt; 0:
        loss += margin
        dW[:, j] += X[i]
        dW[:, y[i]] -= X[i]

  # Right now the loss is a sum over all training examples, but we want it
  # to be an average instead so we divide by num_train.
  loss /= num_train
  dW /= num_train

  # Add regularization to the loss.
  loss += 0.5 * reg * np.sum(W * W)
  dW += reg * W

  #############################################################################
  # TODO:                                                                     #
  # Compute the gradient of the loss function and store it dW.                #
  # Rather that first computing the loss and then computing the derivative,   #
  # it may be simpler to compute the derivative at the same time that the     #
  # loss is being computed. As a result you may need to modify some of the    #
  # code above to compute the gradient.                                       #
  #############################################################################


  return loss, dW


def svm_loss_vectorized(W, X, y, reg):
  &quot;&quot;&quot;
  Structured SVM loss function, vectorized implementation.
  Inputs and outputs are the same as svm_loss_naive.
  &quot;&quot;&quot;
  loss = 0.0
  dW = np.zeros(W.shape) # initialize the gradient as zero

  #############################################################################
  # TODO:                                                                     #
  # Implement a vectorized version of the structured SVM loss, storing the    #
  # result in loss.                                                           #
  #############################################################################
  pass
  N = X.shape[0]
  #scores = np.dot(X, W)
  #margin = scores - scores[range(0, N), y].reshape(N, 1) + 1
  #margin[range(0, N), y] = 0
  #margin = margin * (margin &gt; 0) # max(0, s_j - s_yi + delta)
  #loss += np.sum(margin) / N + 0.5 * reg * np.sum(W * W)
  scores = X.dot(W) # N x C
  margin = scores - scores[range(0,N), y].reshape(-1, 1) + 1 # N x C
  margin[range(N), y] = 0
  margin = (margin &gt; 0) * margin
  loss += margin.sum() / N
  loss += 0.5 * reg * np.sum(W * W)
  #############################################################################
  #                             END OF YOUR CODE                              #
  #############################################################################


  #############################################################################
  # TODO:                                                                     #
  # Implement a vectorized version of the gradient for the structured SVM     #
  # loss, storing the result in dW.                                           #
  #                                                                           #
  # Hint: Instead of computing the gradient from scratch, it may be easier    #
  # to reuse some of the intermediate values that you used to compute the     #
  # loss.                                                                     #
  #############################################################################
  pass
  counts = (margin &gt; 0).astype(int)
  counts[range(N), y] = - np.sum(counts, axis = 1)
  dW += np.dot(X.T, counts) / N + reg * W
  #############################################################################
  #                             END OF YOUR CODE                              #
  #############################################################################

  return loss, dW
</code></pre><p><strong> b.损失函数为Softmax： </strong><br>（不同之处仅在损失函数及其梯度计算部分）：  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170910165843621?watermark/2/text/a
HR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0
JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%"></div></p>
<div align="left">

<p><strong> python代码： </strong></p>
<pre><code>import numpy as np
#from random import shuffle

def softmax_loss_naive(W, X, y, reg):
  &quot;&quot;&quot;
  Softmax loss function, naive implementation (with loops)

  Inputs have dimension D, there are C classes, and we operate on minibatches
  of N examples.

  Inputs:
  - W: A numpy array of shape (D, C) containing weights.
  - X: A numpy array of shape (N, D) containing a minibatch of data.
  - y: A numpy array of shape (N,) containing training labels; y[i] = c means
    that X[i] has label c, where 0 &lt;= c &lt; C.
  - reg: (float) regularization strength

  Returns a tuple of:
  - loss as single float
  - gradient with respect to weights W; an array of same shape as W
  &quot;&quot;&quot;
  # Initialize the loss and gradient to zero.
  loss = 0.0
  dW = np.zeros_like(W)

  #############################################################################
  # TODO: Compute the softmax loss and its gradient using explicit loops.     #
  # Store the loss in loss and the gradient in dW. If you are not careful     #
  # here, it is easy to run into numeric instability. Don&apos;t forget the        #
  # regularization!                                                           #
  #############################################################################
  pass
  N, C = X.shape[0], W.shape[1]
  for i in range(N):
      f = np.dot(X[i], W)
      f -= np.max(f) # f.shape = C
      loss = loss + np.log(np.sum(np.exp(f))) - f[y[i]]
      dW[:, y[i]] -= X[i]
      s = np.exp(f).sum()
      for j in range(C):
          dW[:, j] += np.exp(f[j]) / s * X[i]
  loss = loss / N + 0.5 * reg * np.sum(W * W)
  dW = dW / N + reg * W
  #############################################################################
  #                          END OF YOUR CODE                                 #
  #############################################################################

  return loss, dW


def softmax_loss_vectorized(W, X, y, reg):
  &quot;&quot;&quot;
  Softmax loss function, vectorized version.

  Inputs and outputs are the same as softmax_loss_naive.
  &quot;&quot;&quot;
  # Initialize the loss and gradient to zero.
  loss = 0.0
  dW = np.zeros_like(W)

  #############################################################################
  # TODO: Compute the softmax loss and its gradient using no explicit loops.  #
  # Store the loss in loss and the gradient in dW. If you are not careful     #
  # here, it is easy to run into numeric instability. Don&apos;t forget the        #
  # regularization!                                                           #
  #############################################################################
  pass
  N = X.shape[0]
  f = np.dot(X, W) # f.shape = N, C
  f -= f.max(axis = 1).reshape(N, 1)
  s = np.exp(f).sum(axis = 1)
  loss = np.log(s).sum() - f[range(N), y].sum()

  counts = np.exp(f) / s.reshape(N, 1)
  counts[range(N), y] -= 1
  dW = np.dot(X.T, counts)

  loss = loss / N + 0.5 * reg * np.sum(W * W)
  dW = dW / N + reg * W
  #############################################################################
  #                          END OF YOUR CODE                                 #
  #############################################################################

  return loss, dW
</code></pre><p><strong> 简单的两层网络： </strong></p>
<p>步骤：<br>1。设置learning_rate,<br>learning_rate_decay（每个epoch（数据集）训练后learning_rate下降的倍率，因为越训练到后面学习率应该越低，以防止震荡）,<br>num_iters,batch_size。<br>2。根据随机梯度下降法随机从epoch中抽取batch_size个训练样本。<br>3。根据batch_size个训练样本计算各中间变量（w,b…）的梯度，并根据学习率更新这些中间变量。<br>4。如果一个epoch训练结束，更新学习率进行下一个epoch的训练。</p>
<pre><code>import numpy as np
#import matplotlib.pyplot as plt


class TwoLayerNet(object):
  &quot;&quot;&quot;
  A two-layer fully-connected neural network. The net has an input dimension of
  N, a hidden layer dimension of H, and performs classification over C classes.
  We train the network with a softmax loss function and L2 regularization on the
  weight matrices. The network uses a ReLU nonlinearity after the first fully
  connected layer.
  In other words, the network has the following architecture:
  input - fully connected layer - ReLU - fully connected layer - softmax
  The outputs of the second fully-connected layer are the scores for each class.
  &quot;&quot;&quot;

  def __init__(self, input_size, hidden_size, output_size, std=1e-4):
    &quot;&quot;&quot;
    Initialize the model. Weights are initialized to small random values and
    biases are initialized to zero. Weights and biases are stored in the
    variable self.params, which is a dictionary with the following keys:
    W1: First layer weights; has shape (D, H)
    b1: First layer biases; has shape (H,)
    W2: Second layer weights; has shape (H, C)
    b2: Second layer biases; has shape (C,)
    Inputs:
    - input_size: The dimension D of the input data.
    - hidden_size: The number of neurons H in the hidden layer.
    - output_size: The number of classes C.
    &quot;&quot;&quot;
    self.params = {}
    self.params[&apos;W1&apos;] = std * np.random.randn(input_size, hidden_size)
    self.params[&apos;b1&apos;] = np.zeros(hidden_size)
    self.params[&apos;W2&apos;] = std * np.random.randn(hidden_size, output_size)
    self.params[&apos;b2&apos;] = np.zeros(output_size)

  def loss(self, X, y=None, reg=0.0):
    &quot;&quot;&quot;
    Compute the loss and gradients for a two layer fully connected neural
    network.
    Inputs:
    - X: Input data of shape (N, D). Each X[i] is a training sample.
    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is
      an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it
      is not passed then we only return scores, and if it is passed then we
      instead return the loss and gradients.
    - reg: Regularization strength.
    Returns:
    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is
    the score for class c on input X[i].
    If y is not None, instead return a tuple of:
    - loss: Loss (data loss and regularization loss) for this batch of training
      samples.
    - grads: Dictionary mapping parameter names to gradients of those parameters
      with respect to the loss function; has the same keys as self.params.
    &quot;&quot;&quot;
    # Unpack variables from the params dictionary
    W1, b1 = self.params[&apos;W1&apos;], self.params[&apos;b1&apos;]
    W2, b2 = self.params[&apos;W2&apos;], self.params[&apos;b2&apos;]
    N, D = X.shape

    # Compute the forward pass
    scores = None
    #############################################################################
    # TODO: Perform the forward pass, computing the class scores for the input. #
    # Store the result in the scores variable, which should be an array of      #
    # shape (N, C).                                                             #
    #############################################################################
    pass
    hidden_layer = np.maximum(0, np.dot(X, W1) + b1)
    scores = np.dot(hidden_layer, W2) + b2
    #############################################################################
    #                              END OF YOUR CODE                             #
    #############################################################################

    # If the targets are not given then jump out, we&apos;re done
    if y is None:
      return scores

    # Compute the loss
    loss = None
    #############################################################################
    # TODO: Finish the forward pass, and compute the loss. This should include  #
    # both the data loss and L2 regularization for W1 and W2. Store the result  #
    # in the variable loss, which should be a scalar. Use the Softmax           #
    # classifier loss. So that your results match ours, multiply the            #
    # regularization loss by 0.5                                                #
    #############################################################################
    pass
    f = scores - np.max(scores, axis = 1, keepdims = True)#负值化
    loss = -f[range(N), y].sum() + np.log(np.exp(f).sum(axis = 1)).sum()
    loss = loss / N + 0.5 * reg * (np.sum(W1 * W1) + np.sum(W2 * W2))
    #############################################################################
    #                              END OF YOUR CODE                             #
    #############################################################################

    # Backward pass: compute gradients
    grads = {}
    #############################################################################
    # TODO: Compute the backward pass, computing the derivatives of the weights #
    # and biases. Store the results in the grads dictionary. For example,       #
    # grads[&apos;W1&apos;] should store the gradient on W1, and be a matrix of same size #
    #############################################################################
    pass
    dscore = np.exp(f) / np.exp(f).sum(axis = 1, keepdims = True)
    dscore[range(N), y] -= 1
    dscore /= N
    grads[&apos;W2&apos;] = np.dot(hidden_layer.T, dscore) + reg * W2
    grads[&apos;b2&apos;] = np.sum(dscore, axis = 0)

    dhidden = np.dot(dscore, W2.T)
    dhidden[hidden_layer &lt;= 0.00001] = 0

    grads[&apos;W1&apos;] = np.dot(X.T, dhidden) + reg * W1
    grads[&apos;b1&apos;] = np.sum(dhidden, axis = 0)
    #############################################################################
    #                              END OF YOUR CODE                             #
    #############################################################################

    return loss, grads

  def train(self, X, y, X_val, y_val,
            learning_rate=1e-3, learning_rate_decay=0.95,
            reg=1e-5, num_iters=100,
            batch_size=200, verbose=False):
    &quot;&quot;&quot;
    Train this neural network using stochastic gradient descent.
    Inputs:
    - X: A numpy array of shape (N, D) giving training data.
    - y: A numpy array f shape (N,) giving training labels; y[i] = c means that
      X[i] has label c, where 0 &lt;= c &lt; C.
    - X_val: A numpy array of shape (N_val, D) giving validation data.
    - y_val: A numpy array of shape (N_val,) giving validation labels.
    - learning_rate: Scalar giving learning rate for optimization.
    - learning_rate_decay: Scalar giving factor used to decay the learning rate
      after each epoch.
    - reg: Scalar giving regularization strength.
    - num_iters: Number of steps to take when optimizing.
    - batch_size: Number of training examples to use per step.
    - verbose: boolean; if true print progress during optimization.
    &quot;&quot;&quot;
    num_train = X.shape[0]
    iterations_per_epoch = max(num_train / batch_size, 1)

    # Use SGD to optimize the parameters in self.model
    loss_history = []
    train_acc_history = []
    val_acc_history = []

    for it in xrange(num_iters):
      X_batch = None
      y_batch = None

      #########################################################################
      # TODO: Create a random minibatch of training data and labels, storing  #
      # them in X_batch and y_batch respectively.                             #
      #########################################################################
      pass
      indices = np.random.choice(num_train, batch_size, replace=True)
      X_batch = X[indices]
      y_batch = y[indices]
      #########################################################################
      #                             END OF YOUR CODE                          #
      #########################################################################

      # Compute loss and gradients using the current minibatch
      loss, grads = self.loss(X_batch, y=y_batch, reg=reg)
      loss_history.append(loss)

      #########################################################################
      # TODO: Use the gradients in the grads dictionary to update the         #
      # parameters of the network (stored in the dictionary self.params)      #
      # using stochastic gradient descent. You&apos;ll need to use the gradients   #
      # stored in the grads dictionary defined above.                         #
      #########################################################################
      pass
      self.params[&apos;W1&apos;] -= learning_rate * grads[&apos;W1&apos;]
      self.params[&apos;b1&apos;] -= learning_rate * grads[&apos;b1&apos;]
      self.params[&apos;W2&apos;] -= learning_rate * grads[&apos;W2&apos;]
      self.params[&apos;b2&apos;] -= learning_rate * grads[&apos;b2&apos;]
      #########################################################################
      #                             END OF YOUR CODE                          #
      #########################################################################

      if verbose and it % 100 == 0:
        print &apos;iteration %d / %d: loss %f&apos; % (it, num_iters, loss)

      # Every epoch, check train and val accuracy and decay learning rate.
      if it % iterations_per_epoch == 0:
        # Check accuracy
        train_acc = (self.predict(X_batch) == y_batch).mean()
        val_acc = (self.predict(X_val) == y_val).mean()
        train_acc_history.append(train_acc)
        val_acc_history.append(val_acc)

        # Decay learning rate
        learning_rate *= learning_rate_decay

    return {
      &apos;loss_history&apos;: loss_history,
      &apos;train_acc_history&apos;: train_acc_history,
      &apos;val_acc_history&apos;: val_acc_history,
    }

  def predict(self, X):
    &quot;&quot;&quot;
    Use the trained weights of this two-layer network to predict labels for
    data points. For each data point we predict scores for each of the C
    classes, and assign each data point to the class with the highest score.
    Inputs:
    - X: A numpy array of shape (N, D) giving N D-dimensional data points to
      classify.
    Returns:
    - y_pred: A numpy array of shape (N,) giving predicted labels for each of
      the elements of X. For all i, y_pred[i] = c means that X[i] is predicted
      to have class c, where 0 &lt;= c &lt; C.
    &quot;&quot;&quot;
    y_pred = None

    ###########################################################################
    # TODO: Implement this function; it should be VERY simple!                #
    ###########################################################################
    pass
    W1, b1 = self.params[&apos;W1&apos;], self.params[&apos;b1&apos;]
    W2, b2 = self.params[&apos;W2&apos;], self.params[&apos;b2&apos;]

    hidden_layer = np.maximum(0, np.dot(X, W1) + b1)
    scores = np.dot(hidden_layer, W2) + b2
    y_pred = np.argmax(scores, axis = 1)
    ###########################################################################
    #                              END OF YOUR CODE                           #
    ###########################################################################

    return y_pred
</code></pre></div></div></div></div></div></div>]]></content>
      
        <categories>
            
            <category> CS231n </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Deep Learning读书笔记8--应用]]></title>
      <url>/2017/09/10/Deep%20Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B08--%E5%BA%94%E7%94%A8/</url>
      <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h2 id="1-大规模深度学习"><a href="#1-大规模深度学习" class="headerlink" title="1. 大规模深度学习"></a>1. 大规模深度学习</h2><p>深度学习的基本思想基于联结主义：尽管机器学习模型中单个生物性的神经元或者说是单个特征不是智能的，但是大量的神经元或者特征作用在一起往往能够表现出智能。 </p>
<p>在训练过程中，数据并行某种程度上来说更加困难。 对于随机梯度下降的单步来说，我们可以增加小批量的大小，但是从优化性能的角度来说，我们得到的回报通常并不会线性增长。 使用多个机器并行地计算多个梯度下降步骤是一个更好的选择。 不幸的是，梯度下降的标准定义完全是一个串行的过程： 第$$t$$步的梯度是第$$t-1$$步所得参数的函数。<br>这个问题可以使用<strong>异步随机梯度下降</strong>解决。 在这个方法中，几个处理器的核共用存有参数的内存。 每一个核在<strong>无锁</strong>情况下读取这些参数并计算对应的梯度，然后在无锁状态下更新这些参数。 由于一些核把其他的核所更新的参数覆盖了，因此这种方法减少了每一步梯度下降所获得的平均提升。 但因为更新步数的速率增加，总体上还是加快了学习过程。 </p>
<p><strong>模型压缩</strong></p>
<p>模型压缩的基本思想是用一个更小的模型取代原始耗时的模型，从而使得用来存储与评估所需的内存与运行时间更少。</p>
<p>巨大的模型能够学习到某个函数$$f(x)$$，但选用的参数数量超过了任务所需的参数数量。 只是因为训练样本数是有限的，所以模型的规模才变得必要。 只要我们拟合了函数$$f(x)$$，我们就可以通过将$$f$$作用于随机采样点$$x$$来生成有无穷多训练样本的训练集。 然后，我们使用这些样本训练一个新的更小的模型，使其能够在这些点上拟合$$f(x)$$。 为了更加充分地利用了这个新的小模型的容量，最好从类似于真实测试数据（之后将提供给模型）的分布中采样$$x$$。 这个过程可以通过损坏训练样本或者从原始训练数据训练的生成模型中采样完成。</p>
<p><strong>动态结构</strong></p>
<p>一般来说，加速数据处理系统的一种策略是构造一个系统，这个系统用动态结构描述图中处理输入的所需计算过程。 在给定一个输入的情况中，数据处理系统可以动态地决定运行神经网络系统的哪一部分。 单个神经网络内部同样也存在动态结构，给定输入信息，决定特征（隐藏单元）哪一部分用于计算。 这种神经网络中的动态结构有时被称为条件计算。 由于模型结构许多部分可能只跟输入的一小部分有关，只计算那些需要的特征可以起到加速的目的。</p>
<h2 id="2-计算机视觉"><a href="#2-计算机视觉" class="headerlink" title="2. 计算机视觉"></a>2. 计算机视觉</h2><p><strong>预处理：对比度归一化</strong></p>
<p>对比度指的是图像中亮像素和暗像素之间差异的大小。  在深度学习中，对比度通常指的是图像或图像区域中像素的标准差。 假设我们有一个张量表示的图像$$X \in R^{r\times c\times 3}$$，其中$$X<em>{i,j,1}$$表示第$$i$$行第$$j$$列红色的强度，$$X</em>{i,j,2}$$对应的是绿色的强度，$$X_{i,j,3}$$对应的是蓝色的强度。 然后整个图像的对比度可以表示如下：</p>
<p>$$\begin{align}  \sqrt{ {\frac{1}{3rc}\sum<em>{i=1}^{r}\sum</em>{j=1}^{c}\sum<em>{k=1}^{3}} (X</em>{i,j,k} - \bar{X} )^2} . \end{align}$$</p>
<p>其中$$\bar{X}$$是整个图片的平均强度，满足</p>
<p>$$\begin{align}  \bar{X} = {\frac{1}{3rc}\sum<em>{i=1}^{r}\sum</em>{j=1}^{c}\sum<em>{k=1}^{3}} (X</em>{i,j,k} ). \end{align}$$</p>
<p>全局对比度归一化旨在通过从每个图像中减去其平均值，然后重新缩放其使得其像素上的标准差等于某个常数$$s$$来防止图像具有变化的对比度。 这种方法非常复杂，因为没有缩放因子可以改变零对比度图像（所有像素都具有相等强度的图像）的对比度。 具有非常低但非零对比度的图像通常几乎没有信息内容。 在这种情况下除以真实标准差通常仅能放大传感器噪声或压缩伪像。 这种现象启发我们引入一个小的正的正则化参数$$\lambda$$来平衡估计的标准差。 或者，我们至少可以约束分母使其大于等于$$\epsilon$$。 给定一个输入图像$$X$$，全局对比度归一化产生输出图像$$X’$$，定义为 </p>
<p>$$\begin{align} X’{i,j,k} = s\frac{X{i,j,k} - \bar{X}}{\max {（{ \epsilon, \sqrt{ \lambda + {\frac{1}{3rc}\sum<em>{i=1}^{r}\sum</em>{j=1}^{c}\sum<em>{k=1}^{3}} (X</em>{i,j,k} - \bar{X} )^2} }）}}. \end{align}$$</p>
<p>从大图像中剪切感兴趣的对象所组成的数据集不可能包含任何强度几乎恒定的图像。 在这些情况下，通过设置$$\lambda = 0$$来忽略小分母问题是安全的，并且在非常罕见的情况下为了避免除以$$0$$，通过将$$\epsilon$$设置为一个非常小的值比如说$$10^{-8}$$。<br>尺度参数$$s$$通常可以设置为$$1$$，或选择使所有样本上每个像素的标准差接近$$1$$。</p>
<p>全局对比度归一化常常不能突出我们想要突出的图像特征，例如边缘和角。 如果我们有一个场景，包含了一个大的黑暗区域和一个大的明亮的区域（例如一个城市广场有一半的区域处于建筑物的阴影之中）， 则全局对比度归一化将确保暗区域的亮度与亮区域的亮度之间存在大的差异。 然而，它不能确保暗区内的边缘突出。</p>
<p>局部对比度归一化确保对比度在每个小窗口上被归一化，而不是作为整体在图像上被归一化。<br>我们可以通过减去邻近像素的平均值并除以邻近像素的标准差来修改每个像素。 在一些情况下，要计算以当前要修改的像素为中心的矩形窗口中所有像素的平均值和标准差。 在其他情况下，使用的则是以要修改的像素为中心的高斯权重的加权平均和加权标准差。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171018102431153?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left">  <div align="left"> </div></div></div></p>
<h2 id="3-语音识别"><a href="#3-语音识别" class="headerlink" title="3. 语音识别"></a>3. 语音识别</h2><p>语音识别任务在于将一段包括了自然语言发音的声学信号投影到对应说话人的词序列上。 令$$X=(x^{(1)},x^{(2)},\ldots,x^{(T)})$$表示语音的输入向量（传统做法以$$20$$ms为一帧分割信号）。 许多语音识别的系统通过特殊的手工设计方法预处理输入信号，从而提取特征，但是某些深度学习系统直接从原始输入中学习特征。 令$$y=(y<em>{1},y</em>{2},\ldots,y<em>{N})$$表示目标的输出序列（通常是一个词或者字符的序列）。 自动语音识别任务指的是构造一个函数$$f^*</em>{ASR}$$，使得它能够在给定声学序列$$X$$的情况下计算最有可能的语言序列$$y$$：<br>$$\begin{align} f^<em>_{ASR}(X) = \underset{y}{\arg\max} P^</em>(y \mid X = X), \end{align}$$<br> 其中$$P^*$$是给定输入值$$X$$时对应目标$$y$$的真实条件分布。</p>
<p>从20世纪80年代直到约2009-2012年，最先进的语音识别系统是隐马尔可夫模型和高斯混合模型的结合。 GMM对声学特征和音素之间的关系建模，HMM对音素序列建模。 GMM-HMM模型将语音信号视作由如下过程生成：首先，一个HMM生成了一个音素的序列以及离散的子音素状态（比如每一个音素的开始，中间，结尾），然后GMM把每一个离散的状态转化为一个简短的声音信号。<br>之后，随着更大更深的模型以及更大的数据集的出现，通过使用神经网络代替GMM来实现将声学特征转化为音素（或者子音素状态）的过程可以大大地提高识别的精度。<br>一个重要的创新是，卷积网络在时域与频域上复用了权重，改进了之前的仅在时域上使用重复权值的时延神经网络。 这种新的二维的卷积模型并不是将输入的频谱当作一个长的向量，而是当成是一个图像，其中一个轴对应着时间，另一个轴对应的是谱分量的频率。</p>
<h2 id="4-自然语言处理"><a href="#4-自然语言处理" class="headerlink" title="4. 自然语言处理"></a>4. 自然语言处理</h2><p>自然语言处理让计算机能够使用人类语言，例如英语或法语。 为了让简单的程序能够高效明确地解析，计算机程序通常读取和发出特殊化的语言。 而自然的语言通常是模糊的，并且可能不遵循形式的描述。 自然语言处理中的应用如机器翻译，学习者需要读取一种人类语言的句子，并用另一种人类语言发出等同的句子。 许多NLP应用程序基于语言模型，语言模型定义了关于自然语言中的字、字符或字节序列的概率分布。</p>
<h2 id="5-其他应用"><a href="#5-其他应用" class="headerlink" title="5. 其他应用"></a>5. 其他应用</h2><p><strong>推荐系统</strong></p>
<p>早期推荐系统的工作依赖于这些预测输入的最小信息：用户ID和项目ID。 在这种情况下，唯一的泛化方式依赖于不同用户或不同项目的目标变量值之间的模式相似性。 假设用户1和用户2都喜欢项目A，B和C. 由此，我们可以推断出用户1和用户2具有类似的口味。 如果用户1喜欢项目D，那么这可以强烈提示用户2也喜欢D。 基于此原理的算法称为<strong>协同过滤</strong>。 </p>
<p>协同过滤系统有一个基本限制：当引入新项目或新用户时，缺乏评级历史意味着无法评估其与其他项目或用户的相似性，或者说无法评估新的用户和现有项目的联系。 这被称为冷启动推荐问题。 解决冷启动推荐问题的一般方式是引入单个用户和项目的额外信息。 例如，该额外信息可以是用户简要信息或每个项目的特征。 使用这种信息的系统被称为<strong>基于内容的推荐系统</strong>。 从丰富的用户特征或项目特征集到嵌入的映射可以通过深度学习架构学习。</p>
<p><strong>知识表示、推理和回答</strong></p>
<p>因为使用符号和词嵌入，深度学习方法在语言模型、机器翻译和自然语言处理方面非常成功。 这些嵌入表示关于单个词或概念的语义知识。 研究前沿是为短语或词和事实之间的关系开发嵌入。 搜索引擎已经使用机器学习来实现这一目的，但是要改进这些更高级的表示还有许多工作要做。<br>知识、联系和回答 一个有趣的研究方向是确定如何训练分布式表示才能捕获两个实体之间的关系。</p>
]]></content>
      
        <categories>
            
            <category> Deep Learning Book </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Deep Learning读书笔记7--实践方法论]]></title>
      <url>/2017/09/03/Deep%20Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B07--%E5%AE%9E%E8%B7%B5%E6%96%B9%E6%B3%95%E8%AE%BA/</url>
      <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h2 id="1-性能度量"><a href="#1-性能度量" class="headerlink" title="1. 性能度量"></a>1. 性能度量</h2><p>精度是模型报告的检测是正确的比率，而召回率则是真实事件被检测到的比率。 检测器永远报告没有患者，会得到一个完美的精度，但召回率为零。 而报告每个人都是患者的检测器会得到一个完美的召回率，但是精度会等于人群中患有该病的比例（在我们的例子是$$0.0001\%$$，每一百万人只有一人患病）。 当使用精度和召回率时，我们通常会画PR曲线，$$y$$轴表示精度，$$x$$轴表示召回率。 如果检测到的事件发生了，那么分类器会返回一个较高的得分。 例如，我们将前馈网络设计为检测一种疾病，估计一个医疗结果由特征$$x$$表示的人患病的概率为$$\hat{y} = P(y=1\mid x)$$。 每当这个得分超过某个阈值时，我们报告检测结果。 通过调整阈值，我们能权衡精度和召回率。 在很多情况下，我们希望用一个数而不是曲线来概括分类器的性能。 要做到这一点，我们可以将精度 $$p$$和召回率 $$r$$转换为F分数$$F=2pr/(p+r)$$，另一种方法是报告\,PR曲线下方的总面积。</p>
<h2 id="2-默认的基准模型"><a href="#2-默认的基准模型" class="headerlink" title="2. 默认的基准模型"></a>2. 默认的基准模型</h2><p>首先，根据数据的结构选择一类合适的模型。 如果项目是以固定大小的向量作为输入的监督学习，那么可以使用全连接的前馈网络。 如果输入有已知的拓扑结构（例如，输入是图像），那么可以使用卷积网络。 在这些情况下，刚开始可以使用某些分段线性单元（ReLU\,或者其扩展，如\,Leaky ReLU、PReLU\,和\,maxout）。 如果输入或输出是一个序列，可以使用门控循环网络（LSTM\,或\,GRU）。<br>具有衰减学习率以及动量的SGD是优化算法一个合理的选择 （流行的衰减方法有，衰减到固定最低学习率的线性衰减、指数衰减，或每次发生验证错误停滞时将学习率降低$$2-10$$倍，这些衰减方法在不同问题上好坏不一）。 另一个非常合理的选择是Adam算法。 批标准化对优化性能有着显著的影响，特别是对卷积网络和具有sigmoid非线性函数的网络而言。 虽然在最初的基准中忽略批标准化是合理的，然而当优化似乎出现问题时，应该立刻使用批标准化。<br>除非训练集包含数千万以及更多的样本，否则项目应该在一开始就包含一些温和的正则化。 提前终止也被普遍采用。 Dropout也是一个很容易实现，且兼容很多模型和训练算法的出色正则化项。 批标准化有时也能降低泛化误差，此时可以省略Dropout步骤，因为用于标准化变量的统计量估计本身就存在噪声。 </p>
<h2 id="3-决定是否收集更多数据"><a href="#3-决定是否收集更多数据" class="headerlink" title="3. 决定是否收集更多数据"></a>3. 决定是否收集更多数据</h2><p>首先，确定训练集上的性能是否可接受。 如果模型在训练集上的性能就很差，学习算法都不能在训练集上学习出良好的模型，那么就没必要收集更多的数据。 反之，可以尝试增加更多的网络层或每层增加更多的隐藏单元，以增加模型的规模。 此外，也可以尝试调整学习率等超参数的措施来改进学习算法。 如果更大的模型和仔细调试的优化算法效果不佳，那么问题可能源自训练数据的质量。 数据可能含太多噪声，或是可能不包含预测输出所需的正确输入。 这意味着我们需要重新开始，收集更干净的数据或是收集特征更丰富的数据集。</p>
<p>通常，加入总数目一小部分的样本不会对泛化误差产生显著的影响。 因此，建议在<strong>对数尺度上</strong>考虑训练集的大小，例如在后续的实验中倍增样本数目。</p>
<h2 id="4-选择超参数"><a href="#4-选择超参数" class="headerlink" title="4. 选择超参数"></a>4. 选择超参数</h2><p><strong>手动调整超参数</strong></p>
<p>手动搜索超参数的主要目标是调整模型的有效容量以匹配任务的复杂性。 有效容量受限于三个因素：模型的表示容量、学习算法成功最小化训练模型代价函数的能力以及代价函数和训练过程正则化模型的程度。 具有更多网络层，每层有更多隐藏单元的模型具有较高的表示能力——能够表示更复杂的函数。 然而，如果训练算法不能找到某个合适的函数来最小化训练代价，或是正则化项（如权重衰减）排除了这些合适的函数，那么即使模型的表达能力较高，也不能学习出合适的函数。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171013105553500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p><strong>自动超参数优化算法</strong></p>
<p> 通常，网格搜索大约会在<strong>对数尺度</strong>下挑选合适的值。<br> 网格搜索带来的一个明显问题是，计算代价会随着超参数数量呈指数级增长。 如果有$$m$$个超参数，每个最多取$$n$$个值，那么训练和估计所需的试验数将是$$O(n^m)$$。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171013105648545?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p><strong>基于模型的超参数优化</strong></p>
<p>我们可以对验证集误差建模，然后通过优化该模型来提出新的超参数猜想。 大部分基于模型的超参数搜索算法，都是使用贝叶斯回归模型来估计每个超参数的验证集误差期望和该期望的不确定性。 因此，优化涉及到探索（探索高度不确定的超参数，可能带来显著的效果提升，也可能效果很差）和使用（使用已经确信效果不错的超参数——通常是先前见过的非常熟悉的超参数）之间的权衡。</p>
<p>大部分超参数优化算法比随机搜索更复杂，并且具有一个共同的缺点，在它们能够从实验中提取任何信息之前，它们需要运行完整的训练实验。 相比于人类实践者手动搜索，考虑实验早期可以收集的信息量，这种方法是相当低效的，因为手动搜索通常可以很早判断出某组超参数是否是完全病态的。</p>
<h2 id="5-调试策略"><a href="#5-调试策略" class="headerlink" title="5. 调试策略"></a>5. 调试策略</h2><p><strong>可视化计算中模型的行为：</strong></p>
<p>当训练模型检测图像中的对象时，查看一些模型检测到部分重叠的图像。 在训练语音生成模型时，试听一些生成的语音样本。 这似乎是显而易见的，但在实际中很容易只注意量化性能度量，如准确率或对数似然。 直接观察机器学习模型运行其任务，有助于确定其达到的量化性能数据是否看上去合理。 错误评估模型性能可能是最具破坏性的错误之一，因为它们会使你在系统出问题时误以为系统运行良好。</p>
<p><strong>可视化最严重的错误：</strong> </p>
<p>大多数模型能够输出运行任务时的某种置信度量。 例如，基于softmax函数输出层的分类器给每个类分配一个概率。 因此，分配给最有可能的类的概率给出了模型在其分类决定上的置信估计值。 通常，相比于正确预测的概率最大似然训练会略有高估。 但是由于实际上模型的较小概率不太可能对应着正确的标签，因此它们在一定意义上还是有些用的。 通过查看训练集中很难正确建模的样本，通常可以发现该数据预处理或者标记方式的问题。</p>
<p><strong>根据训练和测试误差检测软件：</strong></p>
<p> 我们往往很难确定底层软件是否是正确实现。 训练和测试误差能够提供一些线索。 如果训练误差较低，但是测试误差较高，那么很有可能训练过程是在正常运行，但模型由于算法原因过拟合了。 另一种可能是，测试误差没有被正确地度量，可能是由于训练后保存模型再重载去度量测试集时出现问题，或者是因为测试数据和训练数据预处理的方式不同。 如果训练和测试误差都很高，那么很难确定是软件错误，还是由于算法原因模型欠拟合。 这种情况需要进一步的测试，如下面所述。</p>
<p><strong>拟合极小的数据集：</strong></p>
<p> 当训练集上有很大的误差时，我们需要确定问题是真正的欠拟合，还是软件错误。 通常，即使是小模型也可以保证很好地拟合一个足够小的数据集。 例如，只有一个样本的分类数据可以通过正确设置输出层的偏置来拟合。 通常，如果不能训练一个分类器来正确标注一个单独的样本，或不能训练一个自编码器来成功地精准再现一个单独的样本，或不能训练一个生成模型来一致地生成一个单独的样本，那么很有可能是由于软件错误阻止训练集上的成功优化。 此测试可以扩展到只有少量样本的小数据集上。</p>
<p><strong>比较反向传播导数和数值导数：</strong></p>
<p> 如果读者正在使用一个需要实现梯度计算的软件框架，或者在添加一个新操作到求导库中，必须定义它的bprop方法，那么常见的错误原因是没能正确地实现梯度表达。 验证这些求导正确性的一种方法是比较实现的自动求导和通过有限差分计算的导数。 因为</p>
<p><strong>监控激活函数值和梯度的直方图：</strong></p>
<p> 可视化神经网络在大量训练迭代后（也许是一个轮）收集到的激活函数值和梯度的统计量往往是有用的。 隐藏单元的预激活值可以告诉我们该单元是否饱和，或者它们饱和的频率如何。 例如，对于整流器，它们多久关一次？是否有单元一直关闭？ 对于双曲正切单元而言，预激活绝对值的平均值可以告诉我们该单元的饱和程度。 在深度网络中，传播梯度的快速增长或快速消失，可能会阻碍优化过程。 最后，比较参数梯度和参数的量级也是有帮助的。 我们希望参数在一个小批量更新中变化的幅度是参数量值$$1\%$$这样的级别，而不是$$50\%$$或者$$0.001\%$$（这会导致参数移动得太慢）。 也有可能是某些参数以良好的步长移动，而另一些停滞。 如果数据是稀疏的（比如自然语言），有些参数可能很少更新，检测它们变化时应该记住这一点。</p>
]]></content>
      
        <categories>
            
            <category> Deep Learning Book </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Deep Learning读书笔记6---序列建模：循环和递归网络]]></title>
      <url>/2017/08/28/Deep%20Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B06---%E5%BA%8F%E5%88%97%E5%BB%BA%E6%A8%A1%EF%BC%9A%E5%BE%AA%E7%8E%AF%E5%92%8C%E9%80%92%E5%BD%92%E7%BD%91%E7%BB%9C/</url>
      <content type="html"><![CDATA[<h2 id="1-展开计算图"><a href="#1-展开计算图" class="headerlink" title="1. 展开计算图"></a>1. 展开计算图</h2><p>循环神经网络使用下面的公式定义隐藏单元的值。 为了表明状态是网络的隐藏单元，我们使用变量 $$ h $$代表状态重写：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011140659996?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="30%" align="center"> <div align="left"> </div></div></p>
<p>上式可以用两种不同的方式绘制。 一种方法是为可能在模型的物理实现中存在的部分赋予一个节点，如生物神经网络。 在这个观点下，网络定义了实时操作的回路，如下图的左侧，其当前状态可以影响其未来的状态。 在本章中，我们使用回路图的黑色方块表明在时刻$$t$$的状态到时刻$$t+1$$的状态单个时刻延迟中的相互作用。 另一个绘制RNN的方法是展开的计算图，其中每一个组件由许多不同的变量表示，每个时间步一个变量，表示在该时间点组件的状态。 每个时间步的每个变量绘制为计算图的一个独立节点，如下图的右侧。 我们所说的展开是将左图中的回路映射为右图中包含重复组件的计算图的操作。 目前，展开图的大小取决于序列长度。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011140819315?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<h2 id="2-循环神经网络"><a href="#2-循环神经网络" class="headerlink" title="2. 循环神经网络"></a>2. 循环神经网络</h2><p>循环神经网络中一些重要的设计模式包括以下几种： </p>
<p>a. 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011141521149?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>b. 每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的隐藏单元之间有循环连接的循环网络：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011141615985?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>c. 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011141713371?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>RNN从特定的初始状态$$h^{(0)}$$开始前向传播。 从$$t= 1$$到$$t = \tau$$的每个时间步，我们应用以下更新方程：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011141924222?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="40%" align="center"> <div align="left"> </div></div></p>
<p>其中的参数的偏置向量$$b$$和$$c$$连同权重矩阵$$U$$、$$V$$和$$W$$，分别对应于输入到隐藏、隐藏到输出和隐藏到隐藏的连接。 这个循环网络将一个输入序列映射到相同长度的输出序列。 与$$x$$序列配对的$$y$$的总损失就是所有时间步的损失之和。 例如，$$L^{(t)}$$为给定的$$x^{(1)}, \dots, x^{(t)}$$后$$y^{(t)}$$的负对数似然，则</p>
<p>$$ L\big( { x^{(1)}, \dots, x^{(\tau)} }, { y^{(1)}, \dots, y^{(\tau)} } \big) $$<br>$$  = \sum_t L^{(t)} $$<br>$$  = - \sum<em>t \log p</em>{\text{model}} \big( y^{(t)} \mid { x^{(1)}, \dots, x^{(t)} } \big)  $$</p>
<p>其中$$p_{\text{model}} \big( y^{(t)} \mid { x^{(1)}, \dots, x^{(t)} } \big) $$需要读取模型输出向量$$\hat y^{(t)}$$中对应于$$y^{(t)}$$的项。 关于各个参数计算这个损失函数的梯度是计算成本很高的操作。 梯度计算涉及执行一次前向传播（如在10.3展开图中从左到右的传播），接着是由右到左的反向传播。 运行时间是$$O(\tau)$$，并且不能通过并行化来降低，因为前向传播图是固有循序的;每个时间步只能一前一后地计算。 前向传播中的各个状态必须保存，直到它们反向传播中被再次使用，因此内存代价也是$$O(\tau)$$。 应用于展开图且代价为$$O(\tau)$$的反向传播算法称为通过时间反向传播（BPTT）。 因此隐藏单元之间存在循环的网络非常强大但训练代价也很大。</p>
<p><strong>导师驱动过程和输出循环网络:</strong></p>
<p>我们使用导师驱动过程的最初动机是为了在缺乏隐藏到隐藏连接的模型中避免通过时间反向传播。只要模型一个时间步的输出与下一时间步计算的值存在连接，导师驱动过程仍然可以应用到这些存在隐藏到隐藏连接的模型。然而，只要隐藏单元成为较早时间步的函数，BPTT 算法是必要的。因此训练某些模型时要同时使用导师驱动过程和BPTT。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011143331622?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>计算循环神经网络的梯度：</strong></p>
<p>我们假设输出$$o^{(t)}$$作为softmax函数的参数，我们可以从softmax函数可以获得关于输出概率的向量$$\hat{y}$$。 我们也假设损失是迄今为止给定了输入后的真实目标$$y^{(t)}$$的负对数似然。 对于所有$$i,t$$，关于时间步$$t$$输出的梯度$$\nabla<em>{o^{(t)}} L$$如下：<br>$$ (\nabla</em>{o^{(t)}} L)_i = \frac{\partial L}{\partial o_i^{(t)}} $$$$= \frac{\partial L}{\partial L^{(t)}} \frac{\partial L^{(t)}}{\partial o_i^{(t)}}$$$$= \hat y_i^{(t)} - \mathbf{1}{i,y^{(t)}}$$</p>
<p> <div align="center"><br><img src="http://img.blog.csdn.net/20171011144402450?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="30%" align="center"> <div align="left"> </div></div></p>
<p>然后，我们可以从时刻$$t=\tau-1$$到$$t=1$$反向迭代， 通过时间反向传播梯度，注意$$h^{(t)}(t &lt; \tau)$$同时具有$$o^{(t)}$$和$$h^{(t+1)}$$两个后续节点。 因此，它的梯度由下式计算 :</p>
<p>$$ \nabla<em>{h^{(t)}} L = \Big( \frac{\partial h^{(t+1)}}{ \partial h^{(t)}} \Big)^\top(\nabla</em>{h^{(t+1)}} L) + \Big( \frac{\partial o^{(t)}}{ \partial h^{(t)}} \Big)^\top (\nabla<em>{o^{(t)}} L) $$$$= W^\top (\nabla</em>{h^{(t+1)}} L) \text{diag} \Big( 1 - (h^{(t+1)})^2 \Big)V^\top ( \nabla_{o^{(t)}} L )$$<br>其中$$\text{diag} \Big( 1 - (h^{(t+1)})^2 \Big) $$ 表示包含元素$$1 - (h_i^{(t+1)})^2$$的对角矩阵。 这是关于时刻$$t+1$$与隐藏单元~$$i$$关联的双曲正切的Jacobian。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011144831980?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" align="center"> <div align="left"> </div></div></p>
<h2 id="3-双向RNN"><a href="#3-双向RNN" class="headerlink" title="3. 双向RNN"></a>3. 双向RNN</h2><p>顾名思义，双向RNN 结合时间上从序列起点开始移动的RNN 和另一个时间上从序列末尾开始移动的RNN。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011153527060?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>这个想法可以自然地扩展到2维输入，如图像，由四个RNN组成，每一个沿着四个方向中的一个计算：上、下、左、右。 如果RNN能够学习到承载长期信息，那在2维网格每个点$$(i, j)$$的输出$$O_{i,j}$$就能计算一个能捕捉到大多局部信息但仍依赖于长期输入的表示。 相比卷积网络，应用于图像的RNN计算成本通常更高，但允许同一特征图的特征之间存在长期横向的相互作用。 实际上，对于这样的RNN，前向传播公式可以写成表示使用卷积的形式，计算自底向上到每一层的输入（在整合横向相互作用的特征图的循环传播之前）。</p>
<h2 id="4-基于编码-解码的序列到序列架构"><a href="#4-基于编码-解码的序列到序列架构" class="headerlink" title="4. 基于编码-解码的序列到序列架构"></a>4. 基于编码-解码的序列到序列架构</h2><p>( 1 ) 编码器或读取器或输入RNN处理输入序列。 编码器输出上下文$$C$$（通常是最终隐藏状态的简单函数）。  </p>
<p> ( 2 ) 解码器或写入器或输出RNN则以固定长度的向量为条件产生输出序列$$Y=(y^{(1)}, \dots, y^{(n_y)})$$。 </p>
<p>这种架构对比本章前几节提出的架构的创新之处在于长度$$n_x$$和$$n_y$$可以彼此不同，而之前的架构约束$$n_x = n_y = \tau$$。 在序列到序列的架构中，两个RNN共同训练以最大化$$\log P( y^{(1)}, \dots, y^{(n_y)} \mid x^{(1)},\dots,x^{(n<em>x)} )$$(关于训练集中所有$$x$$和$$y$$对的平均)。 编码器RNN的最后一个状态$$h</em>{n_x}$$通常被当作输入的表示$$C$$并作为解码器RNN的输入。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011225044448?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<h2 id="5-深度循环网络"><a href="#5-深度循环网络" class="headerlink" title="5. 深度循环网络"></a>5. 深度循环网络</h2><p>Graves第一个展示了将RNN的状态分为多层的显著好处，如下图左。 我们可以认为，在下图(a)所示层次结构中较低的层起到了将原始输入转化为对更高层的隐藏状态更合适表示的作用。 Pascanu更进一步提出在上述三个块中各使用一个单独的MLP（可能是深度的），如下图(b)所示。 考虑表示容量，我们建议在这三个步中都分配足够的容量，但增加深度可能会因为优化困难而损害学习效果。 在一般情况下，更容易优化较浅的架构，加入下图(b)的额外深度导致从时间步$$t$$的变量到时间步$$t+1$$的最短路径变得更长。 例如，如果具有单个隐藏层的MLP被用于状态到状态的转换，那么与下图相比，我们就会加倍任何两个不同时间步变量之间最短路径的长度。 然而Pascanu认为，在隐藏到隐藏的路径中引入跳跃连接可以缓和这个问题，如下图(c)所示。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011225443859?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<h2 id="6-递归神经网络"><a href="#6-递归神经网络" class="headerlink" title="6. 递归神经网络"></a>6. 递归神经网络</h2><p>递归网络的一个明显优势是，对于具有相同长度$$\tau$$的序列，深度（通过非线性操作的组合数量来衡量）可以急剧地从$$\tau$$减小为$$O(\log \tau)$$，这可能有助于解决长期依赖。 一个悬而未决的问题是如何以最佳的方式构造树。 一种选择是使用不依赖于数据的树结构，如平衡二叉树。 在某些应用领域，外部方法可以为选择适当的树结构提供借鉴。 例如，处理自然语言的句子时，用于递归网络的树结构可以被固定为句子语法分析树的结构（可以由自然语言语法分析程序提供）{cite?}。 理想的情况下，人们希望学习器自行发现和推断适合于任意给定输入的树结构。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011225834513?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<h2 id="7-长期依赖的挑战"><a href="#7-长期依赖的挑战" class="headerlink" title="7. 长期依赖的挑战"></a>7. 长期依赖的挑战</h2><p>经过许多阶段传播后的梯度倾向于消失（大部分情况）或爆炸（很少，但对优化过程影响很大）。 即使我们假设循环网络是参数稳定的（可存储记忆，且梯度不爆炸），但长期依赖的困难来自比短期相互作用指数小的权重（涉及许多Jacobian相乘）。</p>
<p>特别地，循环神经网络所使用的函数组合有点像矩阵乘法。 我们可以认为，循环联系</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011230245321?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<h2 id="8-渗漏单元和其他多时间尺度的策略"><a href="#8-渗漏单元和其他多时间尺度的策略" class="headerlink" title="8. 渗漏单元和其他多时间尺度的策略"></a>8. 渗漏单元和其他多时间尺度的策略</h2><p>处理长期依赖的一种方法是设计工作在多个时间尺度的模型，使模型的某些部分在细粒度时间尺度上操作并能处理小细节，而其他部分在粗时间尺度上操作并能把遥远过去的信息更有效地传递过来。 存在多种同时构建粗细时间尺度的策略。 这些策略包括在时间轴增加跳跃连接，”渗漏单元”使用不同时间常数整合信号，并去除一些用于建模细粒度时间尺度的连接。</p>
<p><strong>时间维度的跳跃连接</strong></p>
<p>增加从遥远过去的变量到目前变量的直接连接是得到粗时间尺度的一种方法。梯度可能关于时间步数呈指数消失或爆炸。算法引入了$$d$$延时的循环连接以减轻这个问题。 现在导数指数减小的速度与$$\frac{\tau}{d}$$相关而不是$$\tau$$。 既然同时存在延迟和单步连接，梯度仍可能成$$t$$指数爆炸。 这允许学习算法捕获更长的依赖性，但不是所有的长期依赖都能在这种方式下良好地表示。</p>
<p><strong>渗漏单元和一系列不同时间尺度</strong></p>
<p>获得导数乘积接近1的另一方式是设置线性自连接单元，并且这些连接的权重接近1。我们对某些$$v$$值应用更新$$\mu^{(t)} \gets \alpha \mu^{(t-1)} + (1-\alpha) v^{(t)}$$累积一个滑动平均值$$\mu^{(t)}$$，其中$$\alpha$$是一个从$$ \mu^{(t-1)}$$到$$ \mu^{(t)}$$线性自连接的例子。 当$$\alpha$$接近1时，滑动平均值能记住过去很长一段时间的信息，而当$$\alpha$$接近0，关于过去的信息被迅速丢弃。 线性自连接的隐藏单元可以模拟滑动平均的行为。 这种隐藏单元称为渗漏单元。</p>
<p><strong>删除连接</strong></p>
<p>这个想法与之前讨论的时间维度上的跳跃连接不同，因为它涉及主动删除长度为一的连接并用更长的连接替换它们。 以这种方式修改的单元被迫在长时间尺度上运作。 而通过时间跳跃连接是添加边。 收到这种新连接的单元，可以学习在长时间尺度上运作，但也可以选择专注于自己其他的短期连接。</p>
<h2 id="9-长短期记忆LSTM"><a href="#9-长短期记忆LSTM" class="headerlink" title="9. 长短期记忆LSTM"></a>9. 长短期记忆LSTM</h2><p>实际应用中最有效的序列模型称为门控RNN。 包括基于长短期记忆和基于门控循环单元的网络。</p>
<p>像渗漏单元一样，门控RNN想法也是基于生成通过时间的路径，其中导数既不消失也不发生爆炸。 渗漏单元通过手动选择常量的连接权重或参数化的连接权重来达到这一目的。 门控RNN将其推广为在每个时间步都可能改变的连接权重。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011231628269?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>LSTM不是简单地向输入和循环单元的仿射变换之后施加一个逐元素的非线性。 与普通的循环网络类似，每个单元有相同的输入和输出，但也有更多的参数和控制信息流动的门控单元系统。 最重要的组成部分是状态单元$$s_i^{(t)}$$，与前一节讨论的渗漏单元有类似的线性自环。 然而，此处自环的权重（或相关联的时间常数）由遗忘门$$f_i^{(t)}$$控制（时刻$$t$$和细胞$$i$$），由sigmoid单元将权重设置为0和1之间的值：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011232003076?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="40%" align="center"> <div align="left"> </div></div></p>
<p>其中$$x^{(t)}$$是当前输入向量，$$h^{t}$$是当前隐藏层向量，$$h^{t}$$包含所有LSTM细胞的输出。 $$b^f, U^f, W^f$$分别是偏置、输入权重和遗忘门的循环权重。 因此LSTM细胞内部状态以如下方式更新，其中有一个条件的自环权重$$f_i^{(t)}$$：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011232108550?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<h2 id="10-优化长期依赖"><a href="#10-优化长期依赖" class="headerlink" title="10. 优化长期依赖"></a>10. 优化长期依赖</h2><p><strong>截断梯度</strong><br>强非线性函数（如由许多时间步计算的循环网络）往往倾向于非常大或非常小幅度的梯度。 如下图所示，我们可以看到，目标函数（作为参数的函数）存在一个伴随”悬崖”的”地形”：宽且相当平坦区域被目标函数变化快的小区域隔开，形成了一种悬崖。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011232608777?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>一个简单的解决方案已被从业者使用多年：截断梯度。一种选择是在参数更新之前，逐元素地截断小批量产生的参数梯度。 另一种是在参数更新之前截断梯度$$g$$的范数$$|| g ||$$：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011232825223?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="30%" align="center"> <div align="left"> </div></div></p>
<p>其中$$v$$是范数上界，$$g$$用来更新参数。 因为所有参数（包括不同的参数组，如权重和偏置）的梯度被单个缩放因子联合重整化，所以后一方法具有的优点是保证了每个步骤仍然是在梯度方向上的，但实验表明两种形式类似。 </p>
<p><strong>引导信息流的正则化</strong></p>
<p>正则化或约束参数，以引导”信息流”。 特别是即使损失函数只对序列尾部的输出作惩罚，我们也希望梯度向量$$\nabla_{h^{(t)}} L$$在反向传播时能维持其幅度：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011233016573?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<h2 id="11-外显记忆"><a href="#11-外显记忆" class="headerlink" title="11. 外显记忆"></a>11. 外显记忆</h2><p>神经网络擅长存储隐性知识，但是他们很难记住事实。被存储在神经网络参数中之前，随机梯度下降需要多次提供相同的输入，即使如此，该输入也不会被特别精确地存储。 Graves推测这是因为神经网络缺乏工作存储(working memory)系统，即类似人类为实现一些目标而明确保存和操作相关信息片段的系统。 这种外显记忆组件将使我们的系统不仅能够快速”故意”地存储和检索具体的事实，也能利用他们循序推论。 神经网络处理序列信息的需要，改变了每个步骤向网络注入输入的方式，长期以来推理能力被认为是重要的，而不是对输入做出自动的、直观的反应 。</p>
<p>为了解决这一难题，Weston引入了记忆网络，其中包括一组可以通过寻址机制来访问的记忆单元。 记忆网络原本需要监督信号指示他们如何使用自己的记忆单元。 Graves引入的神经网络图灵机，不需要明确的监督指示采取哪些行动而能学习从记忆单元读写任意内容，并通过使用基于内容的软注意机制，允许端到端的训练。 这种软寻址机制已成为其他允许基于梯度优化的模拟算法机制的相关架构的标准。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171011233334217?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
]]></content>
      
        <categories>
            
            <category> Deep Learning Book </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Deep Learning读书笔记5---卷积网络]]></title>
      <url>/2017/08/24/Deep%20Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B05---%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/</url>
      <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h2 id="1-卷积公式："><a href="#1-卷积公式：" class="headerlink" title="1. 卷积公式："></a>1. 卷积公式：</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170927104818444?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/100/fill/I0JBQkFCMA==/dissolve/40/gravity/SouthEast" width="50%" align="center"></div></p>
<div align="left"> 

<h2 id="2-卷积运算特点："><a href="#2-卷积运算特点：" class="headerlink" title="2.卷积运算特点："></a>2.卷积运算特点：</h2><p><strong>稀疏交互：</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170927105858854?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/40/gravity/SouthEast" width="70%" align="center"></div></p>
<div align="left"> 

<p><strong>参数共享：</strong></p>
<p>参数共享是指在一个模型的多个函数中使用相同的参数。 在卷积神经网络中，核的每一个元素都作用在输入的每一位置上（是否考虑边界像素取决于对边界决策的设计）。 卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。</p>
<p><strong>平移等变：</strong></p>
<p>如果一个函数满足输入改变，输出也以同样的方式改变这一性质，我们就说它是等变(equivariant)的。 特别地，如果函数f(x)与g(x)满足$$f(g(x))= g(f(x))$$，我们就说f(x)对于变换g具有等变性。 对于卷积来说，如果令g是输入的任意平移函数，那么卷积函数对于g具有等变性。 </p>
<h2 id="3-池化"><a href="#3-池化" class="headerlink" title="3. 池化"></a>3. 池化</h2><p>池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。 例如，最大池化函数给出相邻矩形区域内的最大值。 其他常用的池化函数包括相邻矩形区域内的平均值、L^2范数以及基于据中心像素距离的加权平均函数。</p>
<p>不管采用什么样的池化函数，当输入作出少量平移时，池化能够帮助输入的表示近似不变。 对于平移的不变性是指当我们对输入进行少量平移时，经过池化函数后的大多数输出并不会发生改变。 </p>
<p><strong>局部平移不变性是一个很有用的性质，尤其是当我们关心某个特征是否出现而不关心它出现的具体位置时。</strong></p>
<p>使用池化可以看作是增加了一个无限强的先验：这一层学得的函数必须具有对少量平移的不变性。 当这个假设成立时，池化可以极大地提高网络的统计效率。</p>
<p>当我们对分离参数的卷积的输出进行池化时，特征能够学得应该对于哪种变换具有不变性，如下图所示。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170927111624901?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"></div></p>
<div align="left"> 

<h2 id="4-基本卷积函数的变体"><a href="#4-基本卷积函数的变体" class="headerlink" title="4. 基本卷积函数的变体"></a>4. 基本卷积函数的变体</h2><p>我们有时会希望跳过核中的一些位置来降低计算的开销（相应的代价是提取特征没有先前那么好了）。 我们可以把这一过程看作是对全卷积函数输出的下采样(downsampling)。 如果我们只想在输出的每个方向上每间隔s个像素进行采样，那么我们可以定义一个下采样卷积函数c使得:</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171115235405274?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"></div></p>
<div align="left"> 

<p>我们把s称为下采样卷积的步幅。 当然也可以对每个移动方向定义不同的步幅。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170927113213889?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast" width="70%" align="center"></div></p>
<div align="left"> 

<p><strong>平铺卷积</strong>对卷积层和局部连接层进行了折衷。 这里并不是对每一个空间位置的权重集合进行学习，我们学习一组核使得当我们在空间移动时它们可以循环利用。 这意味着在近邻的位置上拥有不同的过滤器，就像局部连接层一样，但是对于这些参数的存储需求仅仅会增长常数倍，这个常数就是核的集合的大小，而不是整个输出的特征映射的大小。</p>
<p>局部连接，卷积和全连接的比较 :</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170927112647161?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast" width="70%" align="center"></div></p>
<div align="left"> 

<h2 id="5-结构化输出"><a href="#5-结构化输出" class="headerlink" title="5. 结构化输出"></a>5. 结构化输出</h2><p>对图像逐个像素标记的一种策略是先产生图像标签的原始猜测，然后使用相邻像素之间的交互来修正该原始猜测。 重复这个修正步骤数次对应于在每一步使用相同的卷积，该卷积在深层网络的最后几层之间共享权重。 这使得在层之间共享参数的连续的卷积层所执行的一系列运算，形成了一种特殊的循环神经网络。 下图给出了这样一个循环卷积网络的结构。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170927114217706?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast" width="70%" align="center"></div></p>
<p><div align="left"> </div></p>
<h2 id="6-数据类型"><a href="#6-数据类型" class="headerlink" title="6. 数据类型"></a>6. 数据类型</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170927141521499?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast" width="70%" align="center"></div></p>
<p><div align="left"> </div></p>
<h2 id="7-高效的卷积算法"><a href="#7-高效的卷积算法" class="headerlink" title="7. 高效的卷积算法"></a>7. 高效的卷积算法</h2><ol>
<li>卷积等效于使用傅立叶变换将输入与核都转换到频域、执行两个信号的逐点相乘，再使用傅立叶逆变换转换回时域。</li>
<li>当一个d维的核可以表示成d个向量（每一维一个向量）的外积时，该核被称为可分离的。  它等价于组合d个一维卷积，每个卷积使用这些向量中的一个。 组合方法显著快于使用它们的外积来执行一个d维的卷积。 并且核也只要更少的参数来表示成向量。 如果核在每一维都是w个元素宽，那么朴素的多维卷积需要O(w^d)的运行时间和参数存储空间，而可分离卷积只需要$$O(w\times d)$$的运行时间和参数存储空间。</li>
</ol>
<h2 id="8-随机或无监督的特征"><a href="#8-随机或无监督的特征" class="headerlink" title="8. 随机或无监督的特征"></a>8. 随机或无监督的特征</h2><p>通常，卷积网络训练中最昂贵的部分是学习特征。 减少卷积网络训练成本的一种方式是使用那些不是由监督方式训练得到的特征。</p>
<p>有三种基本策略可以不通过监督训练而得到卷积核：</p>
<ol>
<li>简单地随机初始化它们。 </li>
<li>手动设计它们，例如设置每个核在一个特定的方向或尺度来检测边缘。 </li>
<li>使用无监督的标准来学习核。 例如，将k均值聚类算法应用于小图像块，然后使用每个学得的中心作为卷积核。  使用无监督的标准来学习特征，允许这些特征的确定与位于网络结构顶层的分类层相分离。 然后只需提取一次全部训练集的特征，构造用于最后一层的新训练集。 假设最后一层类似逻辑回归或者SVM，那么学习最后一层通常是凸优化问题。</li>
</ol>
<h2 id="9-卷积网络的神经科学基础"><a href="#9-卷积网络的神经科学基础" class="headerlink" title="9. 卷积网络的神经科学基础"></a>9. 卷积网络的神经科学基础</h2><p>初级视觉皮层细胞具有由Gabor函数所描述的权重。 Gabor函数描述在图像中的2维点处的权重。我们可以认为图像是2维坐标I(x,y)的函数。 类似地，我们可以认为简单细胞是在图像中的一组位置采样，这组位置由一组x坐标X和一组y坐标Y来定义，并且使用的权重$w(x,y)$也是位置的函数。 从这个观点来看，简单细胞对于图像的响应由下式给出:</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170927142822259?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast" width="70%" align="center"></div></p>
<p><div align="left"><br>这里$$\alpha, \beta_x, \beta_y, f, \phi, x_0, y_0, \tau$$都是控制Gabor函数性质的参数。 下图给出了Gabor函数在不同参数集上的一些例子：</div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170927143010384?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast" width="70%" align="center"></div></p>
</div></div></div></div></div></div>]]></content>
      
        <categories>
            
            <category> Deep Learning Book </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Deep Learning读书笔记4---深度模型中的优化]]></title>
      <url>/2017/08/18/Deep%20Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B04---%E6%B7%B1%E5%BA%A6%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96/</url>
      <content type="html"><![CDATA[<h2 id="1-最小化经验风险"><a href="#1-最小化经验风险" class="headerlink" title="1. 最小化经验风险"></a>1. 最小化经验风险</h2><p>利用训练集上的经验分布,  p^(  x  ,  y  )  替代真实分布  p  (  x  ,  y  )  。 现在，我们将最小化经验风险：  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170921113636649?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dis
solve/70/gravity/SouthEast" width="50%"></div></p>
<div align="left"> 

<p>其中  m  表示训练样本的数目。</p>
<h2 id="2-小批量算法"><a href="#2-小批量算法" class="headerlink" title="2.小批量算法"></a>2.小批量算法</h2><p>通常，在所有训练集上准确计算优化参数计算代价非常大，因为我们需要在整个数据集上的每个样本上评估模型。<br>在实践中，我们可以从数据集中随机采样少量的样本，然后计算这些样本上的平均值。<br>随机方法的典型示例是随机梯度下降，小批量的大小通常由以下几个因素决定：<br>  - 更大的批量会计算更精确的梯度估计，但是回报却是小于线性的。<br>  - 极小批量通常难以充分利用多核架构。 这促使我们使用一些绝对最小批量，低于这个值的小批量处理不会减少计算时间。<br>- 如果批量处理中的所有样本可以并行地处理（通常确是如此），那么内存消耗和批量大小会正比。 对于很多硬件设施，这是批量大小的限制因素。<br>- 在某些硬件上使用特定大小的数组时，运行时间会更少。 尤其是在使用,GPU,时，通常使用  2  的幂数作为批量大小可以获得更少的运行时间。一般，  2的幂数的取值范围是  32  到  256  ，  16  有时在尝试大模型时使用。<br>- 可能是由于小批量在学习过程中加入了噪声，它们会有一些正则化效果。 泛化误差通常在批量大小为  1  时最好。因为梯度估计的高方差，小批量训练需要较小的学习率以保持稳定性。因为降低的学习率和消耗更多步骤来遍历整个训练集都会产生更多的步骤，所以会导致总的运行时间非常大。</p>
<h2 id="3-神经网络优化中的挑战"><a href="#3-神经网络优化中的挑战" class="headerlink" title="3.神经网络优化中的挑战"></a>3.神经网络优化中的挑战</h2><h3 id="3-1-病态"><a href="#3-1-病态" class="headerlink" title="3.1 病态"></a>3.1 病态</h3><p>在优化凸函数时，会遇到一些挑战。 这其中最突出的是Hessian矩阵的病态。<br>我们通常通过（方向）二阶导数预期一个梯度下降步骤能表现得多好。如果我们使用学习率  ϵ  ，那么新的点  x  将会是  x  (  0  )  −  ϵg  。我们在该点处作函数  f  (  x  )  的近似二阶泰勒级数：  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922094440191?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%"></div></p>
<div align="left"> 

<p>当  $$1/2ϵ^2g^THg$$  超过  $$ϵg⊤g$$  时，梯度的病态会成为问题。<br>判断病态是否不利于神经网络训练任务，我们可以监测平方梯度范数$$g^⊤g$$和$$g^⊤Hg$$。<br>在很多情况中，梯度范数不会在训练过程中显著缩小，但是$$g^⊤Hg$$的增长会超过一个数量级。<br>其结果是尽管梯度很强，学习会变得非常缓慢，因为学习率必须收缩以弥补更强的曲率。</p>
<h3 id="3-2-局部极小值"><a href="#3-2-局部极小值" class="headerlink" title="3.2 局部极小值"></a>3.2 局部极小值</h3><p>如果一个足够大的训练集可以唯一确定一组模型参数，那么该模型被称为辨识性的。由于模型可辨识性问题，神经网络和任意具有多个等效参数化潜变量的模型都会具有多个局部极小值。 带有潜变量的模型通常是不可辨认的，因为通过相互交换潜变量我们能得到等价的模型。 例如，考虑神经网络的第一层，我们可以交换单元i和单元j的传入权重向量、传出权重向量而得到等价的模型。如果神经网络有m层，每层有n个单元，那么会有n!m种排列隐藏单元的方式。<br>这种不可辨认性被称为权重空间对称性。<br>除了权重空间对称性，很多神经网络还有其他导致不可辨认的原因。 例如，在任意整流线性网络或者maxout网络中， 我们可以将传入权重和偏置扩大α倍，然后将传出权重扩大1/α倍，而保持模型等价。<br>这意味着，如果代价函数不包括如权重衰减这种直接依赖于权重而非模型输出的项，那么整流线性网络或者maxout网络的每一个局部极小点都在等价的局部极小值的(m×n)维双曲线上。</p>
<h3 id="3-3-高原、鞍点和其他平坦区域"><a href="#3-3-高原、鞍点和其他平坦区域" class="headerlink" title="3.3 高原、鞍点和其他平坦区域"></a>3.3 高原、鞍点和其他平坦区域</h3><p>对于很多高维非凸函数而言，局部极小值（以及极大值）事实上都远少于另一类梯度为零的点：鞍点。 鞍点附近的某些点比鞍点有更大的代价，而其他点则有更小的代价。<br>在鞍点处，Hessian,矩阵同时具有正负特征值。<br>位于正特征值对应的特征向量方向的点比鞍点有更大的代价，反之，位于负特征值对应的特征向量方向的点有更小的代价。<br>我们可以将鞍点视为代价函数某个横截面上的局部极小点，同时也可以视为代价函数某个横截面上的局部极大点。</p>
<h3 id="3-4-悬崖和梯度爆炸"><a href="#3-4-悬崖和梯度爆炸" class="headerlink" title="3.4 悬崖和梯度爆炸"></a>3.4 悬崖和梯度爆炸</h3><p><div align="center"><br><img src="http://img.blog.csdn.net/20170922102754955?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left"> 

<p>其基本想法源自梯度并没有指明最佳步长，只说明了在无限小区域内的最佳方向。<br>当传统的梯度下降算法提议更新很大一步时，启发式梯度截断会干涉来减小步长，从而使其不太可能走出梯度近似为最陡下降方向的悬崖区域。</p>
<h3 id="3-5-长期依赖"><a href="#3-5-长期依赖" class="headerlink" title="3.5 长期依赖"></a>3.5 长期依赖</h3><p>当计算图变得极深时，神经网络优化算法会面临的另外一个难题就是长期依赖问题——由于变深的结构使模型丧失了学习到先前信息的能力，让优化变得极其困难。<br>深层的计算图不仅存在于前馈网络，还存在于之后介绍的循环网络中。<br>因为循环网络要在很长时间序列的各个时刻重复应用相同操作来构建非常深的计算图，并且模型参数共享，这使问题更加凸显。</p>
<h3 id="3-6-非精确梯度"><a href="#3-6-非精确梯度" class="headerlink" title="3.6 非精确梯度"></a>3.6 非精确梯度</h3><p>大多数优化算法的先决条件都是我们知道精确的梯度或是Hessian矩阵。<br>在实践中，通常这些量会有噪声，甚至是有偏的估计。几乎每一个深度学习算法都需要基于采样的估计，至少使用训练样本的小批量来计算梯度。<br>在其他情况，我们希望最小化的目标函数实际上是难以处理的。 当目标函数不可解时，通常其梯度也是难以处理的。<br>各种神经网络优化算法的设计都考虑到了梯度估计的缺陷。 我们可以选择比真实损失函数更容易估计的代理损失函数来避免这个问题。</p>
<h3 id="3-7-局部和全局结构间的弱对应"><a href="#3-7-局部和全局结构间的弱对应" class="headerlink" title="3.7 局部和全局结构间的弱对应"></a>3.7 局部和全局结构间的弱对应</h3><p><div align="center"><br><img src="http://img.blog.csdn.net/20170922103155685?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left"> 

<h2 id="4-基本算法"><a href="#4-基本算法" class="headerlink" title="4.基本算法"></a>4.基本算法</h2><h3 id="4-1-SGD"><a href="#4-1-SGD" class="headerlink" title="4.1 SGD"></a>4.1 SGD</h3><p><div align="center"><br><img src="http://img.blog.csdn.net/20170922103316817?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left"> 

<p>SGD算法中的一个关键参数是学习率。 之前，我们介绍的SGD使用固定的学习率。 在实践中，有必要随着时间的推移逐渐降低学习率，因此我们将第k步迭代的学习率记作  ϵ  k  。<br>实践中，一般会线性衰减学习率直到第  τ  次迭代：<br>$$ ϵ_k=(1−α)ϵ_0+αϵ<em>t $$<br>其中$$ α=k</em>τ $$ 。 在  τ  步迭代之后，一般使  ϵ  保持常数。</p>
<p><strong> 4.2 动量 </strong>   </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922104728067?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left"> 


<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922105042316?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left"> 

<p>速度  v  累积了梯度元素  $$∇θ(1/m∑  m _{i  =  1}  L  (  f  (  x  (  i  )  ;  θ  ),  y  (  i  )  )  )$$  。 相对于ϵ  ，  α  越大，之前梯度对现在方向的影响也越大。之前，步长只是梯度范数乘以学习率。<br>现在，步长取决于梯度序列的大小和排列。 当许多连续的梯度指向相同的方向时，步长最大。</p>
<p><strong> 4.3 Nesterov 动量 </strong></p>
<p>Nesterov 动量可以解释为往标准动量方法中添加了一个校正因子。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922105656434?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left"> 

<p>在凸批量梯度的情况下，Nesterov 动量将额外误差收敛率从  O  (  1  /  k  )  （  k  步后）改进到  O  (  1  /  k<br>2  )  可惜，在随机梯度的情况下，Nesterov 动量没有改进收敛率。</p>
<h2 id="5-参数初始化策略"><a href="#5-参数初始化策略" class="headerlink" title="5. 参数初始化策略"></a>5. 参数初始化策略</h2><p>一种初始化  m  个输入和  n  输出的全连接层的权重的启发式方法是从分布  U  (  −  1/ √  m ,  1/√ m )中采样权重， 而Bingo建议使用标准初始化：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922112203688?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="30%"></div></p>
<div align="left"> 

<p>martens提出了一种被称为稀疏初始化的方案，每个单元初始化为恰好有  k  个非零权重。 这个想法保持该单元输入的总数量独立于输入数目  m，而不使单一权重元素的大小随  m  缩小。 稀疏初始化有助于实现单元之间在初始化时更具多样性。 但是，获得较大取值的权重也同时被加了很强的先验。<br>因为梯度下降需要很长时间缩小“不正确”的大值，这个初始化方案可能会导致某些单元出问题，例如,maxout单元有几个过滤器，互相之间必须仔细调整。</p>
<p>设置偏置的方法必须和设置权重的方法协调。 设置偏置为零通常在大多数权重初始化方案中是可行的。</p>
<h2 id="6-自适应学习率算法"><a href="#6-自适应学习率算法" class="headerlink" title="6. 自适应学习率算法"></a>6. 自适应学习率算法</h2><h3 id="6-1-AdaGrad"><a href="#6-1-AdaGrad" class="headerlink" title="6.1 AdaGrad"></a>6.1 AdaGrad</h3><p>AdaGrad算法，如下图所示，能独立地适应所有模型参数的学习率，缩放每个参数反比于其所有梯度历史平方值总和的平方根。<br>具有损失最大偏导的参数相应地有一个快速下降的学习率，而具有小偏导的参数在学习率上有相对较小的下降。 净效果是在参数空间中更为平缓的倾斜方向会取得更大的进步。<br>在凸优化背景中，AdaGrad 算法具有一些令人满意的理论性质。<br>然而，经验上已经发现，对于训练深度神经网络模型而言，从训练开始时积累梯度平方会导致有效学习率过早和过量的减小。<br>AdaGrad在某些深度学习模型上效果不错，但不是全部。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922113116588?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left"> 

<h3 id="6-2-RMSProp"><a href="#6-2-RMSProp" class="headerlink" title="6.2 RMSProp"></a>6.2 RMSProp</h3><p>RMSProp算法修改AdaGrad以在非凸设定下效果更好，改变梯度积累为指数加权的移动平均。 AdaGrad旨在应用于凸问题时快速收敛。<br>当应用于非凸函数训练神经网络时，学习轨迹可能穿过了很多不同的结构，最终到达一个局部是凸碗的区域。<br>AdaGrad根据平方梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就变得太小了。<br>RMSProp使用指数衰减平均以丢弃遥远过去的历史，使其能够在找到凸碗状结构后快速收敛， 它就像一个初始化于该碗状结构的AdaGrad算法实例。</p>
<p>RMSProp的标准形式如算法8.5所示，算法8.6为结合Nesterov动量的形式。 相比于AdaGrad，使用移动平均引入了一个新的超参数  ρ，用来控制移动平均的长度范围。  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922113523234?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left"> 


<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922113752900?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left"> 

<h3 id="6-3-Adam"><a href="#6-3-Adam" class="headerlink" title="6.3 Adam"></a>6.3 Adam</h3><p>首先，在Adam中，动量直接并入了梯度一阶矩（指数加权）的估计。 将动量加入RMSProp最直观的方法是将动量应用于缩放后的梯度。<br>结合缩放的动量使用没有明确的理论动机。 其次，Adam包括偏置修正，修正从原点初始化的一阶矩（动量项）和（非中心的）二阶矩的估计。<br>RMSProp也采用了（非中心的）二阶矩估计，然而缺失了修正因子。 因此，不像Adam，RMSProp二阶矩估计可能在训练初期有很高的偏置。<br>Adam通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922113939300?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left"> 

<h2 id="7-二阶近似方法"><a href="#7-二阶近似方法" class="headerlink" title="7. 二阶近似方法"></a>7. 二阶近似方法</h2><h3 id="7-1-牛顿法"><a href="#7-1-牛顿法" class="headerlink" title="7.1 牛顿法"></a>7.1 牛顿法</h3><p>牛顿法是基于二阶泰勒级数展开在某点$$θ_0$$附近来近似  J  (  θ  )  的优化方法，其忽略了高阶导数：  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922144108575?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dis
solve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left"> 

<p>其中  H  是  J  相对于  θ  的Hessian矩阵在 $$θ_0$$处的估计。 如果我们再求解这个函数的临界点，我们将得到牛顿参数更新规则：  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922144225958?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dis
solve/70/gravity/SouthEast" width="30%"></div></p>
<div align="left"> 

<p>因此，对于局部的二次函数（具有正定的  H  ,），用$$H^{−1}$$重新调整梯度，牛顿法会直接跳到极小值。<br>如果目标函数是凸的但非二次的（有高阶项），该更新将是迭代的，得到和牛顿法相关的算法，如算法8.8所示。  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922143919690?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left"> 

<p>牛顿法只适用于Hessian矩阵是正定的情况。 在深度学习中，目标函数的表面通常非凸（有很多特征），如鞍点。如果Hessian矩阵的特征值并不都是正的，例如，靠近鞍点处，牛顿法实际上会导致更新朝错误的方向移动。 这种情况可以通过正则化Hessian矩阵来避免。<br>常用的正则化策略包括在Hessian矩阵对角线上增加常数  α  。 正则化更新变为  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922144700517?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dis
solve/70/gravity/SouthEast" width="30%"></div></p>
<div align="left"> 

<p>只要Hessian矩阵的负特征值仍然相对接近零，效果就会很好。 在曲率方向更极端的情况下，  α  的值必须足够大，以抵消负特征值。 然而，如果α持续增加，Hessian,矩阵会变得由对角矩阵  α  I  主导，通过牛顿法所选择的方向会收敛到普通梯度除以  α  。 当很强的负曲率存在时，  α可能需要特别大，以致于牛顿法比选择合适学习率的梯度下降的步长更小。</p>
<h2 id="7-2-共轭梯度"><a href="#7-2-共轭梯度" class="headerlink" title="7.2 共轭梯度"></a>7.2 共轭梯度</h2><p>对于二次曲面而言，共轭方向确保梯度沿着前一方向大小不变。 因此，我们在前一方向上仍然是极小值。 其结果是，在k维参数空间中，共轭梯度只需要至多k次线搜索就能达到极小值。  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922145245567?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left"> 

<h2 id="8-优化策略和元算法"><a href="#8-优化策略和元算法" class="headerlink" title="8. 优化策略和元算法"></a>8. 优化策略和元算法</h2><h2 id="8-1-批标准化"><a href="#8-1-批标准化" class="headerlink" title="8.1 批标准化"></a>8.1 批标准化</h2><p>重参数化显著减少了多层之间协调更新的问题。 批标准化可应用于网络的任何输入层或隐藏层。 设  H是需要标准化的某层的小批量激活函数，排布为设计矩阵，每个样本的激活出现在矩阵的每一行中。 为了标准化H，我们将其替换为  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922153951385?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="20%"></div></p>
<div align="left"> 

<p>其中  μ  是包含每个单元均值的向量，  σ  是包含每个单元标准差的向量。 此处的算术是基于广播向量  μ  和向量  σ  应用于矩阵  H的每一行。 在每一行内，运算是逐元素的，因此  $$H _{(i,j)}$$  标准化为减去  $$μ_j$$  再除以  $$σ_j$$  。</p>
<p>在 <strong> 训练阶段 </strong> ，  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922154334976?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="30%"></div></p>
<div align="left"> 

<p>其中  δ  是个很小的正值，比如  $$10^{−8}$$  ，以强制避免遇到z的梯度在  z  =  0  处未定义的问题。<br>至关重要的是，我们通过反向传播这些操作来计算均值和标准差，并应用它们于标准化  H  。 这意味着，梯度不会再简单地增加  h  i的标准差或均值；标准化操作会除掉这一操作的影响，归零其在梯度中的元素。<br>以前的方法添加代价函数的惩罚，以鼓励单元标准化激活统计量，或是在每个梯度下降步骤之后重新标准化单元统计量。<br>前者通常会导致不完全的标准化，而后者通常会显著地消耗时间，因为学习算法会反复改变均值和方差而标准化步骤会反复抵消这种变化。<br>批标准化重参数化模型，以使一些单元总是被定义标准化，巧妙地回避了这两个问题。</p>
<p>在 <strong> 测试阶段 </strong> ，  μ  和  σ  可以被替换为训练阶段收集的运行均值。 这使得模型可以对单一样本评估，而无需使用定义于整个小批量的  μ和  σ  。</p>
<p>标准化一个单元的均值和标准差会降低包含该单元的神经网络的表达能力。 为了保持网络的表现力，通常会将批量隐藏单元激活  H  替换为  γ  H  ′  +β  ，而不是简单地使用标准化的  H  ′  。 变量  γ  和  β  是允许新变量有任意均值和标准差的学习参数。<br>乍一看，这似乎是无用的——为什么我们将均值设为  0  ，然后又引入参数允许它被重设为任意值  β  ？答案是新的参数可以表示旧参数作为输入的同一族函数，但是新参数有不同的学习动态。 在旧参数中，  H  的均值取决于  H  下层中参数的复杂关联。<br>在新参数中，  γ  H  ′  +  β  的均值仅由  β  确定。 新参数很容易通过梯度下降来学习。</p>
<h2 id="8-2-坐标下降"><a href="#8-2-坐标下降" class="headerlink" title="8.2 坐标下降"></a>8.2 坐标下降</h2><p>在某些情况下，将一个优化问题分解成几个部分，可以更快地解决原问题。 如果我们相对于某个单一变量  x  i  最小化  f  (  x  )，然后相对于另一个变量  x  j  等等，反复循环所有的变量，我们会保证到达（局部）极小值。 这种做法被称为坐标下降，因为我们一次优化一个坐标。<br>更一般地，块坐标下降是指对于某个子集的变量同时最小化。 术语”坐标下降”通常既指块坐标下降，也指严格的单个坐标下降。</p>
<p>当优化问题中的不同变量能够清楚地分成相对独立的组，或是当优化一组变量明显比优化所有变量效率更高时，坐标下降最有意义。</p>
<h2 id="8-3-Polyak平均"><a href="#8-3-Polyak平均" class="headerlink" title="8.3 Polyak平均"></a>8.3 Polyak平均</h2><p>Polyak平均会平均优化算法在参数空间访问轨迹中的几个点。 如果  t  次迭代梯度下降访问了点  θ  (  1  )  ,  …  ,  θ  (t  )  ，那么Polyak平均算法的输出是 $$ θ^t =  1  /t  ∑_  iθ^i$$  。<br>在某些问题中，如梯度下降应用于凸问题时，这种方法具有较强的收敛保证。 当应用于神经网络时，其验证更多是启发式的，但在实践中表现良好。<br>基本想法是，优化算法可能会来回穿过山谷好几次而没经过山谷底部附近的点。 尽管两边所有位置的均值应比较接近谷底。</p>
<p>在非凸问题中，优化轨迹的路径可以非常复杂，并且经过了许多不同的区域。<br>包括参数空间中遥远过去的点，可能与当前点在代价函数上相隔很大的障碍，看上去不像一个有用的行为。<br>其结果是，当应用Polyak平均于非凸问题时，通常会使用指数衰减计算平均值：  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922161152564?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="30%"></div></p>
<div align="left"> 

<h3 id="8-4-监督预训练"><a href="#8-4-监督预训练" class="headerlink" title="8.4 监督预训练"></a>8.4 监督预训练</h3><p>有时，如果模型太复杂难以优化，或是如果任务非常困难，直接训练模型来解决特定任务的挑战可能太大。<br>训练模型来求解一个简化的问题，然后转移到最后的问题，有时也会更有效些。 这些在直接训练目标模型求解目标问题之前，训练简单模型求解简化问题的方法统称为预训练。</p>
<p>贪心算法将问题分解成许多部分，然后独立地在每个部分求解最优值。 令人遗憾的是，结合各个最佳的部分不能保证得到一个最佳的完整解。<br>然而，贪心算法计算上比求解最优联合解的算法高效得多，并且贪心算法的解在不是最优的情况下，往往也是可以接受的。<br>贪心算法也可以紧接一个精调阶段，联合优化算法搜索全问题的最优解。 使用贪心解初始化联合优化算法，可以极大地加速算法，并提高寻找到的解的质量。  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170922161601583?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left"> 

<h3 id="8-5-设计有助于优化的模型"><a href="#8-5-设计有助于优化的模型" class="headerlink" title="8.5 设计有助于优化的模型"></a>8.5 设计有助于优化的模型</h3><p><strong> 在实践中，选择一族容易优化的模型比使用一个强大的优化算法更重要。 </strong></p>
<p>具体来说，现代神经网络的设计选择体现在层之间的线性变换，几乎处处可导的激活函数，和大部分定义域都有明显的梯度。<br>特别地，创新的模型，如LSTM，整流线性单元和maxout单元都比先前的模型（如基于sigmoid单元的深度网络）使用更多的线性函数。<br>这些模型都具有简化优化的性质。 如果线性变换的Jacobian具有相对合理的奇异值，那么梯度能够流经很多层。<br>此外，线性函数在一个方向上一致增加，所以即使模型的输出远离正确值，也可以简单清晰地计算梯度，使其输出方向朝降低损失函数的方向移动。<br>换言之，现代神经网络的设计方案旨在使其局部梯度信息合理地对应着移向一个遥远的解。</p>
<p>其他的模型设计策略有助于使优化更简单。 例如，层之间的线性路径或是跳跃连接减少了从较低层参数到输出最短路径的长度，因而缓解了梯度消失的问题。<br>一个和跳跃连接相关的想法是添加和网络中间隐藏层相连的输出的额外副本，如GoogLeNet和深度监督网络。<br>这些”辅助头”被训练来执行和网络顶层主要输出相同的任务，以确保底层网络能够接受较大的梯度。 当训练完成时，辅助头可能被丢弃。<br>这是前一小节介绍到的预训练策略的替代方法。<br>以这种方式，我们可以在一个阶段联合训练所有层，而不改变架构，使得中间层（特别是低层）能够通过更短的路径得到一些如何更新的有用信息。<br>这些信息为底层提供了误差信号。</p>
<h3 id="8-6-延拓法和课程学习"><a href="#8-6-延拓法和课程学习" class="headerlink" title="8.6 延拓法和课程学习"></a>8.6 延拓法和课程学习</h3><p>延拓法是一族通过挑选初始点使优化更容易的方法，以确保局部优化花费大部分时间在表现良好的空间。 延拓法的背后想法是构造一系列具有相同参数的目标函数。<br>为了最小化代价函数  J  (  θ  )  ，我们构建新的代价函数  J  (  0  )  ,  …  ,  J  (  n  )  。<br>这些代价函数的难度逐步提高，其中  J  (  0  )  是最容易最小化的，  J  (  n  )  是最难的，真正的代价函数驱动整个过程。 当我们说J  (  i  )  比  J  (  i  +  1  )  更容易时，是指其在更多的  θ  空间上表现良好。<br>随机初始化更有可能落入局部下降可以成功最小化代价函数的区域，因为其良好区域更大。 这系列代价函数设计为前一个解是下一个的良好初始点。<br>因此，我们首先解决一个简单的问题，然后改进解以解决逐步变难的问题，直到我们求解真正问题的解。</p>
</div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>]]></content>
      
        <categories>
            
            <category> Deep Learning Book </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Deep Learning读书笔记3---深度学习中的正则化]]></title>
      <url>/2017/08/13/Deep%20Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B03---%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96/</url>
      <content type="html"><![CDATA[<h2 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h2><p>正则化定义为“ <strong> 对学习算法的修改——旨在减少泛化误差而不是训练误差 </strong> ”。<br>目前有许多正则化策略。 有些策略向机器学习模型添加限制参数值的额外约束。 有些策略向目标函数增加额外项来对参数值进行软约束。<br>有时侯，这些约束和惩罚被设计为编码特定类型的先验知识； 其他时候，这些约束和惩罚被设计为偏好简单模型，以便提高泛化能力。<br>有时，惩罚和约束对于确定欠定的问题是必要的。 其他形式的正则化，如被称为集成的方法，则结合多个假说来解释训练数据。</p>
<h2 id="2-参数范数惩罚"><a href="#2-参数范数惩罚" class="headerlink" title="2.参数范数惩罚"></a>2.参数范数惩罚</h2><p>在神经网络中，参数包括每一层仿射变换的权重和偏置，我们通常 <strong> 只对权重 </strong> 做惩罚而不对偏置做正则惩罚。 精确拟合偏置所需的数据通常比拟合权重少得多。<br>每个权重会指定两个变量如何相互作用。 我们需要在各种条件下观察这两个变量才能良好地拟合权重。 而每个偏置仅控制一个单变量。<br>这意味着，我们不对其进行正则化也不会导致太大的方差。 另外，正则化偏置参数可能会导致明显的欠拟合。</p>
<h3 id="2-1-L-2-参数正则化"><a href="#2-1-L-2-参数正则化" class="headerlink" title="2.1  L  2  参数正则化"></a>2.1  L  2  参数正则化</h3><p>目标函数：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170915094204772?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">

<p>L2  参数正则化效应示意图：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170915095451330?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">

<p><strong> 说明： </strong> 只有在显著减小目标函数方向上的参数会保留得相对完好。 在无助于目标函数减小的方向上改变参数不会显著增加梯度。 这种不重要方向对应的分量会在训练过程中因正则化而衰减掉。 </p>
<h3 id="2-2-L-1-参数正则化"><a href="#2-2-L-1-参数正则化" class="headerlink" title="2.2  L  1  参数正则化"></a>2.2  L  1  参数正则化</h3><p>形式地，对模型参数w的L1正则化被定义为：</p>
<p>$$Ω(θ)=||w||^1=∑_i|w_i|$$</p>
<p>即各个参数的绝对值之和正则化。</p>
<p>相比  L  2  正则化，  L  1  正则化会产生更稀疏的解。 此处稀疏性指的是最优值中的一些参数为  0  。  L  2<br>正则化不会使参数变得稀疏，而  L  1  正则化有可能通过足够大的  α  实现稀疏。</p>
<p>由  L  1  正则化导出的稀疏性质已经被广泛地用于特征选择机制。 特征选择从可用的特征子集选择出有意义的特征，化简机器学习问题。<br>著名的LASSO（Least Absolute Shrinkage and Selection Operator）模型将L1惩罚和线性模型结合，并使用最小二乘代价函数。  L  1  惩罚使部分子集的权重为零，表明相应的特征可以被安全地忽略。</p>
<h2 id="3-作为约束的范数惩罚"><a href="#3-作为约束的范数惩罚" class="headerlink" title="3.作为约束的范数惩罚"></a>3.作为约束的范数惩罚</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170915102248001?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">

<p>这和最小化  $$J^~$$  的正则化训练问题是完全一样的。 因此，我们可以把参数范数惩罚看作对权重强加的约束。 如果  Ω  是  L  2范数，那么权重就是被约束在一个  L  2  球中。 如果  Ω  是  L  1  范数，那么权重就是被约束在一个  L  1  范数限制的区域中。<br>通常我们不知道权重衰减系数  α  约束的区域大小，因为  α  的值不直接告诉我们  k  的值。 原则上我们可以解得  k  ，但  k  和  α∗  之间的关系取决于  J  的形式。 虽然我们不知道约束区域的确切大小，但我们可以通过增加或者减小  α  来大致扩大或收缩约束区域。 较大的  α，将得到一个较小的约束区域。</p>
<p>有时候，我们希望使用显式的限制，而不是惩罚。 我们可以修改下降算法（如随机梯度下降算法），使其先计算  J  ( θ  )  的下降步长，然后将θ投影到满足  Ω  (  θ  )  &lt; k  的最近点。 如果我们知道什么样的  k  是合适的，而不想花时间寻找对应于此k处的α值，这会非常有用。原因如下：<br>a.惩罚可能会导致目标函数非凸而使算法陷入局部极小(对应于小的  θ  ）。<br>b.当使用较高的学习率时，很可能进入正反馈，即大的权重诱导大梯度，然后使得权重获得较大更新。 如果这些更新持续增加权重的大小，  θ就会迅速增大，直到离原点很远而发生溢出。 重投影的显式约束可以防止这种反馈环引起权重无限制地持续增加。</p>
<p>建议结合使用约束和高学习速率，这样能更快地探索参数空间，并保持一定的稳定性。</p>
<h2 id="4-数据集增强"><a href="#4-数据集增强" class="headerlink" title="4.数据集增强"></a>4.数据集增强</h2><p>让机器学习模型泛化得更好的最好办法是使用更多的数据进行训练。 当然，在实践中，我们拥有的数据量是很有限的。<br>解决这个问题的一种方法是创建假数据并添加到训练集中。方式包括加入特定噪声（如高斯噪声），做一定的几何变换等等。</p>
<p>另一种正则化模型的噪声使用方式是将其 <strong> 加到权重 </strong> ，这项技术主要用于循环神经网络。 这可以被解释为关于权重的贝叶斯推断的随机实现。<br>贝叶斯学习过程将权重视为不确定的，并且可以通过概率分布表示这种不确定性。 向权重添加噪声是反映这种不确定性的一种实用的随机方法。</p>
<p>大多数数据集的  y  标签都有一定错误。 错误的  y  不利于最大化  log  p  (  y  |x  )  。<br>避免这种情况的一种方法是显式地对 <strong> 标签上的噪声 </strong> 进行建模。 例如，我们可以假设，对于一些小常数  ϵ  ，训练集标记  y  是正确的概率1  −  ϵ  ，（以  ϵ  的概率）任何其他可能的标签也可能是正确的。 这个假设很容易就能解析地与代价函数结合，而不用显式地抽取噪声样本。<br>例如，标签平滑（label smoothing）通过把确切分类目标从0和1替换成  ϵ  k  −  1  和  1  −  ϵ  ，正则化具有  k个输出的softmax函数的模型。 标准交叉熵损失可以用在这些非确切目标的输出上。 使用softmax函数和明确目标的最大似然学习可能永远不会收敛——softmax函数永远无法真正预测0概率或1概率，因此它会继续学习越来越大的权重，使预测更极端。 使用如权重衰减等其他正则化策略能够防止这种情况。<br>标签平滑的优势是能够防止模型追求确切概率而不影响模型学习正确分类。</p>
<h2 id="5-半监督学习"><a href="#5-半监督学习" class="headerlink" title="5.半监督学习"></a>5.半监督学习</h2><p>在半监督学习的框架下，  P  (  x  )  产生的未标记样本和  P  (  x  ,  y  )  中的标记样本都用于估计  P  (  y  ∣x  )  或者根据  x  预测  y  。</p>
<p>在深度学习的背景下，半监督学习通常指的是学习一个表示  h  =  f  (  x  )  。 学习表示的目的是使相同类中的样本有类似的表示。<br>无监督学习可以为如何在表示空间聚集样本提供有用线索。 在输入空间紧密聚集的样本应该被映射到类似的表示。<br>在许多情况下，新空间上的线性分类器可以达到较好的泛化。 这种方法的一个经典变种是使用主成分分析作为分类前（在投影后的数据上分类）的预处理步骤。</p>
<p>我们可以构建这样一个模型，其中生成模型  P  (  x  )  或  P  (  x  ,  y  )  与判别模型  P  (  y  ∣  x  )共享参数，而不用分离无监督和监督部分。 我们权衡监督模型准则 log  P  (y∣x)  和无监督或生成模型准则（如  −log  P  (  x  )  或  −  log  P  (  x  ,  y  )  ）。 生成模型准则表达了对监督学习问题解的特殊形式的先验知识，即P  (  x  )  的结构通过某种共享参数的方式连接到  P  (  y  ∣  x  )  。<br>通过控制在总准则中的生成准则，我们可以获得比纯生成或纯判别训练准则更好的权衡。</p>
<h2 id="6-多任务学习"><a href="#6-多任务学习" class="headerlink" title="6.多任务学习"></a>6.多任务学习</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170915105548176?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">

<p>从深度学习的观点看，底层的先验知识如下：能解释数据变化（在与之相关联的不同任务中观察到）的因素中，某些因素是跨两个或更多任务共享的。</p>
<h2 id="7-提前终止"><a href="#7-提前终止" class="headerlink" title="7.提前终止"></a>7.提前终止</h2><p>当训练有足够的表示能力甚至会过拟合的大模型时，我们经常观察到，训练误差会随着时间的推移逐渐降低但验证集的误差会再次上升。</p>
<p>这意味着我们只要返回使验证集误差最低的参数设置，就可以获得验证集误差更低的模型（并且因此有希望获得更好的测试误差）。<br>在每次验证集误差有所改善后，我们存储模型参数的副本。 当训练算法终止时，我们返回这些参数而不是最新的参数。<br>当验证集上的误差在事先指定的循环次数内没有进一步改善时，算法就会终止。</p>
<h2 id="8-参数绑定与参数共享"><a href="#8-参数绑定与参数共享" class="headerlink" title="8. 参数绑定与参数共享"></a>8. 参数绑定与参数共享</h2><p>我们经常想要表达的一种常见依赖是某些参数应当彼此接近。 考虑以下情形：我们有两个模型执行相同的分类任务（具有相同类别），但输入分布稍有不同。<br>形式地，我们有参数为  w  (  A  )  的模型  A  和参数为  w  (  B  )  的模型  B  。<br>这两种模型将输入映射到两个不同但相关的输出：  $$y  ^  (  A  )  =  f  (  w  (  A  )  ,  x  )$$  和 $$y  ^(  B  )  =  f  (  w  (  B  )  ,  x  ) $$ 。</p>
<p>我们可以想象，这些任务会足够相似（或许具有相似的输入和输出分布），因此我们认为模型参数应彼此靠近：  ∀  i  ,  w  (  A  )_i  应该与w  (  B  ) _i  接近。 我们可以通过正则化利用此信息。 具体来说，我们可以使用以下形式的参数范数惩罚：  Ω  (  w  (  A  ),  w  (  B  )  )  =  $$||w  (  A  )  −  w  (  B  )||^2$$  。 在这里我们使用  L2惩罚，但也可以使用其他选择。</p>
<p>参数范数惩罚是正则化参数使其彼此接近的一种方式，而更流行的方法是使用约束： <strong> 强迫某些参数相等 </strong> 。<br>由于我们将各种模型或模型组件解释为共享唯一的一组参数，这种正则化方法通常被称为参数共享。<br>和正则化参数使其接近（通过范数惩罚）相比，参数共享的一个显著优点是，只有参数（唯一一个集合）的子集需要被存储在内存中。<br>对于某些特定模型，如卷积神经网络，这可能可以显著减少模型所占用的内存。</p>
<h2 id="9-Bagging和其他集成方法"><a href="#9-Bagging和其他集成方法" class="headerlink" title="9. Bagging和其他集成方法"></a>9. Bagging和其他集成方法</h2><p>Bagging是通过结合几个模型降低泛化误差的技术。 主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。<br>这是机器学习中常规策略的一个例子，被称为模型平均。 采用这种策略的技术被称为集成方法。</p>
<p>模型平均奏效的原因是不同的模型通常不会在测试集上产生完全相同的误差。</p>
<h2 id="10-Dropout"><a href="#10-Dropout" class="headerlink" title="10.Dropout"></a>10.Dropout</h2><p>Dropout通过随机行为训练网络并平均多个随机决定进行预测，实现了一种 <strong> 参数共享的Bagging </strong> 形式。<br>在训练中使用Dropout时，我们会使用基于小批量产生较小步长的学习算法，如随机梯度下降等。<br>我们每次在小批量中加载一个样本，然后随机抽样应用于网络中所有输入和隐藏单元的不同二值掩码。 对于每个单元，掩码是独立采样的。<br>掩码值为1的采样概率（导致包含一个单元）是训练开始前一个固定的超参数。 它不是模型当前参数值或输入样本的函数。<br>通常在每一个小批量训练的神经网络中，一个输入单元被包括的概率为  0.8  ，一个隐藏单元被包括的概率为  0.5  。<br>然后，我们运行和之前一样的前向传播、反向传播以及学习更新。<br>更正式地说，假设一个掩码向量  u  指定被包括的单元，  J  (  θ  ,  u  )  是由参数  θ  和掩码  u  定义的模型代价。<br>那么Dropout训练的目标是最小化  E  u  J  (  θ  ,  u  )  。 这个期望包含多达指数级的项，但我们可以通过抽样  mu获得梯度的无偏估计。</p>
<p>Dropout训练与Bagging训练不太一样。 在Bagging的情况下，所有模型都是独立的。<br>在Dropout的情况下，所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。 参数共享使得在有限可用的内存下表示指数级数量的模型变得可能。<br>在Bagging的情况下，每一个模型在其相应训练集上训练到收敛。<br>在Dropout的情况下，通常大部分模型都没有显式地被训练，因为通常父神经网络会很大，以致于到宇宙毁灭都不可能采样完所有的子网络。<br>取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。 这些是仅有的区别。<br>除了这些，Dropout与Bagging算法一样。 例如，每个子网络中遇到的训练集确实是有放回采样的原始训练集的一个子集。</p>
<p>Bagging集成必须根据所有成员的累积投票做一个预测。 在这种背景下，我们将这个过程称为 <strong> 推断 </strong> 。<br>目前为止，我们在介绍Bagging和Dropout时没有要求模型具有明确的概率。 现在，我们假定该模型的作用是输出一个概率分布。<br>在Bagging的情况下，每个模型  i  产生一个概率分布  p  (  i  )  (  y  ∣  x  )  。<br>集成的预测由这些分布的算术平均值给出：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170915135742736?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">

<p>其中  p  (  u  )  是训练时采样  u  的概率分布。</p>
<p>虽然Dropout在特定模型上每一步的代价是微不足道的，但在一个完整的系统上使用Dropout的代价可能非常显著。<br>因为Dropout是一个正则化技术，它减少了模型的有效容量。 为了抵消这种影响，我们必须增大模型规模。<br>不出意外的话，使用Dropout时最佳验证集的误差会低很多，但这是以更大的模型和更多训练算法的迭代次数为代价换来的。<br>对于非常大的数据集，正则化带来的泛化误差减少得很小。 在这些情况下，使用Dropout和更大模型的计算代价可能超过正则化带来的好处。</p>
<h2 id="11-对抗训练"><a href="#11-对抗训练" class="headerlink" title="11. 对抗训练"></a>11. 对抗训练</h2><p>我们可以训练分类器为  x  和  x  ′  分配相同的标签。 这鼓励分类器学习一个沿着未标签数据所在流形上任意微小变化都很鲁棒的函数。<br>驱动这种方法的假设是，不同的类通常位于分离的流形上，并且小扰动不会使数据点从一个类的流形跳到另一个类的流形上。<br>进行对抗训练的目的是使分类结果对训练样本 <strong> 小邻域 </strong> 更加鲁棒！</p>
<h2 id="12-切面距离、正切传播和流形正切分类器"><a href="#12-切面距离、正切传播和流形正切分类器" class="headerlink" title="12. 切面距离、正切传播和流形正切分类器"></a>12. 切面距离、正切传播和流形正切分类器</h2><p>正切传播算法训练带有额外惩罚的神经网络分类器，使神经网络的每个输出  f  (  x  )  对已知的变化因素是局部不变的。<br>这些变化因素对应于沿着的相同样本聚集的流形的移动。 这里实现局部不变性的方法是要求  ∇  x  f  (  x  )  与已知流形的切向  v(i)正交，或者等价地通过正则化惩罚  Ω  使  f  在  x  的  v  (  i  )  方向的导数较小：  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170915142914347?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="30%"></div></p>
<div align="left">


<p><div align="center"><br><img src="http://img.blog.csdn.net/20170915142953389?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">

</div></div></div></div></div></div></div>]]></content>
      
        <categories>
            
            <category> Deep Learning Book </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Deep Learning读书笔记2---深度前馈网络]]></title>
      <url>/2017/08/08/Deep%20Learning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B02---%E6%B7%B1%E5%BA%A6%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C/</url>
      <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h1 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1.基本概念"></a>1.基本概念</h1><h2 id="1-1隐藏层："><a href="#1-1隐藏层：" class="headerlink" title="1.1隐藏层："></a>1.1隐藏层：</h2><p>神经网络中输入与输出层之间的中间层，训练数据并没有给出这些层中的每一层所需的输出，所以叫隐藏层。</p>
<h2 id="1-2模型的宽度："><a href="#1-2模型的宽度：" class="headerlink" title="1.2模型的宽度："></a>1.2模型的宽度：</h2><p>网络中的每个隐藏层通常都是向量值的。这些隐藏层的维数决定了模型的宽度。 向量的每个元素都可以被视为起到类似一个神经元的作用。<br>除了将层想象成向量到向量的单个函数，我们也可以把层想象成由许多并行操作的单元组成，每个单元表示一个向量到标量的函数。<br>每个单元在某种意义上类似一个神经元，它接收的输入来源于许多其他的单元，并计算它自己的激活值。  </p>
<h2 id="1-3激活函数："><a href="#1-3激活函数：" class="headerlink" title="1.3激活函数："></a>1.3激活函数：</h2><p>用于计算隐藏层值的函数，常见的有整流线性单元（分段函数max{0,f(x)}）。</p>
<h2 id="1-4常见的代价函数："><a href="#1-4常见的代价函数：" class="headerlink" title="1.4常见的代价函数："></a>1.4常见的代价函数：</h2><p>交叉熵（负的最大似然函数）</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170907102812491?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dis
solve/70/gravity/SouthEast" width="40%"></div></p>
<div align="left">

<h1 id="2-输出单元"><a href="#2-输出单元" class="headerlink" title="2.输出单元"></a>2.输出单元</h1><h2 id="2-1-线性单元"><a href="#2-1-线性单元" class="headerlink" title="2.1 线性单元"></a>2.1 线性单元</h2><p>线性输出层经常被用来产生条件高斯分布的均值：<br>p(y∣x)=N(y;y^,I).</p>
<h2 id="2-2-logistic-sigmoid函数"><a href="#2-2-logistic-sigmoid函数" class="headerlink" title="2.2 logistic sigmoid函数"></a>2.2 logistic sigmoid函数</h2><p>sigmoid单元可用于Bernoulli输出分布，许多任务需要预测二值型变量  y  的值。<br>具有两个类的分类问题可以归结为这种形式。sigmoid输出单元定义为：  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170907104221249?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="20%"></div></p>
<div align="left"> 


<p><div align="center"><br><img src="http://img.blog.csdn.net/20170907104150199?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="25%"></div></p>
<div align="left">

<h2 id="2-3-softmax单元"><a href="#2-3-softmax单元" class="headerlink" title="2.3 softmax单元"></a>2.3 softmax单元</h2><p>任何时候当我们想要表示一个具有n个可能取值的离散型随机变量的分布时，我们都可以使用softmax函数。softmax函数的形式为:  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170907112810709?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dis
solve/70/gravity/SouthEast" width="30%"></div></p>
<div align="left">

<h1 id="3-隐藏单元"><a href="#3-隐藏单元" class="headerlink" title="3.隐藏单元"></a>3.隐藏单元</h1><h2 id="3-1整流线性单元"><a href="#3-1整流线性单元" class="headerlink" title="3.1整流线性单元"></a>3.1整流线性单元</h2><p>整流线性单元使用激活函数  g  (  z  )  =  max  (  0  ,  z  )</p>
<p><strong> 扩展： </strong>   </p>
<p>a.当$$z_i&lt;0$$时使用一个非零的斜率$$α_i:h_i=g(z,α)i=max(0,z_i)+α_imin(0,z_i)$$ 。<br>b. 绝对值整流固定  α  i  =  −  1  来得到  g  (  z  )  =  |  z  |  。<br>它用于图像中的对象识别，其中寻找在输入照明极性反转下不变的特征是有意义的。<br>c. 渗漏整流线性单元将  α_i固定成一个类似0.01的小值，参数化整流线性单元（PReLU）将  α  i 作为学习的参数。<br>d. maxout单元将  z  划分为每组具有  k  个值的组，而不是使用作用于每个元素的函数  g  (  z  )  。<br>每个maxout单元则输出每组中的最大元素。</p>
<h2 id="3-2-logistic-sigmoid与双曲正切函数"><a href="#3-2-logistic-sigmoid与双曲正切函数" class="headerlink" title="3.2 logistic sigmoid与双曲正切函数"></a>3.2 logistic sigmoid与双曲正切函数</h2><p>在引入整流线性单元之前，大多数神经网络使用logistic sigmoid激活函数<br>g(z)=σ(z)<br>或者是双曲正切激活函数<br>g(z)=tanh(z)<br>这些激活函数紧密相关，因为  tanh  (  z  )  =  2  σ  (  2  z  )  −  1  。<br>与分段线性单元不同，sigmoid单元在其大部分定义域内都饱和——当  z  取绝对值很大的正值时，它们饱和到一个高值，当  z取绝对值很大的负值时，它们饱和到一个低值，并且仅仅当z接近0时它们才对输入强烈敏感。<br>sigmoid单元的广泛饱和性会使得基于梯度的学习变得非常困难。 因为这个原因，现在不鼓励将它们用作前馈网络中的隐藏单元。<br>当使用一个合适的代价函数来抵消sigmoid的饱和性时，它们作为输出单元可以与基于梯度的学习相兼容。</p>
<p>当必须要使用sigmoid激活函数时，双曲正切激活函数通常要比logistic sigmoid函数表现更好。 在  tanh(0 )=0而σ(0)=0.5的意义上，它更像是单位函数。因为tanh在0附近与单位函数类似，训练深层神经网络 $$y^=w^⊤tanh(U^⊤tanh(V^⊤x))$$ 类似于训练一个线性模型$$y^= w^⊤U^⊤V^⊤x$$ ，只要网络的激活能够被保持地很小。 这使得训练tanh网络更加容易。</p>
<h1 id="4-架构设计"><a href="#4-架构设计" class="headerlink" title="4.架构设计"></a>4.架构设计</h1><p><strong> 万能近似定理： </strong> 一个前馈神经网络如果具有线性输出层和至少一层具有任何一种”挤压”性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数。 前馈网络的导数也可以任意好地来近似函数的导数。 </p>
<p>在神经网络框架中，更深的神经网络远远比大模型尺寸的单层网络好得多，这说明神经网络中应该用许多简单的函数组成，而尽量不用复杂庞大的单模型。</p>
<h1 id="5-反向传播"><a href="#5-反向传播" class="headerlink" title="5.反向传播"></a>5.反向传播</h1><h2 id="5-1链式法则"><a href="#5-1链式法则" class="headerlink" title="5.1链式法则"></a>5.1链式法则</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170911163754870?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">

<p>从这里我们看到，变量  x  的梯度可以通过~Jacobian矩阵∂y/∂x和梯度∇yz相乘来得到。<br>反向传播算法由图中每一个这样的~Jacobian~梯度的乘积操作所组成。</p>
<h2 id="5-2反向传播步骤："><a href="#5-2反向传播步骤：" class="headerlink" title="5.2反向传播步骤："></a>5.2反向传播步骤：</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170911165232295?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">

<h2 id="5-3符号到符号的导数"><a href="#5-3符号到符号的导数" class="headerlink" title="5.3符号到符号的导数"></a>5.3符号到符号的导数</h2><p>代数表达式和计算图都对符号或不具有特定值的变量进行操作。 这些代数或者基于图的表达式被称为符号表示。<br>当我们实际使用或者训练神经网络时，我们必须给这些符号赋特定的值。 我们用一个特定的数值来替代网络的符号输入  x  ，例如  [  1.2  ,  3  ,7.65  ,  −  1.8  ]  ⊤  。</p>
<p>一些反向传播的方法采用计算图和一组用于图的输入的数值，然后返回在这些输入值处梯度的一组数值。 我们将这种方法称为符号到数值的微分。<br>这种方法用在诸如Torch和Caffe之类的库中。</p>
<p>另一种方法是采用计算图以及添加一些额外的节点到计算图中，这些额外的节点提供了我们所需导数的符号描述。<br>这是Theano和TensorFlow所采用的方法。这种方法的主要优点是导数可以使用与原始表达式相同的语言来描述。<br>因为导数只是另外一张计算图，我们可以再次运行反向传播，对导数再进行求导就能得到更高阶的导数。</p>
<p>基于符号到符号的方法的描述包含了符号到数值的方法。 符号到数值的方法可以理解为执行了与符号到符号的方法中构建图的过程中完全相同的计算。<br>关键的区别是符号到数值的方法不会显示出计算图。</p>
</div></div></div></div></div></div>]]></content>
      
        <categories>
            
            <category> Deep Learning Book </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[DeepLearning读书笔记1-基础知识篇]]></title>
      <url>/2017/08/01/DeepLearning%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B01-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%AF%87/</url>
      <content type="html"><![CDATA[<p><strong>An MIT Press book</strong></p>
<p>Ian Goodfellow and Yoshua Bengio and Aaron Courville</p>
<p><a href="http://www.deeplearningbook.org/" target="_blank" rel="external"> 英文原版 </a>  </p>
<p>感谢大神在GitHub上共享自己的中文翻译  ： <a href="https://github.com/exacity
/deeplearningbook-chinese" target="_blank" rel="external"> Deep-Learning中文PDF版 </a></p>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h1 id="1-线性代数"><a href="#1-线性代数" class="headerlink" title="1. 线性代数"></a>1. 线性代数</h1><h2 id="1-1-张量"><a href="#1-1-张量" class="headerlink" title="1.1  张量"></a>1.1  张量</h2><p>在某些情况下，我们会讨论坐标超过两维的数组。 一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们称之为张量。</p>
<h2 id="1-2-范数"><a href="#1-2-范数" class="headerlink" title="1.2 范数"></a>1.2 范数</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170817103714165?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/" width="70%"> </div></p>
<p><div align="left"></div></p>
<h2 id="1-3-Moore-Penrose-伪逆"><a href="#1-3-Moore-Penrose-伪逆" class="headerlink" title="1.3 Moore-Penrose 伪逆"></a>1.3 Moore-Penrose 伪逆</h2><p>对于非方矩阵而言，其逆矩阵没有定义。 假设在下面的问题中，我们希望通过矩阵A的左逆B来求解线性方程：</p>
<p>$$A  x  =  y$$ </p>
<p>Moore-Penrose 伪逆使我们在这类问题上取得了一定的进展。 矩阵A的伪逆定义为：</p>
<p>$$A^+ =lim_{α↘0}(A^⊤A +αI)^{-1}{A^⊤}$$</p>
<p>计算伪逆的实际算法没有基于这个定义，而是使用下面的公式：</p>
<p>$$A^+  = VD^+U^⊤$$</p>
<p>其中，矩阵V，D和U  是矩阵A奇异值分解后得到的矩阵。 对角矩阵D的伪逆$$D^+$$是其非零元素取倒数之后再转置得到的。</p>
<p>奇异值分解：</p>
<p>$$A = UDV^⊤$$</p>
<p>假设A是一个mxn的矩阵，那么U是一个mxm的矩阵，D是一个mxn的对角矩阵，V是一个nx n的矩阵。</p>
<h2 id="1-4-迹运算"><a href="#1-4-迹运算" class="headerlink" title="1.4 迹运算"></a>1.4 迹运算</h2><p>迹运算返回的是矩阵对角元素的和：</p>
<p>$$Tr  (  A  )  =  ∑   A _{ i  ,  j } $$</p>
<p>迹运算提供了另一种描述矩阵Frobenius 范数的方式：  </p>
<p>$$∥  A  ∥  F  =  Tr  (  A  A  ⊤  )  $$<br>迹运算性质</p>
<p>$$Tr  (  A  B  C  )  =  Tr  (  C  A  B  )  =  Tr  (  B  C  A  )$$</p>
<h1 id="2-概率与信息论"><a href="#2-概率与信息论" class="headerlink" title="2. 概率与信息论"></a>2. 概率与信息论</h1><h2 id="2-1-协方差"><a href="#2-1-协方差" class="headerlink" title="2.1 协方差"></a>2.1 协方差</h2><p>协方差在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度：</p>
<p>$$Cov  (  f  (  x  )  ,  g  (  y  )  )  =  E  [  (  f  (  x  )  −  E  [  f  (x)  ]  )  (  g  (  y  )  −  E  [  g  (  y  )  ]  )  ] $$</p>
<h2 id="2-2-信息论"><a href="#2-2-信息论" class="headerlink" title="2.2 信息论"></a>2.2 信息论</h2><p>香农熵：  </p>
<p>$$H(x)=E<em>{x∼P}[I(x)]=−E</em>{x∼P}[log(x)]$$</p>
<p>一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量。 它给出了对依据概率分布P生成的符号进行编码所需的比特数在平均意义上的下界(当对数底数不是2时，单位将有所不同)。  </p>
<p>如果我们对于同一个随机变量 x有两个单独的概率分布 P(x)和Q(x)，我们可以使用KL散度来衡量这两个分布的差异：</p>
<p>$$ D<em>{KL}(P||Q)=E</em>{x∼P}[logP(x)Q(x)]=E_{x∼P}[logp(x)− logQ(x)] $$</p>
<p>在离散型变量的情况下，KL散度衡量的是，当我们使用一种被设计成能够使得概率分布 Q产生的消息的长度最小的编码，发送包含由概率分布P产生的符号的消息时，所需要的额外信息量。  KL散度是非负的并且衡量的是两个分布之间的差异，它经常被用作分布之间的某种距离。</p>
<p>交叉熵：</p>
<p>$$ H(P,Q)=−E<em>{x∼P}logQ(x)=H(P)+D</em>{KL}(P||Q) $$</p>
<p>最小化交叉熵等价于最小化KL散度，因为当P已知时，H(P)是常量。</p>
<h1 id="3-数值计算"><a href="#3-数值计算" class="headerlink" title="3. 数值计算"></a>3. 数值计算</h1><h2 id="3-1-方向导数"><a href="#3-1-方向导数" class="headerlink" title="3.1 方向导数"></a>3.1 方向导数</h2><p>方向导数  ：是一个数；  反映的是f(x,y)在P0点沿方向v的变化率。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170822171450834?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%" align="center">  </div></p>
<p><div align="left"><br>梯度的方向就是函数f(x,y)在这点增长最快的方向，梯度的模为方向导数的最大值。  </div></p>
<p>最速下降法就是使点的搜索方向与梯度方向相反，达到更为快速收敛的目的。</p>
<h2 id="3-2-基本牛顿法"><a href="#3-2-基本牛顿法" class="headerlink" title="3.2 基本牛顿法"></a>3.2 基本牛顿法</h2><p>基本牛顿法是一种是用导数的算法，它每一步的迭代方向都是沿着当前点函数值下降的方向。</p>
<p>我们主要集中讨论在一维的情形，对于一个需要求解的优化函数 f(x)，求函数的极值的问题可以转化为求函数导函数，对函数 f(x)进行泰勒展开到二阶，得到<br><img src="http://latex.codecogs.com/gif.latex?f\left&amp;space;(&amp;space;x&amp;space;\right&amp;space;)=f\left&amp;space;(&amp;space;x_k&amp;space;\right&amp;space;)+{f}%27\left&amp;space;(&amp;space;x_k&amp;space;\right&amp;space;)\left&amp;space;(&amp;space;x-x_k&amp;space;\right&amp;space;)+\frac{1}{2}{f}%27%27\left&amp;space;(&amp;space;x_k&amp;space;\right&amp;space;)\left&amp;space;(&amp;space;x-x_k&amp;space;\right&amp;space;)^2" _xhe_src="http://latex.codecogs.com/gif.latex?f\left&amp;space;(&amp;space;x&amp;space;\right&amp;space;)=f\left&amp;space;(&amp;space;x_k&amp;space;\right&amp;space;)+{f}%27\left&amp;space;(&amp;space;x_k&amp;space;\right&amp;space;)\left&amp;space;(&amp;space;x-x_k&amp;space;\right&amp;space;)+\frac{1}{2}{f}%27%27\left&amp;space;(&amp;space;x_k&amp;space;\right&amp;space;)\left&amp;space;(&amp;space;x-x_k&amp;space;\right&amp;space;)^2" title="f\left ( x \right )=f\left ( x_k \right )+{f}'\left ( x_k \right )\left ( x-x_k \right )+\frac{1}{2}{f}''\left ( x_k \right )\left ( x-x_k \right )^2" alt="" style="border:none; max-width:100%"></p>
<p>对上式求导并令其为0，<br>即得到<br>$$ x=x_k - f’(x_k)/f”(x_k) $$</p>
<p>这就是牛顿法的更新公式。</p>
<p><strong>流程：</strong></p>
<ol>
<li>给定终止误差值 0&lt;= ε &lt;&lt;1，初始点 $$x_0∈R^n$$，令 k=0; </li>
<li>计算$$g_k =  △f(x_k)$$，若 $$||g_k|| &lt;= ε$$，则停止，输出 $$x* ≈ x_k$$ ； </li>
<li>计算 $$G_k = △^2f(x_k)$$ ，并求解线性方程组得解$$d_k : G_kd = -g_k$$</li>
<li>令 $$x_{k+1}=x_k+d_k$$ , $$k=k+1$$ ，并转2。 </li>
</ol>
<h2 id="3-3-全局牛顿法"><a href="#3-3-全局牛顿法" class="headerlink" title="3.3 全局牛顿法"></a>3.3 全局牛顿法</h2><p>牛顿法最突出的优点是收敛速度快，具有局部二阶收敛性，但是，基本牛顿法初始点需要足够“靠近”极小点，否则，有可能导致算法不收敛。这样就引入了全局牛顿法。</p>
<p><strong>流程</strong></p>
<ol>
<li>给定终止误差值 $$0&lt;= ε &lt;&lt;1,δ∈(0,1),σ∈(0,0.5)$$，初始点 $$x_0∈R^n$$,令 k=0； </li>
<li>计算 $$g_k =  △f(x_k)$$ ，若 $$||g_k|| &lt;= ε$$，则停止，输出 $$x* ≈ x_k$$ ； </li>
<li>计算 $$G_k =  △^2f(x_k)$$ ，并求解线性方程组得解$$d_k : G_kd = -g_k$$ ;</li>
<li>记 $$m_k$$是不满足下列不等式的最小非负整数 $$m:f(x_k +δ^md_k) &lt;= f(x_k)+σδ^mg_k^Td_k;$$； </li>
<li>令 $$α_k=δ^{m<em>k}$$，$$x</em>{k+1}=x_k+α_kd_k$$，$$k=k+1$$，并转2。 </li>
</ol>
<h2 id="Armijo搜索"><a href="#Armijo搜索" class="headerlink" title="Armijo搜索"></a>Armijo搜索</h2><p>全局牛顿法是基于  Armijo  的搜索，满足  Armijo  准则：</p>
<p>给定β∈(0,1),σ∈(0,0.5)，令步长因子$$α_k=β^{m<em>k}$$，$$x</em>{k+1}=x_k+α_kd_k$$，$$k=k+1$$，其中 <img src="http://latex.codecogs.com/gif.latex?m_k" alt=""> 是满足下列不等式的最小非负整数:</p>
<p><img src="http://latex.codecogs.com/gif.latex?f\left&amp;space;(&amp;space;x_k+\beta&amp;space;^md_k&amp;space;\right&amp;space;)\leq&amp;space;f\left&amp;space;(&amp;space;x_k&amp;space;\right&amp;space;)+\sigma&amp;space;\beta&amp;space;^mg_k^Td_k" _xhe_src="http://latex.codecogs.com/gif.latex?f\left&amp;space;(&amp;space;x_k+\beta&amp;space;^md_k&amp;space;\right&amp;space;)\leq&amp;space;f\left&amp;space;(&amp;space;x_k&amp;space;\right&amp;space;)+\sigma&amp;space;\beta&amp;space;^mg_k^Td_k" title="f\left ( x_k+\beta ^md_k \right )\leq f\left ( x_k \right )+\sigma \beta ^mg_k^Td_k" alt="" style="border:none; max-width:100%"></p>
<h2 id="3-4-KKT条件"><a href="#3-4-KKT条件" class="headerlink" title="3.4 KKT条件"></a>3.4 KKT条件</h2><p><a href="http://www.cnblogs.com/zhangchaoyang/articles/2726873.html" target="_blank" rel="external"> 拉格朗日乘子法和KKT条件  </a></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170822183505539?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/di
ssolve/7/gravity/SouthEast" width="70%" align="center">  </div></p>
<div align="left">  

<p><div align="center"><br><img src="http://img.blog.csdn.net/20170822183419304?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/di
ssolve/7/gravity/SouthEast" width="70%" align="center">  </div></p>
<div align="left"> 



<h1 id="4-机器学习基础"><a href="#4-机器学习基础" class="headerlink" title="4. 机器学习基础"></a>4. 机器学习基础</h1><h2 id="4-1-学习算法"><a href="#4-1-学习算法" class="headerlink" title="4.1 学习算法"></a>4.1 学习算法</h2><p><strong>机器学习定义</strong></p>
<p>对于某类任务T和性能度量 P，一个计算机程序被认为可以从经验 E中学习是指，通过经验 E改进后，它在任务T上由性能度量 P衡量的性能有所提升。 </p>
<p><strong>无监督学习算法</strong></p>
<p>训练含有很多特征的数据集，然后学习出这个数据集上有用的结构性质。 在深度学习中，我们通常要学习生成数据集的整个概率分布，显式地，比如密度估计，或是隐式地，比如合成或去噪。 还有一些其他类型的无监督学习任务，例如聚类，将数据集分成相似样本的集合。 </p>
<p><strong>监督学习算法</strong> </p>
<p>训练含有很多特征的数据集，不过数据集中的样本都有一个标签或目标。 例如，Iris数据集注明了每个鸢尾花卉样本属于什么品种。 监督学习算法通过研究Iris数据集，学习如何根据测量结果将样本划分为三个不同品种。 </p>
<p><strong>欠拟合</strong> </p>
<p>是指模型不能在训练集上获得足够低的误差。 而 <strong>过拟合</strong> 是指训练误差和和测试误差之间的差距太大。 </p>
<h2 id="4-2-交叉验证"><a href="#4-2-交叉验证" class="headerlink" title="4.2 交叉验证"></a>4.2 交叉验证</h2><p><strong>验证集：</strong> </p>
<p>训练数据分成两个不相交的子集。 其中一个用于学习参数，另一个作为验证集，用于估计训练中或训练后的泛化误差，更新超参数。  由于验证集是用来”训练”超参数的，尽管验证集的误差通常会比训练集误差小，验证集会低估泛化误差。 所有超参数优化完成之后，泛化误差可能会通过测试集来估计。    </p>
<p><strong>k折交叉验证过程：</strong> </p>
<p>将数据集分成k个不重合的子集。 测试误差可以估计为k次计算后的平均测试误差。 在第i次测试时，数据的第i个子集用于测试集，其他的数据用于训练集。    </p>
<h2 id="4-3-点估计"><a href="#4-3-点估计" class="headerlink" title="4.3 点估计"></a>4.3 点估计</h2><p>令$${x(1),…,x^{(m)}}$$是m个独立同分布的数据点。 点估计或统计量是这些数据的任意函数：</p>
<p>$$θ  ^  m  =  g  (  x  (  1  )  ,  …  ,  x  (  m  )  )  $$</p>
<p>这个定义不要求g返回一个接近真实θ的值，或者g的值域恰好是  θ  的允许取值范围。 点估计的定义非常宽泛，给了估计量的设计者极大的灵活性。<br>虽然几乎所有的函数都可以称为估计量，但是一个良好的估计量的输出会接近生成训练数据的真实参数  θ  。  </p>
<h2 id="4-4-一致性"><a href="#4-4-一致性" class="headerlink" title="4.4 一致性"></a>4.4 一致性</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170825181336921?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/di
ssolve/7/gravity/Center" width="70%" align="center">  </div></p>
<div align="left">   



<h2 id="4-5-最大似然估计（MLE）"><a href="#4-5-最大似然估计（MLE）" class="headerlink" title="4.5 最大似然估计（MLE）"></a>4.5 最大似然估计（MLE）</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170828114118676?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/di
ssolve/7/gravity/Center" width="70%" align="center">  </div></p>
<div align="left"> 

<p>通常运用于解决 <strong>模型已知，参数未知</strong> 的情况！</p>
<p>注意：最大似然估计只考虑某个模型能产生某个给定观察序列的概率。而未考虑该模型本身的概率，这点与贝叶斯估计区别。  </p>
<p><strong>最大似然估计解决线性回归</strong></p>
<p>设样本数据数据（xi,yi）,现用线性方程的形式去做回归，线性方程结构为：y=f(x)+e,其中f(x)=w<em>x，w是我们要进行估计的参数向量，e是噪声，且该噪声服从均值为0高斯分布，即：$$e ~ N(0,sigema^2)$$.<br>假设有一个w（此w并不是最终的w），对于 <strong> 每一个 </strong> 实际得到的数据我们都可以看成是由均值为w</em>xi,方差为$$sigema^2$$的一个高斯模型生成的。但是在w不定的情况下，高斯模型有无数种可能，我们需要从中选择一个我们想要的，而选择需要一个准则，该准则就是：使得在该模型下生成我们获得的数据的可能性最大。<br>问题来了，什么叫可能性最大？怎样来衡量？——&gt;将每一个数据产生的概率连乘起来（似然函数），将该值最为总的数据产生的可能性。很容易理解。</p>
<h2 id="4-6-贝叶斯估计"><a href="#4-6-贝叶斯估计" class="headerlink" title="4.6 贝叶斯估计"></a>4.6 贝叶斯估计</h2><p><a href="http://blog.csdn.net/guohecang/article/details/52313046" target="_blank" rel="external"> 先验概率、最大似然估计、贝叶斯估计、最大后验概率</a></p>
<p><a href="http://www.cnblogs.com/little-YTMM/p/5399532.html" target="_blank" rel="external">贝叶斯思想以及与最大似然估计、最大后验估计的区别</a></p>
<p>贝叶斯估计，是在给定训练数据D时，确定假设空间H中的最佳假设。 最佳假设：一种方法是把它定义为在给定数据D以及H中不同假设的先验概率的有关知识下的最可能假设。贝叶斯理论提供了一种计算假设概率的方法，基于假设的先验概率、给定假设下观察到不同数据的概率以及观察到的数据本身。</p>
<p>1、 <strong>先验分布</strong> 。总体分布参数θ的一个概率分布。贝叶斯学派的根本观点，是认为在关于总体分布参数θ的任何统计推断问题中，除了使用样本所提供的信息外，还必须规定一个先验分布，它是在进行统计推断时不可缺少的一个要素。他们认为先验分布不必有客观的依据，可以部分地或完全地基于主观信念。</p>
<p>2、 <strong>后验分布</strong> 。根据样本分布和未知参数的先验分布，用概率论中求条件概率分布的方法，求出的在样本已知下，未知参数的条件分布。因为这个分布是在抽样以后才得到的，故称为后验分布。贝叶斯推断方法的关键是任何推断都必须且只须根据后验分布，而不能再涉及样本分布。</p>
<p>贝叶斯公式为：</p>
<p>$$ P(A∩B)=P(A)<em>P(B|A)=P(B)</em>P(A|B) $$</p>
<p>$$P(A|B)=P(B|A)*P(A)/P(B)$$</p>
<p>其中：</p>
<p>1、P(A)是A的先验概率或边缘概率，称作”先验”是因为它不考虑B因素。</p>
<p>2、P(A|B)是已知B发生后A的条件概率  ，也称作A的后验概率  。</p>
<p>3、P(B|A)是已知A发生后B的条件概率，也称作B的后验概率，这里称作似然度  。</p>
<p>4、P(B)是B的先验概率或边缘概率，这里称作标准化常量。</p>
<p>5、P(B|A)/P(B)称作标准似然度。</p>
<h2 id="4-7-最大后验概率（MAP）"><a href="#4-7-最大后验概率（MAP）" class="headerlink" title="4.7 最大后验概率（MAP）"></a>4.7 最大后验概率（MAP）</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20171018144947922?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/di
ssolve/70/gravity/Center" width="60%" align="center">  </div></p>
<div align="left"> 

<p><img src="http://pic002.cnblogs.com/images/2010/156169/2010112402010597.png" alt=""></p>
<p>注：最大后验估计可以看做贝叶斯估计的一种特定形式。</p>
<p><strong>各估计算法的区别：</strong></p>
<ol>
<li>ML（最大似然估计）：就是给定一个模型的参数  θ  ，然后试着  最大化p(D|  θ  )  。即给定参数的情况下，看到样本集的概率。目标是找到使前面概率最大的参数。 <ol>
<li>逻辑回归都是基于ML做的； </li>
<li>缺点：不会把我们的先验知识加入模型中。 </li>
</ol>
</li>
<li>MAP（最大后验估计）：最大化p(  θ  |D)。 </li>
<li>Bayesian：我们的预测是考虑了所有可能的参数，即所有的参数空间（参数的分布） </li>
</ol>
<p>MAP与ML最大的不同在于p(θ)项，MAP可以解决ML缺乏先验知识的缺点，将先验知识加入后，优化损失函数。</p>
<p>其实p(  θ  )项正好起到了正则化的作用。如：如果假设p(  θ  )服从高斯分布，则相当于加了一个  L2 norm  ；如果假设p(θ)服从拉普拉斯分布，则相当于加了一个  L1 norm</p>
<h2 id="4-8-支持向量机（SVM）"><a href="#4-8-支持向量机（SVM）" class="headerlink" title="4.8 支持向量机（SVM）"></a>4.8 支持向量机（SVM）</h2><p>支持向量机的一个重要创新是核技巧。 核技巧观察到许多机器学习算法都可以写成样本间点积的形式。<br>最常用的核函数是高斯核：  </p>
<p>k  (  u  ,  v  )  =  N  (  u  −  v  ;  0  ,  σ^2 )  ,</p>
<p>其中N(x; u, Sigma)是标准正态密度。 这个核也被称为径向基函数核，因为其值沿v中从u向外辐射的方向减小。<br>高斯核对应于无限维空间中的点积，但是该空间的推导没有整数上最小核的示例那么直观。<br>我们可以认为高斯核在执行一种 <strong>模板匹配</strong> 。 训练标签 y相关的训练样本 x变成了类别y的模版。<br>当测试点x‘到x的欧几里得距离很小，对应的高斯核响应很大时，表明x’和模版x非常相似。 该模型进而会赋予相对应的训练标签 y较大的权重。<br>总的来说，预测将会组合很多这种通过训练样本相似度加权的训练标签。</p>
<p>Tips: 利用高斯核函数将低维数据映射到无限维，以达到线性可分的目的。</p>
<h2 id="4-9-k均值聚类"><a href="#4-9-k均值聚类" class="headerlink" title="4.9 k均值聚类"></a>4.9 k均值聚类</h2><p>k均值聚类初始化k个不同的中心点$$(u_1,…,u_k)$$，然后迭代交换两个不同的步骤直到收敛。<br>步骤一，每个训练样本分配到最近的中心点ui所代表的聚类i。<br>步骤二，每一个中心点ui更新为聚类i中所有训练样本 xj的均值。  </p>
<p>关于聚类的一个问题是聚类问题本身是病态的。 这是说没有单一的标准去度量聚类的数据在真实世界中效果如何。<br>我们可以度量聚类的性质，例如类中元素到类中心点的欧几里得距离的均值。 这使我们可以判断从聚类分配中重建训练数据的效果如何。<br>然而我们不知道聚类的性质是否很好地对应到真实世界的性质。 此外，可能有许多不同的聚类都能很好地对应到现实世界的某些属性。<br>我们可能希望找到和一个特征相关的聚类，但是得到了一个和任务无关的，同样是合理的不同聚类。  </p>
<h2 id="4-10-随机梯度下降"><a href="#4-10-随机梯度下降" class="headerlink" title="4.10 随机梯度下降"></a>4.10 随机梯度下降</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170830095345174?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/di
ssolve/70/gravity/Center" width="70%" align="center">  </div></p>
<div align="left">   



<h2 id="4-11-促使深度学习发展的挑战"><a href="#4-11-促使深度学习发展的挑战" class="headerlink" title="4.11 促使深度学习发展的挑战"></a>4.11 促使深度学习发展的挑战</h2><p><strong>维数灾难 </strong> </p>
<p>由维数灾难带来的一个挑战是统计挑战。 统计挑战产生于x的可能配置数目远大于训练样本的数目。</p>
<p><strong>局部不变性和平滑正则化</strong></p>
<p>只要在要学习的真实函数的峰值和谷值处有足够多的样本，那么平滑性假设和相关的无参数学习算法的效果都非常好。<br>当要学习的函数足够平滑，并且只在少数几维变化，这样做一般没问题。 在高维空间中，即使是非常平滑的函数，也会在不同维度上有不同的变化方式。<br>如果函数在不同的区间中表现不一样，那么就非常难用一组训练样本去刻画函数。 如果函数是复杂的（我们想区分多于训练样本数目的大量区间），有希望很好地泛化么？</p>
<p><strong>流形学习</strong></p>
<p>流形指连接在一起的区域。 数学上，它是指一组点，且每个点都有其邻域。 给定一个任意的点，其流形局部看起来像是欧几里得空间。<br>日常生活中，我们将地球视为二维平面，但实际上它是三维空间中的球状流形。<br>机器学习中倾向于更松散地定义一组点，只需要考虑少数嵌入在高维空间中的自由度或维数就能很好地近似。 每一维都对应着局部的变化方向。  </p>
<p>流形学习算法通过一个假设来克服这个障碍，该假设认为Rn中大部分区域都是无效的输入，有意义的输入只分布在包含少量数据点的子集构成的一组流形中，而学习函数的输出中，有意义的变化都沿着流形的方向或仅发生在我们切换到另一流形时。  </p>
</div></div></div></div></div></div>]]></content>
      
        <categories>
            
            <category> Deep Learning Book </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[图像插值中的优化]]></title>
      <url>/2017/07/27/%E5%9B%BE%E5%83%8F%E6%8F%92%E5%80%BC%E4%B8%AD%E7%9A%84%E4%BC%98%E5%8C%96/</url>
      <content type="html"><![CDATA[<p>文章参考 <a href="http://www.cnblogs.com/yssongest/p/5303151.html" target="_blank" rel="external"> OpenCV ——双线性插值（Bilinear interpolation）
</a></p>
<p>一般图像插值算法中有两处可以进行优化：</p>
<h2 id="1-源图像和目标图像几何中心的对齐"><a href="#1-源图像和目标图像几何中心的对齐" class="headerlink" title="1. 源图像和目标图像几何中心的对齐"></a>1. 源图像和目标图像几何中心的对齐</h2><p>在计算源图像的虚拟浮点坐标的时候，一般情况：  </p>
<p>srcX=dstX<em> (srcWidth/dstWidth) ,<br>srcY = dstY </em> (srcHeight/dstHeight)  </p>
<p><strong> 中心对齐(OpenCV也是如此)： </strong>   </p>
<p>SrcX=(dstX+0.5)<em> (srcWidth/dstWidth) -0.5<br>SrcY=(dstY+0.5) </em> (srcHeight/dstHeight)-0.5  </p>
<p><strong> 原理： </strong></p>
<p>将公式变形：srcX=dstX<em> (srcWidth/dstWidth)+0.5</em>(srcWidth/dstWidth-1)相当于我们在原始的浮点坐标上加上了0.5*(srcWidth/dstWidth-1)这样一个控制因子，这项的符号可正可负，与srcWidth/dstWidth的比值也就是当前插值是扩大还是缩小图像有关，有什么作用呢？</p>
<p>看一个例子：假设源图像是3<em>3，中心点坐标（1，1）目标图像是9</em>9，中心点坐标（4，4），我们在进行插值映射的时候，尽可能希望均匀的用到源图像的像素信息，最直观的就是（4,4）映射到（1,1）现在直接计算srcX=4<em>3/9=1.3333！=1，也就是我们在插值的时候所利用的像素集中在图像的右下方，而不是均匀分布整个图像。现在考虑中心点对齐，srcX=(4+0.5)</em>3/9-0.5=1，刚好满足我们的要求。  </p>
<h2 id="2-将浮点运算转换成整数运算"><a href="#2-将浮点运算转换成整数运算" class="headerlink" title="2. 将浮点运算转换成整数运算"></a>2. 将浮点运算转换成整数运算</h2><p>直接进行计算的话，由于计算的srcX和srcY都是浮点数，后续会进行大量的乘法，而图像数据量又大，速度不会理想，解决思路是：浮点运算→→整数运算→→”&lt;&lt;左右移按位运算”。<br>放大的主要对象是u，v这些浮点数， <strong> OpenCV选择的放大倍数是2048 </strong> “如何取这个合适的放大倍数呢，要从三个方面考虑，第一：精度问题，如果这个数取得过小，那么经过计算后可能会导致结果出现较大的误差。第二，这个数不能太大，太大会导致计算过程超过长整形所能表达的范围。第三：速度考虑。假如放大倍数取为12，那么算式在最后的结果中应该需要除以12x12=144，但是如果取为16，则最后的除数为16x16=256，这个数字好，我们可以用右移来实现，而右移要比普通的整除快多了(浮点数如何右移实现放大缩小？存疑！)。”我们利用左移11位操作就可以达到放大目的。</p>
]]></content>
      
        <categories>
            
            <category> Algorithm Optimization </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Algorithm Optimization </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Lucas–Kanade光流算法]]></title>
      <url>/2017/07/25/Lucas%E2%80%93Kanade%E5%85%89%E6%B5%81%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h2 id="1-光流法"><a href="#1-光流法" class="headerlink" title="1. 光流法"></a>1. 光流法</h2><p>光流是一种运动模式，这种运动模式指的是一个物体、表面、边缘在一个视角下由一个观察者（比如眼睛、摄像头等）和背景之间形成的明显移动。光流技术，如运动检测和图像分割，时间碰撞，运动补偿编码，三维立体视差，都是利用了这种边缘或表面运动的技术。<br>二维图像的移动相对于观察者而言是三维物体移动的在图像平面的投影。有序的图像可以估计出二维图像的瞬时图像速率或离散图像转移。<br>光流算法评估了两幅图像的之间的变形，它的基本假设是体素和图像像素守恒。它假设一个物体的颜色在前后两帧没有巨大而明显的变化。基于这个思路，我们可以得到图像约束方程。不同的光流算法解决了假定了不同附加条件的光流问题。</p>
<h2 id="2-Lucas–Kanade光流算法"><a href="#2-Lucas–Kanade光流算法" class="headerlink" title="2. Lucas–Kanade光流算法"></a>2. Lucas–Kanade光流算法</h2><p>在计算机视觉中，Lucas–Kanade光流算法是一种两帧差分的光流估计算法。它由Bruce D. Lucas 和 Takeo Kanade提出。</p>
<h3 id="2-1-假设条件"><a href="#2-1-假设条件" class="headerlink" title="2.1 假设条件"></a>2.1 假设条件</h3><p>（1）亮度恒定，就是同一点随着时间的变化，其亮度不会发生改变。这是基本光流法的假定（所有光流法变种都必须满足），用于得到光流法基本方程；</p>
<p>（2）小运动，这个也必须满足，就是时间的变化不会引起位置的剧烈变化，这样灰度才能对位置求偏导（换句话说，小运动情况下我们才能用前后帧之间单位位置变化引起的灰度变化去近似灰度对位置的偏导数），这也是光流法不可或缺的假定；</p>
<p>（3）空间一致，一个场景上邻近的点投影到图像上也是邻近点，且邻近点速度一致。这是Lucas-Kanade光流法特有的假定，因为光流法基本方程约束只有一个，而要求x，y方向的速度，有两个未知变量。我们假定特征点邻域内做相似运动，就可以连立n多个方程求取x，y方向的速度（n为特征点邻域总点数，包括该特征点）。</p>
<h3 id="2-2-Lucas–Kanade算法原理"><a href="#2-2-Lucas–Kanade算法原理" class="headerlink" title="2.2 Lucas–Kanade算法原理"></a>2.2 Lucas–Kanade算法原理</h3><p>这个算法是最常见，最流行的。它计算两帧在时间t 到t + δt之间每个每个像素点位置的移动。 由于它是基于图像信号的泰勒级数，这种方法称为差分，这就是对于空间和时间坐标使用偏导数。</p>
<p>图像约束方程可以写为:<br>   $$I (x ,y ,z ,t ) = I (x + δx ,y + δy ,z + δz ,t + δt )$$<br>I(x, y,z, t) 为在（x,y,z）位置的体素。</p>
<p>我们假设移动足够的小，那么对图像约束方程使用泰勒公式，我们可以得到：<br>$$I(x+\delta x,y+\delta y,z+\delta z,t+\delta t) = I(x,y,z,t) + \frac{\partial I}{\partial x}\delta x+\frac{\partial I}{\partial y}\delta y+\frac{\partial I}{\partial z}\delta z+\frac{\partial I}{\partial t}\delta t+H.O.T. $$<br>H.O.T. 指更高阶，在移动足够小的情况下可以忽略。<br>从这个方程中我们可以得到：</p>
<p>$$\frac{\partial I}{\partial x}\delta x+\frac{\partial I}{\partial y}\delta y+\frac{\partial I}{\partial z}\delta z+\frac{\partial I}{\partial t}\delta t = 0$$</p>
<p>或者</p>
<p>$$\frac{\partial I}{\partial x}\frac{\delta x}{\delta t}+\frac{\partial I}{\partial y}\frac{\delta y}{\delta t}+\frac{\partial I}{\partial z}\frac{\delta z}{\delta t}+\frac{\partial I}{\partial t}\frac{\delta t}{\delta t} = 0$$</p>
<p>进而得到</p>
<p>$$\frac{\partial I}{\partial x}V_x+\frac{\partial I}{\partial y}V_y+\frac{\partial I}{\partial z}V_z+\frac{\partial I}{\partial t} = 0$$</p>
<p>Vx,Vy,Vz分别是I(x,y,z,t)的光流向量中x，y，z的组成。 $$\frac{\partial I}{\partial x}$$, $$\frac{\partial I}{\partial y}$$, $$\frac{\partial I}{\partial z}$$和 $$\frac{\partial I}{\partial t}$$则是图像在(x,y,z,t)这一点向相应方向的差分。<br>所以：</p>
<p>$$I_xV_x + I_yV_y + I_zV_z= I_t$$</p>
<p>写做：</p>
<p>$$\nabla I^T\cdot\vec{V} = -I_t$$</p>
<p>这个方程有三个未知量，尚不能被解决，这也就是所谓光流算法的光圈问题。那么要找到光流向量则需要另一套解决的方案。而Lucas-Kanade算法是一个非迭代的算法：</p>
<p>假设流(Vx,Vy,Vz)在一个大小为m<em>m</em>m(m&gt;1)的小窗中是一个常数，那么从像素1…n, $$n = m^3$$中可以得到下列一组方程</p>
<p><div align="left"><br><img src="http://img.blog.csdn.net/20171114215124825?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="85%" align="center"></div></p>
<p>为了解决这个超定问题，我们采用最小二乘法：</p>
<p>$$A^TA\vec{v}=A^T(-b)$$</p>
<p>$$\vec{v}=(A^TA)^{-1}A^T(-b)$$</p>
<p>得到：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171114215135763?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"></div></p>
<div align="left">

<p>其中的求和是从1到n。</p>
<p>这也就是说寻找光流可以通过在四维上图像导数的分别累加得出。我们还需要一个权重函数$$W(i, j,k)$$， $$i,j,k \in [1,m]$$来突出窗口中心点的坐标。</p>
<p>这个算法的不足在于它不能产生一个密度很高的流向量，例如在运动的边缘和黑大的同质区域中的微小移动方面流信息会很快的褪去。它的优点在于有噪声存在的鲁棒性还是可以的。</p>
<p>参考文献：Lucas B and Kanade T. An Iterative Image RegistrationTechnique with an Application to Stereo Vision. Proc. Of 7th InternationalJoint Conference on Artificial Intelligence (IJCAI), pp.674-679.是81年发表的。</p>
<p>opencv中实现的是84年发表的算法，</p>
<p>参考文献：Bruce D. Lucas,”Generalized Image Matching by the Method of Differences,” doctoraldissertation, tech. report , Robotics Institute, Carnegie Mellon University,July, 1984</p>
<h3 id="2-3-算法改进"><a href="#2-3-算法改进" class="headerlink" title="2.3 算法改进"></a>2.3 算法改进</h3><h4 id="2-3-1-构建图像金字塔"><a href="#2-3-1-构建图像金字塔" class="headerlink" title="2.3.1 构建图像金字塔"></a>2.3.1 构建图像金字塔</h4><p>Jean-Yves Bouguet提出一种基于金字塔分层，针对仿射变换的改进Lucas-Kanade算法。</p>
<p>为什么要用金字塔？因为LK算法的约束条件即：小速度，亮度不变以及区域一致性都是较强的假设，并不很容易得到满足。如当物体运动速度较快时，假设不成立，那么后续的假设就会有较大的偏差，使得最终求出的光流值有较大的误差。构建图像金字塔可以解决大运动目标跟踪，也可以一定程度上解决孔径问题（相同大小的窗口能覆盖大尺度图片上尽量多的角点，而这些角点无法在原始图片上被覆盖）。<br>考虑物体的运动速度较大时，算法会出现较大的误差。那么就希望能减少图像中物体的运动速度。一个直观的方法就是，缩小图像的尺寸。假设当图像为400×400时，物体速度为[16 16],那么图像缩小为200×200时，速度变为[8,8]。缩小为100*100时，速度减少到[4,4]。所以光流可以通过生成 原图像的金字塔图像，逐层求解，不断精确来求得。简单来说上层金字塔（低分辨率）中的一个像素可以代表下层的两个。</p>
<p>假设I和J是两幅2D的灰度图像，对于图像上每个像素点的灰度值定义为：<br>$$I(x)=I(x,y)$$   和  $$J(x)=j(x,y)$$<br>其中$$x=(x,y)$$是图像上像素点的图像坐标。<br>在实际场景中图像I和图像J可以代表前后两帧图像。对于图像特征点金字塔跟踪来说的目的是：对于前一帧的图像I上一点$$u(u_x,u_y)$$，要在后一帧图像J上找到一点$$v(u_x+d_x,u_y+d_y)$$与之相匹配，即灰度值最接近。那么向量$$d=[d_x,d_y]$$就是图像在点u处的运动速度，也就是所说像素点u的光流。<br>对于Lucas-Kanade改进算法来说，主要的步骤有三步：建立金字塔，基于金字塔跟踪，迭代过程。</p>
<p><strong>金字塔的建立</strong></p>
<p>令I0 = I 是第 0 层的图像，它是金字塔图像中分辨率最高的图像，图像的宽度和高度分别定义为nx0 = nx 和 ny0 = ny 。以一种递归的方式建立金字塔：从I0中计算I1，从I1中计算I2 ，···。用一个[0.25 0.5 0.25]的低通滤波器对$$I_{L-1}$$进行卷积。</p>
<p><strong>金字塔跟踪</strong></p>
<p>总体来讲，金字塔特征跟踪算法描述如下：首先，光流和仿射变换矩阵在最高一层的图像上计算出；将上一层的计算结果作为初始值传递给下一层图像，这一层的图像在这个初始值的基础上，计算这一层的光流和仿射变化矩阵；再将这一层的光流和仿射矩阵作为初始值传递给下一层图像，直到传递给最后一层，即原始图像层，这一层计算出来的光流和仿射变换矩阵作为最后的光流和仿射变换矩阵的结果。</p>
<p>用L+1层得到的最初估计gL对L层作<strong>预平移</strong>，L层在gL的基础上求该层的光流dL，这样求得的残余光流向量$$ dL=[d_{L<em>x},d</em>{L_y}]^T $$就足够小，因此能够通过标准的光流法来求出这个运动矢量。然后得到的dL结合gL又可以对L-1层的gL-1做估计。最终的光流和就是在所有层的分段光流d的叠加。使用金字塔图像计算光流的一个明显的好处是，对于一个有着较大的像素偏移的矢量d，可以通过计算几个比较小的残余光流来得到。这里就是金字塔跟踪算法的核心。</p>
<p>这种算法最明显的优势在于对于每一层的光流都会保持很小，但是最终计算来的光流可以进行累积，便于有效地跟踪特征点。</p>
<p><strong>迭代过程</strong></p>
<p>在金字塔的每一层，通过迭代计算出光流dL和仿射变换矩阵AL从而使误差ξL最小。</p>
<h4 id="2-3-2-引入权重"><a href="#2-3-2-引入权重" class="headerlink" title="2.3.2 引入权重"></a>2.3.2 引入权重</h4><p>在一个小的邻域内，我们通过对下式的加权平方和最小化来估计V：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170928223931959?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/1/fill/I0JBQkFCMA==/dissolve/7/gravity/SouthEast" width="30%" align="center"></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170928223950228?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170928224946668?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/4/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"></div></p>
<div align="left"> 

<h4 id="2-3-3-特征点选择"><a href="#2-3-3-特征点选择" class="headerlink" title="2.3.3 特征点选择"></a>2.3.3 特征点选择</h4><p>为了使得$$(A^TA)^{-1} $$稳定，需要该矩阵的两个特征值不能太小，所以在构建KL方程时可以只挑选合适的特征点（角点）进行计算：</p>
<p>1、计算图像 I 中每一个像素的矩阵G和最小特征值λm。<br>2、寻找整副图像中最小特征值 λm 中的最大特征值λmax。<br>3、保留最小特征值 λm 大于给定阈值的像素点。阈值通常取$$0.05λ<em>{max}-0.01λ</em>{max}$$。<br>4、保留 λm 局部最大值的像素：像素特征值 λm 大于其3x3 邻域中其他像素的特征值 λm 。<br>5、剔除像素密集区域中的一些像素，确保图像中相邻像素的距离都大于给定的阈值（常取5~10 pixels）。<br>上述操作完成后，图像 I 中剩下的像素即为选择的特征点，并作为跟踪特征点。特征点选择算法的步骤5 确保了特征点间的最小距离。<br>没有必要取一个大的综合窗口选择特征点（或计算矩阵G）。大量实验证明，wx = wy =1的 3x3 大小的综合窗口能够取得满意的效果。</p>
<p><strong>参考博文：</strong></p>
<p><a href="http://m.blog.csdn.net/qq_19764963/article/details/42583467" target="_blank" rel="external">Lucas-Kanade光流法</a></p>
<p><a href="http://blog.csdn.net/u014568921/article/details/46638557" target="_blank" rel="external">目标跟踪之Lukas-Kanade光流法</a></p>
</div></div>]]></content>
      
        <categories>
            
            <category> Machine Vision </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Algorithm Optimization </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[定点数与浮点数]]></title>
      <url>/2017/07/22/%E5%AE%9A%E7%82%B9%E6%95%B0%E4%B8%8E%E6%B5%AE%E7%82%B9%E6%95%B0/</url>
      <content type="html"><![CDATA[<p>转载自：<a href="http://www.cnblogs.com/kevinq/p/4480563.html" target="_blank" rel="external">定点数与浮点数</a></p>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<p>本文主要介绍了定点数和浮点数的概念，定点数和浮点数的加减运算（比如34.6f-34.0f），最后介绍了浮点数的特殊值。</p>
<h2 id="I-定点数"><a href="#I-定点数" class="headerlink" title="I.定点数"></a>I.定点数</h2><p>所谓定点格式，即约定机器中所有数据的小数点位置是固定不变的。通常将定点数据表示成纯小数或纯整数，为了将数表示成纯小数，通常把小数点固定在数值部分的最高位之前；而为了将数表示成纯整数，则把小数点固定在数值部分的最后面，如下图所示：</p>
<p> <img src="http://img.blog.csdn.net/20170930183705140?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>图中所标示的小数点在机器中是不表示出来的，而是事先约定在固定的位置。对于一台计算机，一旦确定了小数点的位置，就不再改变。</p>
<p>假设用n位来表示一个定点数$$ x=x_0x_1x<em>2…x</em>(n-1) $$，其中x0用来表示数的符号位，通常放在最左位置，并用数值0和1分别表示正号和负号，其余位数表示它的量值。</p>
<p>如果定点数x表示纯整数，则小数点位于最低位x(n-1)的右边，数值范围是$$0&lt;=|x|&lt;=2^{(n-1)}-1$$，例如1111表示-7；如果定点数x表示纯小数，则小数点位于x0和x1之间，数值范围是$$ 0&lt;=|x|&lt;=1-2^{(-(n-1))}$$，例如1111表示-0.875.</p>
<h2 id="II-定点数加减运算"><a href="#II-定点数加减运算" class="headerlink" title="II.定点数加减运算"></a>II.定点数加减运算</h2><p>不论操作数是正还是负，在做补码加减法时，只需将符号位和数值部分一起参与运算，并且将符号位产生的进位丢掉即可。如：</p>
<p>short A=-9, B=-5;</p>
<p>A+B = -14</p>
<p>推导过程如下：</p>
<p>A的原码为：1000 0000 0000 1001，因此补码为：1111 1111 1111 0111</p>
<p>B的原码为：1000 0000 0000 0101，因此补码位：1111 1111 1111 1011</p>
<p>A+B的补码为：1 1111 1111 1111 0010，将符号位产生的进位丢掉，因此最终结果为：</p>
<p>1111 1111 1111 0010，结果的原码为：1000 0000 0000 1110，即-14。</p>
<h2 id="III-定点数加减运算的溢出判断"><a href="#III-定点数加减运算的溢出判断" class="headerlink" title="III.定点数加减运算的溢出判断"></a>III.定点数加减运算的溢出判断</h2><h3 id="1）用一位符号位判断溢出"><a href="#1）用一位符号位判断溢出" class="headerlink" title="1）用一位符号位判断溢出"></a>1）用一位符号位判断溢出</h3><p>对于加法，只有在正数加正数和负数加负数两种情况下才可能出现溢出，符号不同的两个数相加是不会溢出的。</p>
<p>对于减法，只有在正数减负数和负数减正数两种情况下才可能出现溢出，符号相同的两个数相减是不会溢出的。</p>
<p>由于减法运算在机器中是用加法器实现的，因此：<strong>不论是作加法还是减法，只要实际操作数（减法时即为被减数和“求补”之后的减数）的补码符号位相同，而结果的符号位又与操作数补码符号位不同，即为溢出。</strong>如：</p>
<p>在4位机中，A=5，B=-4，则A-B溢出，推导过程如下：</p>
<p>A的原码为0101，补码为0101；-B的原码为0100，补码为0100； A-B的补码为1001，结果的符号位为1，实际操作数的符号位为0，因此溢出。</p>
<h3 id="2）用两位符号位判断溢出"><a href="#2）用两位符号位判断溢出" class="headerlink" title="2）用两位符号位判断溢出"></a>2）用两位符号位判断溢出</h3><p>此时判断溢出的原则是：当2位符号位不同时，表示溢出；否则无溢出。不论是否发生溢出，高位符号位永远代表真正的符号。如：</p>
<p>x=-0.1011，y=-0.0111，则x+y溢出，推导过程如下：</p>
<p>x的原码为11.1011，补码为11.0101；y的原码为11.0111，补码为11.1001，因此x+y的补码为1 10.1110，将符号位产生的进位丢掉，则结果为10.1110，因此溢出。</p>
<p>注：约定整数的符号位与数值位之间用逗号隔开，小数的符号位与数值位之间用小数点隔开。</p>
<h2 id="IV-浮点数"><a href="#IV-浮点数" class="headerlink" title="IV.浮点数"></a>IV.浮点数</h2><p>定点数表示法的缺点在于其形式过于僵硬，固定的小数点位置决定了固定位数的整数部分和小数部分，不利于同时表达特别大或特别小的数，最终，绝大多数现代的计算机系统采纳了浮点数表达方式，这种表达方式利用科学计数法来表达实数，即用一个尾数(Mantissa，尾数有时也称为有效数字，它实际上是有效数字的非正式说法)，一个基数(Base)，一个指数(Exponent)以及一个表示正负的符号来表达实数，比如123.45用十进制科学计数法可以表示为$$1.2345x10^2$$，其中1.2345为尾数，10为基数，2为指数。浮点数利用指数达到了浮动小数点的效果，从而可以灵活地表达更大范围的实数。</p>
<h3 id="1-IEEE浮点数"><a href="#1-IEEE浮点数" class="headerlink" title="1) IEEE浮点数"></a>1) IEEE浮点数</h3><p>在IEEE标准中，浮点数是将特定长度的连续字节的所有二进制位分割为特定宽度的符号域、指数域和尾数域这三个域，域中的值分别用于表示给定二进制浮点数中的符号、指数和尾数，这样，通过尾数和可以调节的指数就可以表达给定的数值了。</p>
<p>IEEE754指定了两种基本的浮点格式：单精度和双精度。其中单精度格式具有24位有效数字(即尾数)精度，总共占用32位；双精度格式具有53位有效数字(即尾数)精度，总共占有64位       。</p>
<p>两种扩展浮点格式：单精度扩展和双精度扩展。此标准并未规定这些格式的精确精度和大小，但指定了最小精度和大小，例如IEEE双精度扩展格式必须至少具有64位有效数字精度，并总共占用至少79位。</p>
<p>具体的格式参见下面的图例：</p>
<p><img src="http://img.blog.csdn.net/20170930184239105?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h3 id="2-单精度格式"><a href="#2-单精度格式" class="headerlink" title="2) 单精度格式"></a>2) 单精度格式</h3><p>IEEE单精度格式由三个字段组成：23位小数f、8位偏置指数e以及1位符号s，这些字段连续存储在一个32位字中，如下图所示：</p>
<p><img src="http://img.blog.csdn.net/20170930184512641?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>0:22位包含23位小数f，其中第0位是小数的最低有效位，第22位是最高有效位。IEEE标准要求浮点数必须是规范的(浮点数的规范化见后文)，这意味着<strong>尾数的小数点左侧必须为1</strong>，因此我们在保存尾数时，可以省略小数点前面的1，从而腾出一个二进制位来保存更多的尾数，这样我们实际上用23位长的尾数域表达了24位的尾数。</p>
<p>23:30位包含8位偏置指数3，第23位是偏置指数的最低有效位，第30位是最高有效位。8位的指数可以表达0到255之间的256个指数值，但指数可以位正数，也可以为负数，因此为了处理负指数的情况，实际的指数值按要求需要加上一个偏置(Bias)值作为保存在指数域中的值，单精度的偏置值为127(2^7-1)，比如单精度的实际指数值0在指数域中保存为127(0+127)，实际指数值-63在指数域中保存为64(-63+127)。偏置的引入使得对于单精度数，实际可以表达的指数值的范围变为-127到128之间(包含两端)，其中指数值<strong>-127(保存为全0)以及+128(保存为全1)</strong>保留用作特殊值的处理，稍后介绍。如果我们分别用emin和emax来表达其它常规指数值范围的边界，即最小指数和最大指数分别用emin和emax来表示，即-126和127，则保留的特殊指数值可以分别表达为emin-1和emax+1;</p>
<p>最高的第31位包含符号位s，s为0表示数值为正数，s位1表示数值为负数。</p>
<p>值得注意的是，对于单精度数，由于我们只有24位的尾数(小数点左侧的1被隐藏)，所以可以表达的最大尾数为2^24-1=16,777,215，因此单精度的浮点数可以表达的十进制数值中，真正有效的数字不高于8位。</p>
<h3 id="3-双精度格式"><a href="#3-双精度格式" class="headerlink" title="3) 双精度格式"></a>3) 双精度格式</h3><p>IEEE双精度格式由三个字段组成：52位小数f、11位偏置指数e以及1位符号s，这些字段连续存储在两个32位字中，如下图所示：</p>
<p><img src="http://img.blog.csdn.net/20170930185100351?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>在SPARC体系结构中，较高地址的32位字包含小数的32位最低有效位，而在x86体系结构中，则是较低地址的32位字包含小数的32位最低有效位。</p>
<p>以x86体系结构为例，则f[31:0]表示小数的32位最低有效位，其中第0位是整个小数的最低有效位。在另一个32位字中，0:19位表示小数的20位最高有效位f[51:32]，其中第19位是整个小数的最高有效位；20:30位包含11位偏置指数e，其中第20位是偏置指数的最低有效位，第30位是偏置指数的最高有效位；第31位则是符号位s。上图将这两个连续的32位字按一个64位字那样进行了编号，其中：</p>
<p>0:51位包含52位小数f，其中第0位是小数的最低有效位，第51位是小数的最高有效位。IEEE标准要求浮点数必须是规范的，这意味着尾数的小数点左侧必须为1，因此我们在保存尾数时，可以省略小数点前面的1，从而腾出一个二进制位来保存更多的尾数，这样我们实际上用52位长的尾数域表达了53位的尾数。</p>
<p>52:62位包含11位偏置指数e，第52位是偏置指数的最低有效位，第 62 位是最高有效位。11 位的指数为可以表达 0 到 2047 之间的2048个指数值，但指数可以为正数，也可以为负数，因此为了处理负指数的情况，实际的指数值按要求需要加上一个偏差（Bias）值作为保存在指数域中的值，单精度数的偏差值为1023(2^10-1)，偏差的引入使得对于单精度数，实际可以表达的指数值的范围就变成 -1023到1024之间（包含两端）。最小指数和最大指数分别用emin 和 emax来表达，稍后将介绍实际的指数值 -1023(保存为全0)以及 +1024(保存为全 1)保留用作特殊值的处理。</p>
<p>最高的第 63 位包含符号位s，s为0表示数值为正数， s为1则表示负数。</p>
<h3 id="4-双精度扩展格式-SPARC"><a href="#4-双精度扩展格式-SPARC" class="headerlink" title="4) 双精度扩展格式(SPARC)"></a>4) 双精度扩展格式(SPARC)</h3><p>SPARC浮点环境的双精度格式符合IEEE关于双精度扩展格式的定义。</p>
<p>SPARC浮点环境的双精度格式包含三个字段：112位小数f、15位偏置指数e以及1位符号s，这三个字段连续存储，如下图所示：</p>
<p><img src="http://img.blog.csdn.net/20170930185523939?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>地址最高的32位字包含小数的32位最低有效位，用f[31:0]表示；紧邻的两个32位字分别包含f[63:32]和f[95:64]；接下来的32位字中， 0:15 位包含小数的16位最高有效位f[111:96]，其中第15位是整个小数的最高有效位；16:30位包含15位偏置指数e，其中第16位是该偏置指数的最低有效位，第30位是偏置指数的最高有效位；第 31位包含符号位s。</p>
<p>上图将这四个连续的32位字按一个128位字那样进行了编号，其中0:111位存储小数f；112:126位存储15位偏置指数e而第 127 位存储符号位 s。</p>
<h3 id="5-双精度扩展格式-x86"><a href="#5-双精度扩展格式-x86" class="headerlink" title="5) 双精度扩展格式(x86)"></a>5) 双精度扩展格式(x86)</h3><p>x86浮点环境的双精度格式符合IEEE关于双精度扩展格式的定义。</p>
<p>x86浮点环境的双精度格式包含四个字段：63位小数f、1位显式前导有效数位j、15位偏置指数e以及1位符号s。在 x86 体系结构系列中，这些字段连续存储在十个相连地址的8位字节中，由于UNIX System V Application Binary Interface Intel 386 Processor Supplement (Intel ABI) 要求双精度扩展参数，从而占用堆栈中三个相连地址的32位字，其中地址最高字的16位最高有效位未用，如下图所示：</p>
<p><img src="http://img.blog.csdn.net/20170930185554184?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>地址最低的32位字包含小数的32位最低有效位 f[31:0]，其中第0位是整个小数的最低有效位；地址居中的 32 位字中，0:30位包含小数的31位最高有效位 f[62:32]，其中第30位是整个小数的最高有效位，第31位包含显式前导有效数位 j。</p>
<p>地址最高的32位字中，0:14位包含15位偏置指数e，其中第0位是该偏置指数的最低有效位，而第14位是最高有效位；第15位包含符号位s。</p>
<h2 id="VI-浮点数的规范化"><a href="#VI-浮点数的规范化" class="headerlink" title="VI.浮点数的规范化"></a>VI.浮点数的规范化</h2><p>同样的数值可以有多种浮点数表达方式，比如上面例子中的123.45可以表达为12.345x10^1，0.12345x10^3或者1.2345x10^2，因为这种多样性，有必要对其加以规范化以达到统一表达的目标。规范的(Normalized)浮点数表达方式具有如下形式：</p>
<p>$$±d.dd…d ×β^e , (0≤d i&lt;β)$$</p>
<p>其中<strong>d.dd…d为尾数，β为基数，e为指数。</strong>尾数中数字的个数称为精度，用 p 来表示，每个数字d介于0和基数之间，包括0，小数点左侧的数字不为0。</p>
<p>基于规范表达的浮点数对应的具体值可由下面的表达式计算得到：</p>
<p>$$±(d_0 + d<em>1β^{-1} + … + d</em>(p-1)β^{-(p-1)})β^e , (0≤d i&lt;β)$$</p>
<p>对于十进制的浮点数，即基数β等于10的浮点数而言，上面的表达式非常容易理解，也很直白。而计算机内部的数值表达是基于二进制的，从上面的表达式，我们可以知道二进制数同样可以有小数点，也同样具有类似于十进制的表达方式，只是此时β等于2，而每个数字d只能在0和1之间取值，比如二进制数 1001.101相当于1x2^3+0x2^2+0x2^1+1x2^0+1x2^-1+0x2^-2+1x2^-3，对应于十进制的 9.625，其规范浮点数表达为1.001101x2^3。</p>
<h3 id="1-实数和浮点数之间的转换"><a href="#1-实数和浮点数之间的转换" class="headerlink" title="1) 实数和浮点数之间的转换"></a>1) 实数和浮点数之间的转换</h3><p>假定我们有一个32位的数据，它是一个单精度浮点数，十六进制表示为0xC0B40000，为了得到该浮点数实际表达的实数，我们首先将其转换为二进制形式：</p>
<p>1100 0000 1011 0100 0000 0000 0000 0000</p>
<p>接着按照浮点数的格式切分为相应的域：</p>
<p>1 1000_0001 0110_1000_0000_0000_0000_000</p>
<p>符号位1表示这是一个负数，指数域为129，意味着实际值为2，尾数域为01101意味着实际的二进制尾数为1.01101，所以实际的实数为：</p>
<p>-1.01101x2^2=-101.101=-5.625</p>
<p>从实数向浮点数变换稍微麻烦一点，假定我们需要将实数-9.625表达为单精度的浮点数格式，方式是首先将它用二进制浮点数表示，然后变换为相应的浮点数格式。</p>
<p>首先，整数部分，即9的二进制形式为1001，小数部分的算法则是将小数部分连续乘以基数2，并记录结果的整数部分：</p>
<p>0.625x2=1.25     1</p>
<p>0.25x2=0.5         0</p>
<p>0.5x2=1              1</p>
<p>当最后的小数部分为0时，结束该过程，因此小数部分的二进制表达为0.101，这样我们就得到了完整的二进制形式1001.101，用规范浮点数表示为：</p>
<p>1.001101x2^3</p>
<p>因为是负数，因此符号位为1，指数为3，因此指数域为3+127=130，即二进制的1000 0010，尾数域省掉小数点左侧的1，右侧用零补齐，得到最终结果为：<br>1 1000_00100011_0100_0000_0000_0000_000，最后可以将浮点数表示为十六进制的数据如下：1100 0001 0001 1010 0000 0000 0000 0000，最终结果为0xC11A0000  </p>
<p>这里需要注意一个问题，在上面我们有意选择的示例中，不断地将产生的小数部分乘以2的过程掩盖了一个事实，该过程结束的标志是小数部分乘以2的结果为1，但实际上，很多小数根本不能经过有限次这样的过程而得到结果(比如0.1)，但浮点数尾数域的位数是有限的，为此，浮点数的处理方法是持续该过程直到由此得到的尾数足以填满尾数域，之后对多余的位进行舍入。换句话说，除了我们之前讲到的精度问题之外，十进制到二进制的变换也并不能保证总是精确的，而只能是近似值。事实上，只有很少一部分十进制小数具有精确的二进制浮点数表达，再加上浮点数运算过程中的误差累积，结果是很多我们看来非常简单的十进制运算在计算机上却往往出人意料，这就是最常见的浮点运算的”不准确”问题，比如：34.6f-34.0f=0.599998，产生这个误差的原因是34.6f无法精确的表达为相应的浮点数，而只能保存为经过舍入的近似值，这个近似值与34.0f之间的运算自然无法产生精确的结果(具体过程后面会讲)。</p>
<h3 id="2-舍入"><a href="#2-舍入" class="headerlink" title="2) 舍入"></a>2) 舍入</h3><p>根据标准要求，无法精确保存的值必须向最接近可保存的值进行舍入，这有点像我们熟悉的十进制的四舍五入，即不足一半则舍，一半以上(包括一半)则进，不过对于二进制浮点数而言，则是0就舍，但1不一定进，而是在前后两个等距接近的可保存的值中，取其中最后一位有效数字为零的值进行保存，即采取向偶数舍入，比如0.5要舍到0，1.5要入到2(即先试着进1，会得到最后结果，如果这个结果的尾数的最后位为0，则进位成功；否则进位失败，直接舍去)，看下面几个例子：</p>
<p><img src="http://img.blog.csdn.net/20170930190606088?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>结果推导分析：</p>
<p>16777215f的分析过程：</p>
<p>1111 1111 1111 1111 1111 1111                         16777215f(其中第一个1会隐藏)</p>
<p>1.1111_1111 1111_1111_1111_111             　　  刚好是23位小数，不会丢失精度，能精确表示</p>
<p>0 23+127 1111_1111_1111_1111_1111_111 　　(0表示是正数，23+127表示偏移指数值，尾数位为23个1)</p>
<p>0 10010110 1111_1111_1111_1111_1111_111</p>
<p>16777216f的分析过程：</p>
<p>1000 0000 0000 0000 0000 0000 0                    16777216f(最后一个0会被舍去)</p>
<p>1.0000_0000_0000_0000_0000_000 0              (因为尾数域为23位，因此最后一个0被舍去，但是还是能准确显示)</p>
<p>0 24+127 0000_0000_0000_0000_0000_000</p>
<p>0 10010111 0000_0000_0000_0000_0000_000</p>
<p>16777217f的分析过程：</p>
<p>1000 0000 0000 0000 0000 0000 1                    16777217f(最后一个1会被舍去)</p>
<p>1.0000_0000_0000_0000_0000_000 1              (最后一个1被舍去，试着进位)</p>
<ol>
<li><p>0000_0000_0000_0000_0000_001                (由于进位后，尾数域末尾不为0，因此进位失败，直接将1舍去)</p>
</li>
<li><p>0000_0000_0000_0000_0000_000                (这里其实已经说明结果为16777216f)</p>
</li>
</ol>
<p>16777218f的分析过程：</p>
<p>1000 0000 0000 0000 0000 0001 0                    16777218f(最后一个0会被舍去)</p>
<ol>
<li>0000_0000_0000_0000_0000_001 0             (最后一个0被舍去，所以还是能准确显示)</li>
</ol>
<p>0 24+127 0000_0000_0000_0000_0000_001</p>
<p>0 10010111 0000_0000_0000_0000_0000_001</p>
<p>16777219f的分析过程：</p>
<p>1000 0000 0000 0000 0000 0001 1                    16777219f(最后一个1会被舍去)</p>
<ol>
<li><p>0000_0000_0000_0000_0000_001 1             (最后一个1被舍去，尝试进位)</p>
</li>
<li><p>0000_0000_0000_0000_0000_010                (尾数域末位为0，进位成功)</p>
</li>
</ol>
<p>0 24+127 0000_0000_0000_0000_0000_010</p>
<p>010010111 0000_0000_0000_0000_0000_010</p>
<p>… …</p>
<h3 id="3-浮点数的加减运算"><a href="#3-浮点数的加减运算" class="headerlink" title="3) 浮点数的加减运算"></a>3) 浮点数的加减运算</h3><p>浮点数的加减运算一般由以下五个步骤完成：对阶、尾数运算、结果规格化、舍入处理、溢出判断。</p>
<p>tip1.对阶</p>
<p>对阶的目的是使两操作数的小数点位置对齐，即使两数的阶码相等。为此，首先要求出阶差，再按小阶向大阶看齐的原则，使阶小的尾数向右移位，每右移一位，阶码加1，直到两数的阶码相等为止。</p>
<p>tip2.尾数运算</p>
<p>尾数运算就是将对阶后的尾数按定点加减运算规则进行运算。</p>
<p>tip3.结果规格化</p>
<p>在机器中，为保证浮点数表示的唯一性，浮点数在机器中都是以规格化形式存储的。对于IEEE754标准的浮点数来说，就是尾数必须是1.xxxx的形式。由于在进行上述两个定点小数的尾数相加减运算后，尾数有可能是非规格化形式，为此必须进行规格化操作，规格化操作包括左规和右规两种情况。</p>
<p>左规：将尾数左移一位，同时阶码减1，直至尾数成为1.xxxx的形式；</p>
<p>右规：将尾数右移一位，同时阶码增1，便成为规格化的形式了。</p>
<p>注：右规操作只需将尾数右移一位即可，这种情况出现在尾数的最高位（即小数点前一位）运算时出现了进位，使尾数成为10.xxxx或11.xxxx的形式。</p>
<p>tip4.舍入处理</p>
<p>在对阶和右规过程中，可能会将尾数的低位丢失，引起误差，影响精度，为此可用舍入法来提高尾数的精度。IEEE754标准列出了四种可选的舍入处理方法：  </p>
<p>就近舍入（round to neareset）：这是标准列出的默认舍入方式，前面有讲。</p>
<p>朝+∞舍入（round toward +∞）：对正数来说，只要多余位不为全0，则向尾数最低有效位进1；对负数来说，则是简单地舍去。  </p>
<p>朝-∞舍入（round toward -∞）：与朝+∞舍入方法正好相反，对正数来说，只是简单地舍去；对负数来说，只要多余位不为全0，则向尾数最低有效位进1。  </p>
<p>朝0舍入（round toward 0）： 即简单地截断舍去，而不管多余位是什么值。这种方法实现简单，但容易形成累积误差，且舍入处理后的值总是向下偏差。</p>
<p>tip5.溢出判断</p>
<p>与定点数运算不同的是，浮点数的溢出是以其运算结果的阶码的值是否产生溢出来判断的。若阶码的值超过了阶码所能表示的最大正数，则为上溢，进一步，若此时浮点数为正数，则为正上溢，记为+∞，若浮点数为负数，则为负上溢，记为-∞；若阶码的值超过了阶码所能表示的最小负数，则为下溢，进一步，若此时浮点数为正数，则为正下溢，若浮点数为负数，则为负下溢。正下溢和负下溢都作为机器零处理，即将尾数各位强置为零。  </p>
<p>/<em>浮点数加减运算-例1</em>/</p>
<p>设两浮点数的IEEE754标准存储格式分别为  </p>
<p>x＝0 10000010 01101100000000000000000，y＝0 10000100 01011101100000000000000，求x+y，并给出结果的IEEE754标准存储格式。</p>
<p>解：</p>
<p>对于浮点数x,</p>
<p>符号位s=0</p>
<p>指数e=E-127= 10000010-01111111＝00000011＝(3)10  </p>
<p>尾数m＝1.01101100000000000000000</p>
<p>于是有 ｘ＝+1.01101100000000000000000×2^3   </p>
<p>对于浮点数y：  </p>
<p>符号位s＝0  </p>
<p>指数e＝E-127＝10000100-01111111＝00000011＝(5)10 </p>
<p>尾数m＝1.01011101100000000000000</p>
<p>于是有  </p>
<p>y＝+1.01011101100000000000000×2^5 </p>
<p>(1)对阶</p>
<p>⊿E＝Ex-Ey＝3-5＝-2 ，因此x＝1.01101100000000000000000×2^3＝</p>
<p>0.01011011000000000000000×2^5 </p>
<p> (2)尾数相加  </p>
<p>x+y＝00.01011011000000000000000×25 +01.01011101100000000000000×2^5  </p>
<pre><code>＝01.10111000100000000000000×25（其中最高位是符号位）
</code></pre><p> 结果的IEEE754标准存储格式为：0 10000100 10111000100000000000000 </p>
<p>/<em>浮点数加减运算-例2</em>/</p>
<p>求34.6f-34.0f</p>
<p>解：</p>
<p>对于浮点数34.6f：</p>
<p>符号位s=0</p>
<p>指数e=(5)10 </p>
<p>尾数m=1. 000101001100110011001100…，根据就近舍入，得到：</p>
<p>m’=1. 00010100110011001100110</p>
<p>对于浮点数34.0f：</p>
<p>符号位s=0</p>
<p>指数e=(5)10 </p>
<p>尾数m=1.00010000000000000000000</p>
<p>(1)对阶</p>
<p>阶码相同</p>
<p>(2)尾数相加  </p>
<p>34.6f-34.0f = 01. 00010100110011001100110×25 -11.00010000000000000000000×25</p>
<p> = 01. 00010100110011001100110×25 + 10.11110000000000000000000×25</p>
<p>= 100.00000100110011001100110×25</p>
<p>= 00.00000100110011001100110×25（符号位的进位1被舍去）</p>
<p>(3)左规  </p>
<p>34.6f-34.0f = 1.00110011001100110000000×2-1 = (0.59999847)10</p>
<p>结果的IEEE754标准存储格式为：0 01111110 00110011001100110000000</p>
<p>/<em>浮点数加减运算-例3</em>/</p>
<p>求1.5f-2.4f</p>
<p>解：</p>
<p>对于浮点数1.5f：</p>
<p>符号位s=0</p>
<p>指数e=(0)10 </p>
<p>尾数m=1.10000000000000000000000</p>
<p>对于浮点数2.4f：</p>
<p>符号位s=0</p>
<p>指数e=(1)10 </p>
<p>尾数m=1.00110011001100110011010</p>
<p>(1)对阶</p>
<p>⊿E＝0-1＝-1 ，因此1.5f＝0.110000000000000000000000×21</p>
<p>(2)尾数相加  </p>
<p>1.5f-2.4f = 00.110000000000000000000000×21 - 11.00110011001100110011010×21       </p>
<p>= 00.110000000000000000000000×21 + 10.11001100110011001100110×21  </p>
<p>= 11.10001100110011001100110×21（其中最高位是符号位）</p>
<p>= 10.01110011001100110011010×21（将补码转换成原码）</p>
<p>(3)左规  </p>
<p>1.5f-2.4f = -1.11001100110011001101000×2-1 = (-0.90000010)10</p>
<p>结果的IEEE754标准存储格式为：1 01111110 11001100110011001101000</p>
<h3 id="4-特殊值-NaN"><a href="#4-特殊值-NaN" class="headerlink" title="4) 特殊值-NaN"></a>4) 特殊值-NaN</h3><p>当指数为128(指数域全1)，且尾数域不等于0时，该浮点数即为NaN。</p>
<p>IEEE标准没有要求具体的尾数域，所以NaN实际上不是一个，而是一族。</p>
<p>比较操作符&lt;、&lt;=、&gt;、&gt;=在任一操作数为NaN时均返回false，等于操作符==在任一操作数为NaN时均返回false，即使是两个具有相同位模式的NaN也一样，而操作符！=则当任一操作数为NaN时均返回true，这个规则的一个有趣的结果是x!=x，当x为NaN时竟然为真。</p>
<p>用特殊的NaN来表达上述运算错误的意义在于避免了因这些错误而导致运算的不必要的终止。比如，如果一个被循环调用的浮点运算方法，可能由于输入的参数问题而导致发生这些错误，NaN使得即使某次循环发生了这样的错误，也可以简单地继续执行循环以进行那些没有错误的运算。你可能想到，既然Java有异常处理机制，也许可以通过捕获并忽略异常达到相同的效果。但是，要知道，IEEE标准不是仅仅为Java而制定的，各种语言处理异常的机制不尽相同，这将使得代码的迁移变得更加困难。何况，不是所有语言都有类似的异常或者信号（Signal）处理机制。</p>
<h3 id="5-特殊值-无穷"><a href="#5-特殊值-无穷" class="headerlink" title="5) 特殊值-无穷"></a>5) 特殊值-无穷</h3><p>当指数为128(指数域全1)，且尾数域等于0时，该浮点数即为无穷大，用符号位来确定是正无穷大还是负无穷大。</p>
<p>无穷用于表达计算中产生的上溢（Overflow）问题。比如两个极大的数相乘时，尽管两个操作数本身可以用保存为浮点数，但其结果可能大到无法保存为浮点数，而必须进行舍入。根据IEEE标准，此时不是将结果舍入为可以保存的最大的浮点数（因为这个数可能离实际的结果相差太远而毫无意义），而是将其舍入为无穷。对于负数结果也是如此，只不过此时舍入为负无穷，也就是说符号域为1的无穷。有了NaN的经验我们不难理解，特殊值无穷使得计算中发生的上溢错误不必以终止运算为结果。</p>
<p>无穷和除NaN以外的其它浮点数一样是有序的，从小到大依次为负无穷，负的有穷非零值，正负零（随后介绍），正的有穷非零值以及正无穷。除NaN以外的任何非零值除以零，结果都将是无穷，而符号则由作为除数的零的符号决定。</p>
<p>回顾我们对NaN的介绍，当零除以零时得到的结果不是无穷而是NaN。原因不难理解，当除数和被除数都逼近于零时，其商可能为任何值，所以IEEE标准决定此时用NaN作为商比较合适。</p>
<h3 id="6-特殊值-有符号的零"><a href="#6-特殊值-有符号的零" class="headerlink" title="6) 特殊值-有符号的零"></a>6) 特殊值-有符号的零</h3><p>因为IEEE标准的浮点数格式中，小数点左侧的1是隐藏的，而零显然需要尾数必须是零。所以，零也就无法直接用这种格式表达而只能特殊处理。</p>
<p>当指数为-127(指数域全0)，且尾数域等与0时，该浮点数即为零，考虑到符号域的作用，所以存在着两个零，即+0和-0。不同于正负无穷之间是有序的，IEEE标准规定正负零是相等的。</p>
<p>零有正负之分，的确非常容易让人困惑，这一点是基于数值分析的多种考虑，经利弊权衡后形成的结果。有符号的零可以避免运算中，特别是涉及无穷的运算中，符号信息的丢失。举例而言，如果零无符号，则等式1/(1/x)=x当x=±∞时不再成立。原因是如果零无符号，1和正负无穷的比值为同一个零，然后1与0的比值为正无穷，符号没有了。解决这个问题，除非无穷也没有符号，但是无穷的符号表达了上溢发生在数轴的哪一侧，这个信息显然是不能不要的，因此零有符号。</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> <span class="keyword">class</span> test</div><div class="line"></div><div class="line">&#123;</div><div class="line"></div><div class="line">      <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> main(String[] args)</div><div class="line"></div><div class="line">      &#123;</div><div class="line"></div><div class="line">             <span class="comment">//指数域全0，尾数域为0时，浮点数为有符号的零</span></div><div class="line"></div><div class="line">             System.out.<span class="keyword">println</span>(<span class="keyword">Float</span>.intBitsToFloat(<span class="number">0</span>x0));</div><div class="line"></div><div class="line">             <span class="comment">//指数域全1，尾数域为0时，浮点数为无穷，符号位决定是正无穷还是负无穷</span></div><div class="line"></div><div class="line">             System.out.<span class="keyword">println</span>(<span class="keyword">Float</span>.intBitsToFloat(<span class="number">0</span>x7F800000));</div><div class="line"></div><div class="line">             System.out.<span class="keyword">println</span>(<span class="keyword">Float</span>.intBitsToFloat(<span class="number">0</span>xFF800000));</div><div class="line"></div><div class="line">             <span class="comment">//指数域全1，尾数域不为0时，浮点数为NaN</span></div><div class="line"></div><div class="line">             System.out.<span class="keyword">println</span>(<span class="keyword">Float</span>.intBitsToFloat(<span class="number">0</span>x7FC00000));</div><div class="line"></div><div class="line">             System.out.<span class="keyword">println</span>(<span class="keyword">Float</span>.intBitsToFloat(<span class="number">0</span>x7F800001));</div><div class="line"></div><div class="line">             System.out.<span class="keyword">println</span>(<span class="keyword">Float</span>.intBitsToFloat(<span class="number">0</span>xFF800001));</div><div class="line"></div><div class="line">      &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20170930190812597?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>解释一下几个方法：</p>
<p>Float.floatToIntBits(float f)：按照IEEE754标准，返回指定浮点数的表达形式。举个例子：Float.floatToIntBits(20.5f)按照如下方式计算：20.5D=10100.1B=1.01001x2^4，因此可将浮点数20.5f表示为十六进制数据：0100 0001 1010 0100 0000 0000 0000 0000，转换成10进制即1101266944</p>
<p>Float.intBitsToFloat(int i)：按照IEEE754标准，返回指定整数的表达形式。举个例子：Float.intBitsToFloat(1101266944)即为20.5f</p>
<p>Integer.toBinaryString(int i)：将指定整数表示成相应的二进制数，举个例子：</p>
<p>Ineger.toBinaryString(128)即为10000000</p>
<p>注：javac编译编码GBK的不可映射字符时，可以通过-encoding来指定编码方式，如：</p>
<p>javac -encoding utf-8 test.java</p>
<h3 id="7-非规范化数"><a href="#7-非规范化数" class="headerlink" title="7) 非规范化数"></a>7) 非规范化数</h3><p>这个数的定义和有符号0一样，不过尾数不能为0，用于小出范围的数。</p>
<p>我们来考察浮点数的一个特殊情况。选择两个绝对值极小的浮点数，以单精度的二进制浮点数为例，比如1.001×2^-125和1.0001×2^-125这两个数（分别对应于十进制的2.6448623×10^-38和2.4979255×10^-38）。显然，他们都是普通的浮点数（指数为-125，大于允许的最小值-126；尾数更没问题），按照IEEE754可以分别保存为0 00000010 00100000000000000000000（0x1100000）和0 00000010 00010000000000000000000（0x1080000）。</p>
<p>现在我们看看这两个浮点数的差值。</p>
<p>不难得出，该差值为0.0001×2^-125，表达为规范浮点数则为1.0×2^-129。问题在于其指数小于允许的最小指数值，所以无法保存为规范浮点数，最终只能近似为零（FlushtoZero）。这种特殊情况意味着下面本来十分可靠的代码也可能出现问题：</p>
<p>if(x!=y)</p>
<p>{</p>
<p>　　z=1/(x-y);</p>
<p>}</p>
<p>正如我们精心选择的两个浮点数展现的问题一样，即使x不等于y，x和y的差值仍然可能绝对值过小，而近似为零，导致除以0的情况发生。</p>
<p>为了解决此类问题，IEEE标准中引入了非规范（Denormalized）浮点数。规定当浮点数的指数为允许的最小指数值，即emin时，尾数不必是规范化的。比如上面例子中的差值可以表达为非规范的浮点数0.001×2^-126，其中指数-126等于emin。为了保存非规范浮点数，IEEE标准采用了类似处理特殊值零时所采用的办法，即用特殊的指数域值emin-1加以标记，当然，此时的尾数域不能为零。这样，例子中的差值可以保存为00000000000100000000000000000000（0x100000），没有隐含的尾数位。</p>
<p>有了非规范浮点数，去掉了隐含的尾数位的制约，可以保存绝对值更小的浮点数。而且，也由于不再受到隐含尾数域的制约，上述关于极小差值的问题也不存在了，因为所有可以保存的浮点数之间的差值同样可以保存。</p>
<p>注意，规定的是”不必”，这也就意味着”可以”，因此当浮点数实际的指数为emin，该浮点数仍是规范的，也就是说，保存时隐含着一个隐藏的尾数位。</p>
]]></content>
      
        <categories>
            
            <category> Algorithm Optimization </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Algorithm Optimization </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[平方根运算的软件与硬件的加速计算]]></title>
      <url>/2017/07/20/%E5%B9%B3%E6%96%B9%E6%A0%B9%E8%BF%90%E7%AE%97%E7%9A%84%E8%BD%AF%E4%BB%B6%E4%B8%8E%E7%A1%AC%E4%BB%B6%E7%9A%84%E5%8A%A0%E9%80%9F%E8%AE%A1%E7%AE%97/</url>
      <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>


<h2 id="1-平方根运算软件算法"><a href="#1-平方根运算软件算法" class="headerlink" title="1. 平方根运算软件算法"></a>1. 平方根运算软件算法</h2><h3 id="1-1-二分法"><a href="#1-1-二分法" class="headerlink" title="1.1 二分法"></a>1.1 二分法</h3><p>利用二分进行开平方的思想很简单：假定中值为最终解。假定下限为0，上限为x，然后求中值；然后比较中值的平方和x的大小，并根据大小修改下限或者上限；重新计算中值，开始新的循环，直到前后两次中值的距离小于给定的精度为止。**需要注意的一点是，如果x小于1，我们需要将上限置为1。</p>
<p>代码如下：</p>
<figure class="highlight excel"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">float SqrtByBisection(float <span class="built_in">n</span>)</div><div class="line">&#123;</div><div class="line">	float low,up,<span class="built_in">mid</span>,last; </div><div class="line">	low=<span class="number">0</span>,up=(<span class="built_in">n</span>&lt;<span class="number">1</span>?<span class="symbol">1:n</span>); </div><div class="line">	<span class="built_in">mid</span>=(low+up)/<span class="number">2</span>; </div><div class="line">	do</div><div class="line">	&#123;</div><div class="line">		<span class="built_in">if</span>(<span class="built_in">mid</span>*<span class="built_in">mid</span>&gt;<span class="built_in">n</span>)</div><div class="line">			up=<span class="built_in">mid</span>; </div><div class="line">		else </div><div class="line">			low=<span class="built_in">mid</span>;</div><div class="line">		last=<span class="built_in">mid</span>;</div><div class="line">		<span class="built_in">mid</span>=(up+low)/<span class="number">2</span>; </div><div class="line">	&#125;while(fabsf(<span class="built_in">mid</span>-last) &gt; eps);  //eps为精度，一般为<span class="number">0.000001</span></div><div class="line"></div><div class="line">	return <span class="built_in">mid</span>; </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里有一点需要特别注意：<strong>在精度判别时不能利用上下限而要利用前后两次mid值，否则可能会陷入死循环</strong>！这是因为由于精度问题，在循环过程中可能会产生mid值和up或low中的一个相同。这种情况下，后面的计算都不会再改变mid值，因而在达不到精度内时就陷入死循环。</p>
<h3 id="1-2-牛顿迭代法"><a href="#1-2-牛顿迭代法" class="headerlink" title="1.2  牛顿迭代法"></a>1.2  牛顿迭代法</h3><p>将中值替换为切线方程的零根作为最终解，原理可以利用下图解释：</p>
<p><img src="http://img.blog.csdn.net/20170928161138122?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>具体代码：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">float</span> <span class="title">SqrtByNewton</span><span class="params">(<span class="keyword">float</span> x)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">	<span class="keyword">int</span> temp = (((*(<span class="keyword">int</span> *)&amp;x)&amp;<span class="number">0xff7fffff</span>)&gt;&gt;<span class="number">1</span>)+(<span class="number">64</span>&lt;&lt;<span class="number">23</span>);</div><div class="line">	<span class="keyword">float</span> val=*(<span class="keyword">float</span>*)&amp;temp;       <span class="comment">//初始值</span></div><div class="line">	<span class="keyword">float</span> last;</div><div class="line">	<span class="keyword">do</span></div><div class="line">	&#123;</div><div class="line">		last = val;</div><div class="line">		val =(val + x/val) / <span class="number">2</span>;     <span class="comment">//利用一阶梯度更新结果</span></div><div class="line">	&#125;<span class="keyword">while</span>(fabsf(val-last) &gt; eps);  <span class="comment">//eps为精度，一般为0.000001</span></div><div class="line">	<span class="keyword">return</span> val;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>注意到代码中初始值的选择，解释如下：<br>IEEE浮点标准用的形式来表示一个数，将该数存入float类型之后变为：</p>
<p><div align="center"><img src="http://img.blog.csdn.net/20170928161138122?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%"><div align="left"></div></div></p>
<p><div align="center"><img src="http://img.blog.csdn.net/20171114212446698?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/20/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%"><div align="left"></div></div></p>
<p>现在需要对这个浮点数进行开方，我们看看各部分都会大致发生什么变化。指数E肯定会除以2,127保持不变，m需要进行开方。由于指数部分是浮点数的大头，所以对指数的修改最容易使初始值接近精确值。幸运的是，对指数的开平方我们只需要除以2即可，也即右移一位。但是由于E+127可能是奇数，右移一位会修改指数，我们将先将指数的最低位清零，这就是&amp; 0xff7fffff的目的。然后将该转换后的整数右移一位，也即将指数除以2，同时尾数也除以2（其实只是尾数的小数部分除以2）。由于右移也会将127除以2，所以我们还需要补偿一个64，这就是最后还需要加一个(64&lt;&lt;23)的原因。</p>
<p>这里大家可能会有疑问，最后为什么加(64&lt;&lt;23)而不是(63&lt;&lt;23)，还有能不能不将指数最后一位清零？答案是都可以，但是速度都没有我上面写的快。这说明我上面的估计更接近精确值。下面简单分析一下原因。首先假设e为偶数，不妨设e=2n，开方之后e则应该变为n,127保持不变，我们看看上述代码会变为啥。e+127是奇数，会清零，这等价于e+126，右移一位变为n+63，加上补偿的64，指数为n+127，正是所需！再假设e为奇数，不妨设e=2n+1，开方之后e应该变为n+1（不精确），127保持不变，我们看看上述代码会变为啥。e+127是偶数等于2n+128，右移一位变为n+64，加上补偿的64，指数为n+1+127，也是所需！这确实说明上述的估计比其他方法更精确一些，因而速度也更快一些。</p>
<h3 id="1-3-卡马克算法—快速平方根倒数算法"><a href="#1-3-卡马克算法—快速平方根倒数算法" class="headerlink" title="1.3 卡马克算法—快速平方根倒数算法"></a>1.3 卡马克算法—快速平方根倒数算法</h3><p>此法实质上是进行了1~2次的牛顿迭代法求开方倒数，求平方根的倒数，实际就是求方程$$ 1/(x^2)-a=0 $$的解。将该方程按牛顿迭代法的公式展开为：$$ x_{(n+1)}=1/2<em>x_n</em>(3-a<em>x_n</em>x_n) $$。这个方法的一个关键地方还在于起始值的选取，算法选取一个“magic number”，使算法的初始解非常接近精确解！</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">float</span> <span class="title">SqrtByCarmack</span><span class="params">( <span class="keyword">float</span> number )</span>  </span></div><div class="line"><span class="function"></span>&#123;  </div><div class="line">    <span class="keyword">int</span> i;  </div><div class="line">    <span class="keyword">float</span> x2, y;  </div><div class="line">    <span class="keyword">const</span> <span class="keyword">float</span> threehalfs = <span class="number">1.5F</span>;  </div><div class="line">  </div><div class="line">    x2 = number * <span class="number">0.5F</span>;  </div><div class="line">    y  = number;  </div><div class="line">    i  = * ( <span class="keyword">long</span> * ) &amp;y;       </div><div class="line">    i  = <span class="number">0x5f375a86</span> - ( i &gt;&gt; <span class="number">1</span> );   <span class="comment">//得到初始化值</span></div><div class="line">    y  = * ( <span class="keyword">float</span> * ) &amp;i;  </div><div class="line">    <span class="comment">//第一次迭代，可以根据精度要求重复此过程，一般一次迭代精度就够用了</span></div><div class="line">    y  = y * ( threehalfs - ( x2 * y * y ) );   </div><div class="line">    <span class="keyword">return</span> number*y;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="2-整数平方根———硬件加速算法"><a href="#2-整数平方根———硬件加速算法" class="headerlink" title="2. 整数平方根———硬件加速算法"></a>2. 整数平方根———硬件加速算法</h2><h3 id="2-1-手工开平方算法法"><a href="#2-1-手工开平方算法法" class="headerlink" title="2.1 手工开平方算法法"></a>2.1 手工开平方算法法</h3><p>先以10进制为例，解释手动开平方算法：</p>
<p>首先将数字每两位分成一段。如：745836942。就分成：<br>7|45|83|69|42，即从个位开始分。共分成五段，第一段是7。对第一段的数字7开方取整，可得a=2。此时，要在2后面接一个数字b，并在7后面加上下一段的数45，使产生的两位数ab的平方不大于745。<br>我们知道，数ab实际值表示为<strong>10a+b</strong>，其平方是$$100a^2+20ab+b^2$$。我们可以暂时忽略$$b^2$$，而产生一个“试商”b。即$$b = (745 - 100a^2) / 20a = (745 - 100<em>2</em>2) / (20 * 2) = 8.625 ≈ 8$$(向下取整)。但是，我们发现$$28^2 = 784&gt;745$$，于是这个试商需要减少为7。然而，当a=0时，上述求试商的方法不在适用，但我们可以直接取下一段的两位数开方。如√45≈6。求出试商后，用$$745-ab^2$$得到新的“第一段”的数。</p>
<p><strong>具体过程：</strong></p>
<p>取出第一段的数mn，开方得到a，然后接上第二段的数pq，用$$mnpq-100a^2$$得到“余数”x，$$x/20a$$得到试商b，然后调整b（当$$20<em>a</em>b+b<em>b&gt;x$$时b需要减少），调整后，将$$x-20a</em>b-b*b$$作为新的余数x’，新的a就是手动开平方的简易方法。由于前面已经将$$100a^2$$减掉，所以后面每次都不用再减去$$100a^2$$。重复步骤，直到开方完毕，或达到要求的精度为止。最后得到的a就是平方根。</p>
<p>如求$$745836942$$的平方根：<br> $$7|45|83|69|42$$<br>$$①a=sqrt(7)≈2，b=(745-400)/40≈8，28^2=784&gt;745  ==&gt;  b=7  ==&gt; 27^2=729&lt;745，745-729 = 16$$</p>
<p>$$②a=27，b=1683/540≈3，27<em>20</em>3+3*3=1629&lt;1683，1683-1629 = 54$$</p>
<p>$$③a=273，b=5469/5460≈1，273<em>20</em>1+1*1=5461<5469 =="">  b=1， 5469-5461=8$$</5469></p>
<p>当运用到二进制时，数ab实际值表示为<strong>a&lt;&lt;1+b</strong>，其平方是$$a^2&lt;&lt;2+ab&lt;&lt;2+b^2$$。我们可以暂时忽略$$b^2$$，而产生一个“试商”b，依此类推下去。以下代码为求一个32位数的平方根，算法将原始值两两分组进行计算，注意：根应至多为16位且每次计算的b值只能是0或1：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">unsigned int sqrt_16_1(unsigned int M) </div><div class="line">&#123;</div><div class="line">	unsigned int N,N2, i,j; </div><div class="line">	unsigned int tmp; <span class="comment">// 每轮计算的残差 </span></div><div class="line">	if (M == <span class="number">0</span>)      <span class="comment">// 被开方数为0，开方结果也为0 </span></div><div class="line">		return <span class="number">0</span>;</div><div class="line">	N = <span class="number">0</span>;</div><div class="line">	N2 = <span class="number">0</span>;</div><div class="line">	tmp = <span class="number">0</span>;</div><div class="line">	for (i=<span class="number">16</span>; i&gt;<span class="number">0</span>; i-=<span class="number">1</span>) <span class="comment">//结果为16位 </span></div><div class="line">	&#123; </div><div class="line">		N &lt;&lt;= <span class="number">1</span>;        <span class="comment">// 左移一位 </span></div><div class="line">		N2 = N &lt;&lt; <span class="number">1</span>;   <span class="comment">//N2代表a&lt;&lt;2</span></div><div class="line">		tmp &lt;&lt;= <span class="number">2</span>;   <span class="comment">// tmp储存的是每次计算完剩下的残差</span></div><div class="line">		tmp += ((M&amp;<span class="number">0xc0000000</span>) &gt;&gt; <span class="number">30</span>);   <span class="comment">//再加上此次计算的2bit的值</span></div><div class="line">		M &lt;&lt;= <span class="number">2</span>; </div><div class="line">		if( tmp &gt;= N2 + <span class="number">1</span> )              <span class="comment">//试值ab&lt;&lt;2+bi^2与残差的比较，此时设bi=1</span></div><div class="line">		&#123;</div><div class="line">			tmp -= N2 + <span class="number">1</span>;               <span class="comment">//计算此轮的残差</span></div><div class="line">			N++;                         <span class="comment">//确定bi=1</span></div><div class="line">		&#125;</div><div class="line">	&#125; </div><div class="line">	return N; </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>将原始值分类大小由2变成4，那么每次要计算2位二进制数值，可能情况有00,01,10,11，00情况时由于不更新结果，直接移动两位即可，所以可不进行处理，内部循环因此为1~3！<br>也可把分类大小变成8,16…只要是2的倍数均可，算法流程类似！</p>
<p><strong>具体代码如下：</strong></p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">unsigned int sqrt_16_2(unsigned int M) </div><div class="line">&#123;</div><div class="line">	unsigned int N,N2, i,j; </div><div class="line">	if (M == <span class="number">0</span>)                 <span class="comment">// 被开方数，开方结果也为0 </span></div><div class="line">		return <span class="number">0</span>;</div><div class="line">	N = <span class="number">0</span>;</div><div class="line">	N2 = <span class="number">0</span>;</div><div class="line">	tmp = <span class="number">0</span>;</div><div class="line">	for (i=<span class="number">16</span>; i&gt;<span class="number">0</span>; i-=<span class="number">2</span>)        <span class="comment">// 求剩余的15位，每次2位 </span></div><div class="line">	&#123; </div><div class="line">		N &lt;&lt;= <span class="number">2</span>;                 <span class="comment">// 左移两位 </span></div><div class="line">		N2 = N &lt;&lt; <span class="number">1</span>;             <span class="comment">// N2代表2a</span></div><div class="line">		tmp &lt;&lt;= <span class="number">4</span>;               <span class="comment">// tmp储存的是每次查表完剩下的残差</span></div><div class="line">		tmp += (M &gt;&gt; <span class="number">28</span>);        <span class="comment">// 再加上下一个4bit的值</span></div><div class="line"></div><div class="line">		for (j=<span class="number">1</span>;j&lt;<span class="number">4</span>;j++)</div><div class="line">		 &#123; </div><div class="line">			 ttp[j] = N2*j + j*j; <span class="comment">//求2ab+bi^2，bi可为01,10,11</span></div><div class="line">		&#125;</div><div class="line">		M &lt;&lt;= <span class="number">4</span>;</div><div class="line">		for (j=<span class="number">3</span>;j&gt;<span class="number">0</span>;j--)</div><div class="line">		&#123;</div><div class="line">			if (tmp &gt;= ttp[j])     <span class="comment">//试值ab&lt;&lt;2+bi^2与残差的比较，bi可为01,10,11</span></div><div class="line">			&#123;</div><div class="line">				tmp -= ttp[j]; </div><div class="line">				N+=j;</div><div class="line">				break;            <span class="comment">//bi只可取一个值</span></div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125; </div><div class="line">	return N; </div><div class="line"> &#125;<span class="comment">//32-&gt;16</span></div></pre></td></tr></table></figure>]]></content>
      
        <categories>
            
            <category> Algorithm Optimization </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Algorithm Optimization </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[SVM--支持向量机简述]]></title>
      <url>/2017/07/15/SVM--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%AE%80%E8%BF%B0/</url>
      <content type="html"><![CDATA[<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<p>此文是根据July大神所写的 <a href="http://download.csdn.net/download/u012554092/9979469" target="_blank" rel="external"> 支持向量机通俗导论（理解SVM的三层境界）
</a> 一文的读后感，记录下自己的一点感悟与体会。</p>
<p><strong> SVM </strong> 它本质上即是一个二分类方法，用$$wT+b$$定义分类函数，于是求w、b，为寻最大间隔，引出$$1/2∥w∥^2$$  ，继而引入拉格朗日因子，化为对拉格朗日乘子α的求解（求解过程中会涉及到一系列最优化或凸二次规划等问题），如此，求w、b 与求α 等价，而α的求解可以用一种快速学习算法SMO，至于核函数，是为处理非线性情况，若直接映射到高维计算恐维度爆炸，故在低维计算，等效高维表现。 </p>
<h1 id="1-算法步骤："><a href="#1-算法步骤：" class="headerlink" title="1.算法步骤："></a>1.算法步骤：</h1><p>1.1 选取合适的核函数  $$K（x_i,x_j）$$  。<br>1.2 将训练数据 xi带入分类函数中，利用SMO求解系数αi。  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170914221119079?watermark/2/text/aHR0cDo
vL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFC
MA==/dissolve/70/gravity/SouthEast" width="70%"></div></p>
<p><div align="left"><br>这里，大部分αi为0，因为其对应的训练数据xi,有yi∗(w∗xi+b)&gt;1,只有w∗xi+b=1的训练数据才真正对分类有效，即其对应的αi不为0，这些数据即为支持向量。<br>1.3 根据上一步求得得分类函数对待分类数据x进行二分类。即判断f(x)的正负。</div></p>
<h2 id="2-核函数"><a href="#2-核函数" class="headerlink" title="2.核函数"></a>2.核函数</h2><p>核函数：  K(xi,xj)  ,一般选高斯核函数即可。而核函数的选择往往不是最关键的，分类器性能更依赖于训练数据。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170914220000498?watermark/2/text/aHR0cDo
vL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFC
MA==/dissolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">      

<p><strong> 核函数原理： </strong><br>将低维线性不可分数据通过核函数映射到高维，达到线性可分的目的，并利用内积函数使运算量并未明显增加。</p>
<h2 id="3-多分类器的应用"><a href="#3-多分类器的应用" class="headerlink" title="3.多分类器的应用"></a>3.多分类器的应用</h2><p>SVM是二分类器，对于N类问题的分类有两种办法：<br>1. 训练N个SVM分类器，每个分类器对应分类一种类别与其他N-1种类别，最终分类结果是具有最大正距离的类别，即正向离超平面距离最远。实际应用上应该是取f(x)的最大值。  </p>
<p>2. 训练  N  ∗  (  N  −  1  )  /  2  个SVM分类器，每个分类器对应两两类别的分类，结果通过投票决定，投票依据仍为f(x)。</p>
<h2 id="4-拉格朗日对偶性"><a href="#4-拉格朗日对偶性" class="headerlink" title="4.拉格朗日对偶性"></a>4.拉格朗日对偶性</h2><p>凸函数寻找最大值问题的求解方法，其局部最大值即全局最大值！</p>
<p>目标函数：  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170914223601086?watermark/2/text/aHR0cDo
vL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFC
MA==/dissolve/70/gravity/SouthEast" width="60%"></div></p>
<div align="left">      



<p><div align="center"><br><img src="http://img.blog.csdn.net/20170914223659799?watermark/2/text/aHR0cDo
vL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFC
MA==/dissolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">      



<p><div align="center"><br><img src="http://img.blog.csdn.net/20170914223803442?watermark/2/text/aHR0cDo
vL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFC
MA==/dissolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">      


<p>这里的某些条件即KKT条件：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170914223905254?watermark/2/text/aHR0cDo
vL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFC
MA==/dissolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">      


<h2 id="5-使用松弛变量处理outliers-方法"><a href="#5-使用松弛变量处理outliers-方法" class="headerlink" title="5.使用松弛变量处理outliers 方法"></a>5.使用松弛变量处理outliers 方法</h2><p>数据本身是非线性结构的，而只是因为数据有噪声。对于这种偏离正常位置很远的数据点，我们称之为outlier ，在我们原来的SVM模型里，outlier的存在有可能造成很大的影响，因为超平面本身就是只有少数几个support vector 组成的，如果这些support vector里又存在outlier 的话，其影响就很大了。为了处理这种情况，SVM 允许数据点在一定程度上偏离一下超平面。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170914224828260?watermark/2/text/aHR0cDo
vL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/60/fill/I0JBQkFC
MA==/dissolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">      


</div></div></div></div></div></div>]]></content>
      
        <categories>
            
            <category> Machine Learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[积分图的优化]]></title>
      <url>/2017/07/11/%E7%A7%AF%E5%88%86%E5%9B%BE%E7%9A%84%E4%BC%98%E5%8C%96/</url>
      <content type="html"><![CDATA[<p>原文： <a href="http://www.cnblogs.com/Imageshop/p/6219990.html" target="_blank" rel="external"> 13行代码实现最快速最高效的积分图像算法。 </a>  </p>
<p><strong>Tips：</strong><br>1.某个点的积分图反应的是原图中此位置左上角所有像素之和，这里是的累加和是不包括这个点像素本身的，那么，对于原图的第一行和第一列的所有像素，其对应位置的积分图就应该是0, 这样考虑到所有的像素，为了能容纳最后一列和最后一行的情况，最终的积分图就应该是 (W + 1) X (H + 1)大小。</p>
<p>2.优化方式：  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170908182829328?watermark/2/text/aHR0cDo
vL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFC
MA==/dissolve/70/gravity/SouthEast" width="80%"></div></p>
<div align="left">

<pre><code>原始算法                      列优化                             行优化
</code></pre><p><strong>分析： </strong><br>行优化最优，相比于列优化，更符合图像数据在计算机中的存储顺序，因此并且在计算过程中无需申请额外空间保存各列像素之和，只需一个变量保存行像素之和便可！详见开源软件scantailor里的push函数。</p>
<p>3. 在反汇编中，max和min充分利用了条件按传送指令比如cmovg,cmovl，而一般的if语句很大程度上会调用jmp指令的（当然和常数比较一般编译也会用条件传送替代的，比如和0比较），jmp指令是比较耗时的。</p>
<p><strong>开源软件scantailor里的积分图算法的实现：</strong><br>（调用的时候先调用beginRow定位行指针，然后循环调用push推入该行的像素值，所有行循环完毕就计算完成，然后可以调用sum计算区域累加和。）</p>
<pre><code>/**
 * \brief Provides the sum of values in a sub-rectangle of a 2D array in constant time.
 *
 * Suppose you have a MxN array of some values.  Now if we are not going to
 * alter it, but are going to calculate the sum of values in various
 * rectangular sub-regions, it&apos;s best to use an integral image for this purpose.
 * We construct it once, with complexity of O(M*N), and then obtain the sum
 * of values in a rectangular sub-region in O(1).
 *
 * \note Template parameter T must be large enough to hold the sum of all
 *       values in the array.
 */
template&lt;typename T&gt;
class IntegralImage
{
    DECLARE_NON_COPYABLE(IntegralImage)
public:
    IntegralImage(int width, int height);

    explicit IntegralImage(QSize const&amp; size);

    ~IntegralImage();

    /**
     * \brief To be called before pushing new row data.
     */
    void beginRow();

    /**
     * \brief Push a single value to the integral image.
     *
     * Values must be pushed row by row, starting from row 0, and from
     * column to column within each row, starting from column 0.
     * At the beginning of a row, a call to beginRow() must be made.
     *
     * \note Pushing more than width * height values results in undefined
     *       behavior.
     */
    void push(T val);

    /**
     * \brief Calculate the sum of values in the given rectangle.
     *
     * \note If the rectangle exceeds the image area, the behaviour is
     *       undefined.
     */
    T sum(QRect const&amp; rect) const;
private:
    void init(int width, int height);

    T* m_pData;
    T* m_pCur;
    T* m_pAbove;
    T m_lineSum;
    int m_width;
    int m_height;

};

template&lt;typename T&gt;
IntegralImage&lt;T&gt;::IntegralImage(int const width, int const height)
:   m_lineSum() // init with 0 or with default constructor.
{
    // The first row and column are fake.
    init(width + 1, height + 1);
}

template&lt;typename T&gt;
IntegralImage&lt;T&gt;::IntegralImage(QSize const&amp; size)
:   m_lineSum() // init with 0 or with default constructor.
{
    // The first row and column are fake.
    init(size.width() + 1, size.height() + 1);
}

template&lt;typename T&gt;
IntegralImage&lt;T&gt;::~IntegralImage()
{
    delete[] m_pData;
}

template&lt;typename T&gt;
void
IntegralImage&lt;T&gt;::init(int const width, int const height)
{
    m_width = width;
    m_height = height;

    m_pData = new T[width * height];

    // Initialize the first (fake) row.
    // As for the fake column, we initialize its elements in beginRow().
    T* p = m_pData;
    for (int i = 0; i &lt; width; ++i, ++p) {
        *p = T();
    }

    m_pAbove = m_pData;
    m_pCur = m_pAbove + width; // Skip the first row.
}

template&lt;typename T&gt;
void
IntegralImage&lt;T&gt;::push(T const val)
{
    m_lineSum += val;
    *m_pCur = *m_pAbove + m_lineSum;
    ++m_pCur;
    ++m_pAbove;
}

template&lt;typename T&gt;
void
IntegralImage&lt;T&gt;::beginRow()
{
    m_lineSum = T();

    // Initialize and skip the fake column.
    *m_pCur = T();
    ++m_pCur;
    ++m_pAbove;
}

template&lt;typename T&gt;
inline T
IntegralImage&lt;T&gt;::sum(QRect const&amp; rect) const
{
    // Keep in mind that row 0 and column 0 are fake.
    int const pre_left = rect.left();
    int const pre_right = rect.right() + 1; // QRect::right() is inclusive.
    int const pre_top = rect.top();
    int const pre_bottom = rect.bottom() + 1; // QRect::bottom() is inclusive.
    T sum(m_pData[pre_bottom * m_width + pre_right]);
    sum -= m_pData[pre_top * m_width + pre_right];
    sum += m_pData[pre_top * m_width + pre_left];
    sum -= m_pData[pre_bottom * m_width + pre_left];
    return sum;
}
</code></pre></div>]]></content>
      
        <categories>
            
            <category> Image Processing </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Image Processing </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[分水岭算法的原理及实现]]></title>
      <url>/2017/07/08/%E5%88%86%E6%B0%B4%E5%B2%AD%E7%AE%97%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/</url>
      <content type="html"><![CDATA[<p><strong> 算法步骤： </strong><br>1.构建图像梯度图像。<br>2.通过一定规则生成n个最初的注水区域（先验知识或局部梯度最小值）。<br>3.往注水区域内加水，当两注水区域即将合并时，记录下此时的边界。<br>4.当图像边缘彻底被分割成n个独立区域是算法结束。</p>
<p><strong>算法过程示意图：</strong></p>
<p><img src="http://img.blog.csdn.net/20170904222403387?watermark/2/text/aHR0cDo
vL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFC
MA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p><strong>参考博客：</strong><br><a href="http://blog.csdn.net/zhangsmile123456/article/details/47271955" target="_blank" rel="external"> OpenCV学习(7) 分水岭算法
</a></p>
<p><a href="http://blog.csdn.net/iracer/article/details/49225823" target="_blank" rel="external"> OpenCV—图像分割中的分水岭算法原理与应用 OpenCV—图像分割中的分水岭算法原理与应用
</a></p>
]]></content>
      
        <categories>
            
            <category> Image Processing </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Image Processing </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[利用Python实现下载百度图片]]></title>
      <url>/2017/07/03/%E5%88%A9%E7%94%A8Python%E5%AE%9E%E7%8E%B0%E4%B8%8B%E8%BD%BD%E7%99%BE%E5%BA%A6%E5%9B%BE%E7%89%87/</url>
      <content type="html"><![CDATA[<p>原文地址： <a href="http://blog.sina.com.cn/s/blog_13927ddb50102w2m1.html" target="_blank" rel="external"> Python 3 多线程下载百度图片搜索结果
</a></p>
<pre><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Author: loveNight

import json
import itertools
import urllib
import requests
import os
import re
import sys


str_table = {
&apos;_z2C$q&apos;: &apos;:&apos;,
&apos;_z&amp;e3B&apos;: &apos;.&apos;,
&apos;AzdH3F&apos;: &apos;/&apos;
}

char_table = {
&apos;w&apos;: &apos;a&apos;,
&apos;k&apos;: &apos;b&apos;,
&apos;v&apos;: &apos;c&apos;,
&apos;1&apos;: &apos;d&apos;,
&apos;j&apos;: &apos;e&apos;,
&apos;u&apos;: &apos;f&apos;,
&apos;2&apos;: &apos;g&apos;,
&apos;i&apos;: &apos;h&apos;,
&apos;t&apos;: &apos;i&apos;,
&apos;3&apos;: &apos;j&apos;,
&apos;h&apos;: &apos;k&apos;,
&apos;s&apos;: &apos;l&apos;,
&apos;4&apos;: &apos;m&apos;,
&apos;g&apos;: &apos;n&apos;,
&apos;5&apos;: &apos;o&apos;,
&apos;r&apos;: &apos;p&apos;,
&apos;q&apos;: &apos;q&apos;,
&apos;6&apos;: &apos;r&apos;,
&apos;f&apos;: &apos;s&apos;,
&apos;p&apos;: &apos;t&apos;,
&apos;7&apos;: &apos;u&apos;,
&apos;e&apos;: &apos;v&apos;,
&apos;o&apos;: &apos;w&apos;,
&apos;8&apos;: &apos;1&apos;,
&apos;d&apos;: &apos;2&apos;,
&apos;n&apos;: &apos;3&apos;,
&apos;9&apos;: &apos;4&apos;,
&apos;c&apos;: &apos;5&apos;,
&apos;m&apos;: &apos;6&apos;,
&apos;0&apos;: &apos;7&apos;,
&apos;b&apos;: &apos;8&apos;,
&apos;l&apos;: &apos;9&apos;,
&apos;a&apos;: &apos;0&apos;
}

# str 的translate方法需要用单个字符的十进制unicode编码作为key
# value 中的数字会被当成十进制unicode编码转换成字符
# 也可以直接用字符串作为value
char_table = {ord(key): ord(value) for key, value in char_table.items()}

# 解码图片URL
def decode(url):
    # 先替换字符串
    for key, value in str_table.items():
        url = url.replace(key, value)
    # 再替换剩下的字符
    return url.translate(char_table)

# 生成网址列表
def buildUrls(word):
    word = urllib.parse.quote(word)
    url = r&quot;http://image.baidu.com/search/acjson?tn=resultjson_com&amp;ipn=rj&amp;ct=201326592&amp;fp=result&amp;queryWord={word}&amp;cl=2&amp;lm=-1&amp;ie=utf-8&amp;oe=utf-8&amp;st=-1&amp;ic=0&amp;word={word}&amp;face=0&amp;istype=2nc=1&amp;pn={pn}&amp;rn=60&quot;
    urls = (url.format(word=word, pn=x) for x in itertools.count(start=0, step=60))
    return urls

# 解析JSON获取图片URL
re_url = re.compile(r&apos;&quot;objURL&quot;:&quot;(.*?)&quot;&apos;)

def resolveImgUrl(html):
    imgUrls = [decode(x) for x in re_url.findall(html)]
    return imgUrls

def downImg(imgUrl, dirpath, imgName):
    filename = os.path.join(dirpath, imgName)
    try:
        res = requests.get(imgUrl, timeout=15)
        if str(res.status_code)[0] == &quot;4&quot;:
           print(str(res.status_code), &quot;:&quot; , imgUrl)
           return False
    except Exception as e:
           print(&quot;抛出异常：&quot;, imgUrl)
           print(e)
           return False
    with open(filename, &quot;wb&quot;) as f:
         f.write(res.content)
    return True


def mkDir(dirName):
    dirpath = os.path.join(sys.path[0], dirName)
    if not os.path.exists(dirpath):
           os.mkdir(dirpath)
    return dirpath

if __name__ == &apos;__main__&apos;:
   print(&quot;欢迎使用百度图片下载脚本！\n目前仅支持单个关键词。&quot;)
   print(&quot;下载结果保存在脚本目录下的results文件夹中。&quot;)
   print(&quot;=&quot; * 50)
   word = input(&quot;请输入你要下载的图片关键词：\n&quot;)

   dirpath = mkDir(&quot;results&quot;)

   urls = buildUrls(word)
   index = 0
   for url in urls:
       print(&quot;正在请求：&quot;, url)
       html = requests.get(url, timeout=10).content.decode(&apos;utf-8&apos;)
       imgUrls = resolveImgUrl(html)
       if len(imgUrls) == 0: # 没有图片则结束
          break
       for url in imgUrls:
           if downImg(url, dirpath, str(index) + &quot;.jpg&quot;):
              index += 1
              print(&quot;已下载 %s 张&quot; % index)
</code></pre>]]></content>
      
        <categories>
            
            <category> Code </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Canny边缘检测算法]]></title>
      <url>/2017/07/01/Canny%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<h1 id="1-Canny边缘检测原理"><a href="#1-Canny边缘检测原理" class="headerlink" title="1.Canny边缘检测原理"></a>1.Canny边缘检测原理</h1><p>引用文章：</p>
<p><a href="http://blog.csdn.net/jia20003/article/details/41173767" target="_blank" rel="external"> 图像处理之Canny边缘检测 </a></p>
<p><strong>Tips：</strong></p>
<p>Canny边缘检测算法  ：</p>
<p>step1:用高斯滤波器平滑图象；</p>
<p>step2:用一阶偏导的有限差分来计算梯度的幅值和方向（可选用Sobel算子等）；</p>
<p>step3:对梯度幅值进行非极大值抑制（  比较当前点的梯度强度和正负梯度方向点的梯度强度  ）；</p>
<p>step4:用双阈值算法检测和连接边缘(比较的也是像素梯度值)。</p>
<p>优点：</p>
<p>相比普通的梯度算法大大抑制了噪声引起的伪边缘，而且是边缘细化，易于后续处理。对于对比度较低的图像，通过调节参数，Canny算法也能有很好的效果。  </p>
<h1 id="2-Canny边缘检测算法的改进"><a href="#2-Canny边缘检测算法的改进" class="headerlink" title="2. Canny边缘检测算法的改进"></a>2. Canny边缘检测算法的改进</h1><p><a href="http://www.cnblogs.com/mightycode/p/6560784.html" target="_blank" rel="external"> 一些关于Canny边缘检测算法的改进 </a>  </p>
<p><strong>Tips:</strong></p>
<p>1.高斯模糊可能吧图像中的边缘模糊掉，因此可以设置一个模糊阀值，仅仅让和中心像素灰度的差值小于这个阀值的像素参与计算。这样和中心像素相差过大的像素被认为是带有有效的信息，而不是噪声，不会参与平滑计算，从而保留了这些有用的高频信号，那么边缘信号自然也在保留的范围。</p>
<p>2. 在最初的Canny算法中是使用的最小的2x2领域来计算梯度幅值的。这种方法对噪声很敏感，比较容易检测到伪边缘或漏掉真是边缘。实际上3x3的Sobel梯度算子是一种比较好的选择。</p>
<p>3. 传统Canny算法的双阀值是全局固定的，因此双阀值大小的选取对最终的结果影响很大，也有一些经验，比如选择低阀值是高阀值的0.4或0.5。然而这毕竟是一种经验选择，阀值的确定仍然很难决定一个最优值。而且一个图像的不同局部区域可能需要各不相同的阀值来精确地找到真实边缘，因此全局阀值就不太合适了。</p>
<p>4. 传统算法仍然可能产生一条宽度大于1的边缘，达不到满意的高精度单点响应。也就是需要继续细化边缘。</p>
]]></content>
      
        <categories>
            
            <category> Image Processing </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Image Processing </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Randon变换与Hough变换]]></title>
      <url>/2017/06/26/Randon%E5%8F%98%E6%8D%A2%E4%B8%8EHough%E5%8F%98%E6%8D%A2/</url>
      <content type="html"><![CDATA[<h1 id="1-Randon-变换"><a href="#1-Randon-变换" class="headerlink" title="1. Randon 变换"></a>1. Randon 变换</h1><p><a href="http://blog.csdn.net/carson2005/article/details/40535199" target="_blank" rel="external"> Radon变换简介 </a></p>
<h2 id="1-1-Radon变换的基本原理"><a href="#1-1-Radon变换的基本原理" class="headerlink" title="1.1 Radon变换的基本原理"></a>1.1 Radon变换的基本原理</h2><p>一个平面内沿不同的直线（直线与原点的距离为d，方向角为α）对f（x，y）做线积分，得到的像F(d,α)就是函数f的Radon变换。也就是说，平面（d，α）的每个点的像函数值对应了原始函数的某个线积分值。  </p>
<h2 id="1-2-Radon变换的基本思想"><a href="#1-2-Radon变换的基本思想" class="headerlink" title="1.2 Radon变换的基本思想"></a>1.2 Radon变换的基本思想</h2><p>Radon变换可以理解为图像在(ρ,θ)空间的投影,(ρ,θ)空间的每一点对应一条直线，而Radon变换是图像像素点在每一条直线上的积分。因此，图像中高灰度值的直线会在(ρ,θ)空间形成亮点，而低灰度值的线段在(ρ,θ)空间形成暗点。对直线的检测转化为在变换区域对亮点、暗点的检测。</p>
<h1 id="2-Hough变换"><a href="#2-Hough变换" class="headerlink" title="2. Hough变换"></a>2. Hough变换</h1><p><a href="http://blog.csdn.net/ccxcau/article/details/7816588" target="_blank" rel="external"> Hough Transform(霍夫变换) </a>  </p>
<h2 id="2-1-Hough变换的基本原理"><a href="#2-1-Hough变换的基本原理" class="headerlink" title="2.1 Hough变换的基本原理"></a>2.1 Hough变换的基本原理</h2><p>其基本原理在于利用点与线的对偶性，将原始图像空间给定的曲线通过曲线表达形式变为参数空间的一个点。这样就把原始图像中给定曲线的检测问题转化为寻找参数空间中的峰值问题。也即把检测整体特性转化为检测局部特性。比如可以将Hough变换推广为检测直线、椭圆、圆、弧线等。 </p>
<h2 id="2-2-Hough变换的基本思想"><a href="#2-2-Hough变换的基本思想" class="headerlink" title="2.2 Hough变换的基本思想"></a>2.2 Hough变换的基本思想</h2><p>在原始图像坐标系下的一个点对应了参数坐标系中的一条直线，同样参数坐标系的一条直线对应了原始坐标系下的一个点，然后，原始坐标系下呈现直线的所有点，它们的斜率和截距是相同的，所以它们在参数坐标系下对应于同一个点。这样在将原始坐标系下的各个点投影到参数坐标系下之后，看参数坐标系下有没有聚集点，这样的聚集点就对应了原始坐标系下的直线。 </p>
<h2 id="2-3-对于未知半径的圆检测的简化"><a href="#2-3-对于未知半径的圆检测的简化" class="headerlink" title="2.3 对于未知半径的圆检测的简化"></a>2.3 对于未知半径的圆检测的简化</h2><p>a. 图像边缘除了位置信息，还有方向信息也很重要，这里就用上了。根据圆的性质，圆的半径一定在垂直于圆的切线的直线上，也就是说，在圆上任意一点的法线上。这样，解决上面的问题，我们仍采用2维的参数空间，对于图像上的每一前景点，加上它的方向信息，都可以确定出一条直线，圆的圆心就在这条直线上。这样一来，问题就会简单了许多。  </p>
<p>问题：实际上这种处理方式存在精度问题，即实际场景中由于噪声、边缘断裂等原因往往不能得到精确边缘方向！  </p>
<p>b. 同样是利用边缘点的信息，但是引入边缘方向误差Δφ，根据当前图像空间点x的边缘方向φ(x)，对映射到参数空间（a,b）的部分圆参与累计，即： </p>
<p>$$a = x1-R*cos(φ(x)) $$ </p>
<p>$$b = x1-R*cos(φ(x)) $$ </p>
<p>$$ φ(x)∈[ φ(x) - Δφ,φ(x) + Δφ ]$$</p>
<p>（详见图像处理、分析与机器视觉P156）</p>
<h2 id="2-4-广义霍夫变换"><a href="#2-4-广义霍夫变换" class="headerlink" title="2.4 广义霍夫变换"></a>2.4 广义霍夫变换</h2><p><a href="http://blog.csdn.net/tiandijun/article/details/47251913" target="_blank" rel="external"> 霍夫变换到广义霍夫变换 </a></p>
<p><strong>Tips： </strong> </p>
<p>Ballard (1981) 的广义霍夫变换最精妙之处在于为参数增加了两个关联，使得有平移和旋转（无缩放）的情况只需要遍历一个参数，三个参数分别是图形的中心坐标（横纵），旋转角度（相对参考图形），Ballard 的算法预先把参考图形边缘点对中心的径向量保存起来，利用待搜索图形边缘点的梯度方向（用相对坐标轴的角度表示）作为索引找到相应的径向量，加上该量后就完成了投射，所以要遍历的参数只有旋转角度，所以说有两个关联。当然如果加上缩放就要遍历两个参数，这也只是和霍夫检测圆的规模一样而已。</p>
<h1 id="3-Hough变换与Radon变换的联系与区别"><a href="#3-Hough变换与Radon变换的联系与区别" class="headerlink" title="3. Hough变换与Radon变换的联系与区别"></a>3. Hough变换与Radon变换的联系与区别</h1><p><a href="http://blog.sina.com.cn/s/blog_66d4b4620101eig3.html" target="_blank" rel="external"> Hough变换与Radon变换的联系与区别
</a></p>
<p>Hough变换把图像空间中给定的曲线按曲线的参数表达式变换成参数空间中的点，然后通过在参数空间中寻找峰值来达到在图像空间中寻找曲线的目的。可以使用Hough变换来寻找图像中的直线。</p>
<p>Radon变换则以线积分的形式把图像空间投影到ρθ空间（等同于直线的参数空间）。</p>
<p>直线Hough变换与Radon变换的区别在于前者是直线参数变换的离散形式，而后者则是直线参数变换的连续形式。所以Hough变换直接应用在二值图像上，而Radon变换直接应用在灰度图像上。另外，由于二值图像只需要处理前景或者背景像素，所以Hough变换速度一般更快。Hough变换通常用在几何形状检测、文档版面分割等领域。</p>
<p>而Radon变换也有独特的优势。由于二值图像的不连续性，表面上看Hough变换结果中峰值位置明显，效果比Radon变换好，但实际上由于通常意义上难以对一幅图像进行恰当的二值分割，所以 <strong> 在一般情况下Radon变换要比Hough变换更精致而且准确 </strong>。Radon变换是全面的变换，可以从Radon变换的结果重建变换前的图像。所以在断层扫描中大量使用了Radon变换及其逆变换。</p>
]]></content>
      
        <categories>
            
            <category> Image Processing </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Math </tag>
            
            <tag> Image Processing </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[提升人脸检测效率的方法总结]]></title>
      <url>/2017/06/21/%E6%8F%90%E5%8D%87%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B%E6%95%88%E7%8E%87%E7%9A%84%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<p>人脸检测是一个非常经典的问题，很多人认为这是一个“已经解决”了的问题。人脸检测最经典的方法是Haar+AdaBoost。采用开源的Haar+AdaBoost实现（如OpenCV中的训练和检测程序），我们可以很容易的训练一个还算不错的人脸检测器。但是，一旦将人脸检测技术投入实际应用，一系列问题便会冒出来。如（1）误检（把非人脸的物体当作人脸）较多，非人脸图像当作人脸送入后续算法，会引起一系列不良后果。（2）漏检问题，例如戴墨镜、大胡子、逆光条件、黑种人、倾斜姿态较大的脸无法检测到。（3）速度问题，虽很多人脸检测算法的速度已经很快，但在一个人脸分析（如人脸识别）系统中，人脸检测步骤的计算量往往超过50%。大部分检测算法采用窗口扫描的方式，窗口数目巨大，所以计算量居高不下。很多系统运行在低功耗平台，对计算量尤其敏感。假如手机一拍照片就发烫就很不好。</p>
<p>下图中人脸因为有墨镜和大胡子遮挡，加上角度倾斜，较难检测。图中红框本不是人脸，如被当作人脸，则是误检测。</p>
<p>下面介绍一下我们设计Boosting人脸检测方法的一些心得体会：</p>
<h1 id="1-特征设计"><a href="#1-特征设计" class="headerlink" title="1. 特征设计"></a>1. 特征设计</h1><p>特征设计是重中之重，如果特征从原理上就是计算量大，后面无论如何优化，都很难降计算量。</p>
<p>先说Haar特征，毫无疑问Haar特征用在人脸检测里具有里程碑式的意义。但现在看Haar特征，会发现它有一些明显的缺点。Haar特征是一种很弱的特征，这意味着需要很多特征组合在一起才能构成强分类器，对提升速度不利；如果正样本分布比较分散比较难区分时（例如正面侧面人脸全放进去），用这种弱特征分类会比较吃力。此外，在Haar特征的实现中，为了解决亮度归一化问题，需计算像素值的平方和（square sum），平方和需要64位整数来存储；还需要开方（sqrt）运算。64位整数运算和开方运算，对很多嵌入式系统来讲，都是高计算量操作。</p>
<p>HOG特征是一个描述能力特别强的特征，也可以用在人脸检测上。HOG特征需要计算梯度的方向和长度。计算方向需要ctan三角函数（可用查表法加速）以及开方操作。统计直方图时，为了效果好，需要soft voting加权投票。因此HOG是强描述特征，但特征提取的计算量较大。</p>
<p>   （图片来自 <a href="http://dlib.net）" target="_blank" rel="external">http://dlib.net）</a></p>
<p>LBP是一个好特征，描述能力强，特征提取仅需要逻辑运算和加法运算，值得推荐！LBP依然有吐槽点。如果你实现Multi-Block LBP，需要积分图来加速。积分图是很多算法的加速武器，但构建积分图时，很难用SIMD（单指令多数据）指令优化。</p>
<p>前面三种特征的对比分析，是为了说明好的特征要表达能力强且计算简单。很多二值特征（Binary feature）符合这一特点。我也偏好二值特征。二值特征还可以天然地解决图像的亮度变化问题，不需要事先对图像进行亮度均衡化。（例如使用Haar+AdaBoost检测人脸前，先对图像做直方图均衡化再检测，效果会好很多，不信你去试。）</p>
<h1 id="2-样本"><a href="#2-样本" class="headerlink" title="2. 样本"></a>2. 样本</h1><p>很多论文中会洋洋洒洒地介绍算法的先进性，但很少有论文分析样本对结果的影响。可能分析样本显得很工程化很low吧。而描述样本在高维特征空间的分布，应该是很多模式识别问题的核心问题。</p>
<p>好，不谈理论谈经验。样本选择是一般人不提的重要事情。如果你用手机自拍照片训练人脸检测器，应用在视频监控中，一般效果不会太好；如果你对所有人脸样本进行人脸对齐，要求双目绝对水平，那么训练出的分类器速度会比较快，但不能处理人脸姿态变化。即人脸样本越单一，训练出的分类器的速度会越快，但正确检测率低；如果样本复杂，速度变慢但检测率升高。如何平衡样本的复杂性和检测速度，需要针对具体应用斟酌。</p>
<p>此外负样本也很关键。如果你从几千张风景图里抠图作为负样本进行训练，那么基本上会overfitting，即训练时误检率很低，但实际应用时误检率比较高。要准确刻画非人脸图像，负样本的规模一定要大，负样本的内容一定要多样化！我们平时训练用的负样本图像大约有20GB，包含各种内容的图像。</p>
<h1 id="3-分类器训练"><a href="#3-分类器训练" class="headerlink" title="3. 分类器训练"></a>3. 分类器训练</h1><p>这个可说的不多，Boosting方法就那些，可以尝试各种Boosting方法和各种参数，找到合适的。</p>
<p>后面会说说代码层面的优化。</p>
<h1 id="4-代码优化：消灭重复计算"><a href="#4-代码优化：消灭重复计算" class="headerlink" title="4. 代码优化：消灭重复计算"></a>4. 代码优化：消灭重复计算</h1><p>通过分析工具，找出最影响速度的代码段，有针对性地优化。一般来说是判断窗口是否是人脸的代码最耗时，因为调用次数最多。代码里首先要消灭的是重复计算，如代码<br>int b1 = pixels[y<em>step + x] - pixels[y</em>step + x + 1];<br>int b2 = pixels[y<em>step + x + 2] - pixels[y</em>step + x + 3];<br>可以写为<br>int offset = y *step + x;<br>int b1 =pixels[offset] - pixels[offset+1];<br>int b2 = pixels[offset+2] - pixels[offset+3];</p>
<h1 id="5-代码优化：展开循环"><a href="#5-代码优化：展开循环" class="headerlink" title="5. 代码优化：展开循环"></a>5. 代码优化：展开循环</h1><p>如果循环次数是固定的，可以去掉for循环，直接展开。代码行数虽然多了，但是少了for循环的条件判断，可以加速，例如可加速10%-20%。另外可以在设计分类器的时候，就把这些因素考虑进去，由训练程序生成的强分类器包含固定数目的弱分类器，或者某种规律数目的弱分类器，这样有利于检测代码优化。</p>
<h1 id="6-代码优化：利用SIMD指令"><a href="#6-代码优化：利用SIMD指令" class="headerlink" title="6. 代码优化：利用SIMD指令"></a>6. 代码优化：利用SIMD指令</h1><p>无论Intel CPU还是ARM CPU，都有SIMD（单指令多数据）指令。利用这些指令，可以一次算多个数据。例如两个BYTE向量相加，支持128位的SIMD指令可以一次算16个BYTE的加法，理论上可以加速16倍。我们常用的加速利器积分图，它的构建过程很难用SIMD加速。如果你有更好的策略，可以果断抛弃积分图。</p>
<h1 id="7-代码优化：多核并行运算"><a href="#7-代码优化：多核并行运算" class="headerlink" title="7. 代码优化：多核并行运算"></a>7. 代码优化：多核并行运算</h1><p>OpenMP或者Intel TBB可以让我们充分利用CPU的多个内核进行并行运算，提升速度。但用了OpenMP或TBB，未必可以加速，或未必可以加速到期望的倍数。多核并行，任务的拆分的粒度应该尽可能粗，不同的任务尽可能不用同一块内存，也就是任务之间的相关度低一些有利于加速。</p>
<p>举给例子：如果两个矩阵相加，按像素进行并行操作，加速效果会很糟糕；如果按行并行，效果会好很多，但不要按列并行！</p>
<h1 id="8-代码优化：定点化"><a href="#8-代码优化：定点化" class="headerlink" title="8. 代码优化：定点化"></a>8. 代码优化：定点化</h1><p>有些低功耗嵌入式系统不支持硬件浮点运算，特征提取和分类器设计应尽可能避免浮点运算。不可避免的浮点数可以转为定点数，当然这会损失精度。我们曾经将float类型转为8位整数，而准确率无明显影响。</p>
<h1 id="9-代码优化：GPU优化"><a href="#9-代码优化：GPU优化" class="headerlink" title="9. 代码优化：GPU优化"></a>9. 代码优化：GPU优化</h1><p>GPU跟CPU不同，有自己独特的特点。GPU特别适合做无复杂逻辑但计算量大的事情。例如图像resize、图像滤波等。Boosting算法可以运行在GPU上，并获得加速，但GPU加速Boosting优势并没有你想象的明显。Boosting算法中逻辑分支较多，也就是有不定长的for循环，有if-else判断；并行的时候每个运算单元运算量并不相同，有些运行时间长，有些运行时间短。运行时间短的要等运行时间长的。我们做了很多努力，GPU版本（低端GTX750i显卡）速度大约可以做到CPU版本（i7多核并行）的6倍左右。跟OpenCV中用GPU加速Haar+Adaboost的倍数一致，而没有达到期望的几十倍加速。</p>
<h1 id="10-吐槽：非连续内存读写"><a href="#10-吐槽：非连续内存读写" class="headerlink" title="10. 吐槽：非连续内存读写"></a>10. 吐槽：非连续内存读写</h1><p>读写非连续内存，速度可能变慢。如下图Haar积分图计算像素和的公式是D-B-C+A，需要读4个元素值，但这4个值在内存里并不连续，特别是A和D距离较远，这样降低了CPU缓存的命中率，也就降低了速度。对缓存小的低价位CPU影响尤为明显。虽然公式D-B-C+A的计算量跟E-F-G-H相同，但一般来说后者速度会快。此外，Boosting方法选出的多个弱特征，在图像上往往也不相邻，同样是低效的非连续内存的读写，这是Boosting类方法的一个缺点。</p>
<h1 id="11-未来展望"><a href="#11-未来展望" class="headerlink" title="11. 未来展望"></a>11. 未来展望</h1><p>到目前为止，Boosting方法在人脸检测中依然具有明显的速度优势。但基于深度学习的目标检测方法进展迅速，不容小视。深度学习的方法可以很容易地获得非常高的准确率，如在FDDB人脸检测评库上，传统方法能达到85%准确率就非常高了，但深度学习方法可以轻松超过90%，甚至超过95%。如果想兼顾速度和准确率，传统方法和深度学习的方法结合也许是一个思路，现在已经有一些这样的尝试了。此外，CNN专用硬件应可以弥补深度学习方法的劣势。</p>
]]></content>
      
        <categories>
            
            <category> Image Processing </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Image Processing </tag>
            
            <tag> algorithm </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[白平衡算法总结]]></title>
      <url>/2017/06/21/%E7%99%BD%E5%B9%B3%E8%A1%A1%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<h1 id="1-图像自动白平衡算法的原理、实现及效果"><a href="#1-图像自动白平衡算法的原理、实现及效果" class="headerlink" title="1.图像自动白平衡算法的原理、实现及效果"></a>1.图像自动白平衡算法的原理、实现及效果</h1><p><a href="http://www.cnblogs.com/Imageshop/archive/2013/04/20/3032062.html" target="_blank" rel="external">基于灰度世界、完美反射、动态阈值等图像自动白平衡算法的原理、实现及效果</a></p>
<p><strong>TIPS:</strong></p>
<ol>
<li><p>灰度世界算法（Gray World)是以灰度世界假设为基础的,该假设认为对于一幅有着大量色彩变化的图像, R、 G、 B 三个分量的平均值趋于同一个灰度K。</p>
</li>
<li><p>完美全反射理论perfect Reflector假设图像上最亮点就是白点，并以此白点为参考对图像进行自动白平衡,最亮点定义为R+G+B的最大值。</p>
</li>
</ol>
<h1 id="2-基于色温估计的自动白平衡"><a href="#2-基于色温估计的自动白平衡" class="headerlink" title="2. 基于色温估计的自动白平衡"></a>2. 基于色温估计的自动白平衡</h1><p><a href="http://blog.csdn.net/lz0499/article/details/70495698" target="_blank" rel="external">基于色温估计的自动白平衡</a></p>
<p><strong>Tips:</strong></p>
<ol>
<li><p>相同的物体在不同的色温光源下呈现出相同的颜色，这就称为色彩恒常性。</p>
</li>
<li><p>白平衡的目的就是使得图像传感器在不同色温光源下拍摄的物体进行一个图像纠正的过程，还原物体本来的颜色。也可以说是在任意色温条件下，图像传感器所拍摄的标准白色经过白平衡的调整，使之成像后仍然为白色。</p>
</li>
</ol>
<p><a href="http://blog.csdn.net/wzwxiaozheng/article/details/38434391" target="_blank" rel="external">自动白平衡(AWB)算法—1,色温曲线</a><br><a href="http://blog.csdn.net/wzwxiaozheng/article/details/40586293" target="_blank" rel="external">自动白平衡(AWB)算法—2,色温计算</a></p>
]]></content>
      
        <categories>
            
            <category> Image Processing </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Image Processing </tag>
            
            <tag> algorithm </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[PCA与SVD]]></title>
      <url>/2017/06/16/PCA%E4%B8%8ESVD/</url>
      <content type="html"><![CDATA[<h1 id="1-PCA算法"><a href="#1-PCA算法" class="headerlink" title="1. PCA算法"></a>1. PCA算法</h1><h2 id="1-1-PCA的数学原理"><a href="#1-1-PCA的数学原理" class="headerlink" title="1.1 PCA的数学原理"></a>1.1 PCA的数学原理</h2><p>转载： <a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="external"> PCA的数学原理 </a></p>
<p><strong>PCA的算法步骤：</strong></p>
<p>设有m条n维数据。</p>
<p>1）将原始数据按列组成n行m列矩阵X</p>
<p>2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</p>
<p>3）求出协方差矩阵  $$C=1/mXX^T$$</p>
<p>4）求出协方差矩阵的特征值及对应的特征向量</p>
<p>5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P</p>
<p>6）  Y=PX  即为降维到k维后的数据</p>
<h2 id="1-2-主成分分析-PCA-原理及推导"><a href="#1-2-主成分分析-PCA-原理及推导" class="headerlink" title="1.2 主成分分析(PCA)原理及推导"></a>1.2 主成分分析(PCA)原理及推导</h2><p><a href="http://blog.csdn.net/zhongkejingwang/article/details/42264479" target="_blank" rel="external"> 主成分分析(PCA)原理及推导
</a>  </p>
<p><strong>Tips:</strong></p>
<p>a.  A与B的内积等于A到B的投影长度乘以B的模。</p>
<p>b.两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去  。更抽象的说，一个矩阵可以表示一种线性变换。</p>
<h1 id="2-SVD"><a href="#2-SVD" class="headerlink" title="2. SVD"></a>2. SVD</h1><p><a href="http://blog.sciencenet.cn/blog-696950-699432.html" target="_blank" rel="external"> 奇异值分解(SVD) -– 几何意义 </a>  </p>
<p><strong>Tips：</strong></p>
<p>a. SVD分解公式：  $$M  =  U  Σ  V^T$$</p>
<p>这表明任意的矩阵  M  是可以分解成三个矩阵。  V  表示了原始域的标准正交基，  u  表示经过  M  变换后的co-domain的标准正交基，<br>Σ  表示了  V  中的向量与  u  中相对应向量之间的关系。  </p>
<h1 id="3-PCA与SVD之间的关系"><a href="#3-PCA与SVD之间的关系" class="headerlink" title="3. PCA与SVD之间的关系"></a>3. PCA与SVD之间的关系</h1><p><a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-
applications.html" target="_blank" rel="external"> 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用
</a>  </p>
<p><strong>Tips：</strong></p>
<p>a.  特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的，对于普通矩阵，只能用SVD。</p>
<p>b.  PCA几乎可以说是对SVD的一个包装，如果我们实现了SVD，那也就实现了PCA了，而且更好的地方是，有了SVD，我们就可以得到两个方向的PCA，如果<br>我们对A’A进行特征值的分解，只能得到一个方向的PCA。</p>
<p>c.SVD分解基本方法：</p>
<p>我们将一个矩阵A的转置 * A，将会得到一个方阵，我们用这个方阵求特征值可以得到：</p>
<p><div align="center"><br><img src="http://images.cnblogs.com
/cnblogs_com/LeftNotEasy/201101/201101192226348223.png" width="15%" align="center"></div></p>
<p>这里得到的v，就是我们上面的右奇异向量。此外我们还可以得到：</p>
<p><div align="center"><br><img src="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/2011011922
26346304.png" width="10%" align="center"></div></p>
<p>这里的σ就是上面说的奇异值，u就是上面说的左奇异向量。奇异值σ跟特征值类似，在矩阵Σ中也是从大到小排列，而且σ的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了  。也就是说，我们也可以用前r大的奇异值来近似描述矩阵，这里定义一下部分奇异值分解  ：</p>
<p><div align="center"><br><img src="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/2011011922
26358289.png" width="25%" align="center"></div></p>
]]></content>
      
        <categories>
            
            <category> Math </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Algorithm </tag>
            
            <tag> Math </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Opencv中Adaboost的具体实现及使用资料总结]]></title>
      <url>/2017/06/06/Opencv%E4%B8%ADAdaboost%E7%9A%84%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0%E5%8F%8A%E4%BD%BF%E7%94%A8%E8%B5%84%E6%96%99%E6%80%BB%E7%BB%93/</url>
      <content type="html"><![CDATA[<h1 id="1-opencv级联分类器训练"><a href="#1-opencv级联分类器训练" class="headerlink" title="1.opencv级联分类器训练"></a>1.opencv级联分类器训练</h1><blockquote>
<p><a href="http://www.opencv.org.cn/opencvdoc/2.3.2/html/doc/user_guide/ug_traincascade.html?highlight=opencv_traincascade" target="_blank" rel="external">opencv官方指南</a></p>
</blockquote>
<p><a href="http://blog.csdn.net/zdyueguanyun/article/details/8572385" target="_blank" rel="external">opencv级联分类器训练</a></p>
<p><strong>Tips：</strong></p>
<p>1.opencv_createsamples可以根据需要通过图像处理生成更多正样本。<br>2.在利用opencv_traincascade进行训练时可适当提高precalcIdxBufSize和precalcValBufSize缓存大小，缓存越大，训练时间越短。<br>3.weightTrimRate：Specifies whether trimming should be used and its weight. 一个还不错的数值是0.95。影响参与训练的样本，不管是正样本还是负样本，当更新完样本权重之后，将样本权重按照从小到大的顺序排列，当从后累加样本权重不小于weightTrimWeight时前面的样本就不参与后面的训练了，这里有一个优化是等于该阈值的样本还是参与训练的，这样可以在保证训练精度的基础上缩短检测时间，因为我们认为是样本权重很小的时候表示该样本被正确分类了，而忽略了样本很多的时候，尽管样本没有被正确分类，也可能被排除在外了。还有一种情况就是正样本比例悬殊的时候，例如1:20，当正样本完全分类正确了，那么正样本比例总和仅占4.8%，后续参与训练的正样本可能就没有了</p>
<h1 id="2-AdaBosst算法分析"><a href="#2-AdaBosst算法分析" class="headerlink" title="2 AdaBosst算法分析"></a>2 AdaBosst算法分析</h1><h2 id="2-1-Haar特征详细介绍"><a href="#2-1-Haar特征详细介绍" class="headerlink" title="2.1 Haar特征详细介绍"></a>2.1 Haar特征详细介绍</h2><p><a href="http://blog.csdn.net/zy1034092330/article/details/48850437" target="_blank" rel="external">Haar特征详细介绍</a></p>
<p><strong>Tips：</strong></p>
<p>  1..在保存分类器的OpenCV XML文件中，每一个Haar特征都被保存在2~3个形如&lt;x y width height weight&gt;的标签中。<br>  2.计算完相应的特征值后，还会检测窗口的灰度值及灰度值平方进行压缩特征值范围。</p>
<h2 id="2-2-积分图和45°旋转积分图"><a href="#2-2-积分图和45°旋转积分图" class="headerlink" title="2.2 积分图和45°旋转积分图"></a>2.2 积分图和45°旋转积分图</h2><p>  <a href="http://blog.csdn.net/zy1034092330/article/details/50383097" target="_blank" rel="external">积分图和45°旋转积分图</a></p>
<p><strong>Tips：</strong></p>
<p>Opencv对原积分图进行了“扩边”，积分图中第0行和第0列的值都为0。</p>
<h2 id="2-3-级联分类器结构与XML文件含义"><a href="#2-3-级联分类器结构与XML文件含义" class="headerlink" title="2.3 级联分类器结构与XML文件含义"></a>2.3 级联分类器结构与XML文件含义</h2><p><a href="http://blog.csdn.net/zy1034092330/article/details/53231759" target="_blank" rel="external">级联分类器结构与XML文件含义</a></p>
<p><strong>Tips:</strong></p>
<p>1.一个完整的弱分类器包括： </p>
<p>a.若干个Haar特征 + 和Haar特征数量相等的弱分类器阈值</p>
<p>b. 若干个leftValue</p>
<p>c. 若干个rightValue</p>
<p>2.弱分类器中的idx用法很精妙，注意体会！</p>
<p>3.一般来说，如果用用硬件实现则缩小图像更快，用软件实现算法则放大检测窗口更快?</p>
<h2 id="2-4-利用并查集合并检测窗口-NMS"><a href="#2-4-利用并查集合并检测窗口-NMS" class="headerlink" title="2.4 利用并查集合并检测窗口(NMS)"></a>2.4 利用并查集合并检测窗口(NMS)</h2><p><a href="http://blog.csdn.net/zy1034092330/article/details/53231773" target="_blank" rel="external">利用并查集合并检测窗口(NMS)</a></p>
<p><strong>Tips:</strong></p>
<p>1.在partition函数中，体会nodes[root2][RANK] += rank == rank2（Line57）以及nodes[root][RANK] = ~nclasses++;（Line93）<br>思考：<br>    使用并查集可以明显看出其算法复杂程度为o(n2)，在实际应用中可以发现，其实有大量的检测框是重复的，即实际人脸数K《 检测框N！我们是否考虑在第二次循环时仅与每个集合的平均检测框进行匹配，匹配成功则更新这个集合的平均检测框数值，否则增加为新的集合，这样算法复杂度就为0(k*n).</p>
<h2 id="2-5-AdaBoost之DAB与GAB"><a href="#2-5-AdaBoost之DAB与GAB" class="headerlink" title="2.5 AdaBoost之DAB与GAB"></a>2.5 AdaBoost之DAB与GAB</h2><p><a href="http://blog.csdn.net/zy1034092330/article/details/50445726" target="_blank" rel="external">AdaBoost之DAB与GAB</a></p>
<p><strong>Tips:</strong></p>
<p>GAB和DAB有2处不同，解释如下：</p>
<ol>
<li>DAB和GAB使用的分类器权重误差不一样，GAB是“weighted least-squares”，也就是上面的WSE。应该比较好理解。</li>
<li>DAB和GAB的弱分类器对样本xi的f(xi)不一样。DAB的f(xi)不是+1就是-1；而GAB的f(xi)输出的是一种类似于概率的值。</li>
</ol>
<h2 id="2-6-minHitRate与maxFalseAlarm"><a href="#2-6-minHitRate与maxFalseAlarm" class="headerlink" title="2.6 minHitRate与maxFalseAlarm"></a>2.6 minHitRate与maxFalseAlarm</h2><p><a href="http://blog.csdn.net/zy1034092330/article/details/54428704" target="_blank" rel="external">minHitRate与maxFalseAlarm</a></p>
<p><strong>Tips:</strong></p>
<ol>
<li>由于串联的stage数量很多，minHitRate必须非常接近1，才能保证最终检测器有较好的recall；</li>
<li>falseAlarmRate相当于对检测器的precision作了约束；</li>
<li>相对于maxFalseAlarmRate，minHitRate更加敏感。</li>
</ol>
<h2 id="2-7-分类器训练过程"><a href="#2-7-分类器训练过程" class="headerlink" title="2.7  分类器训练过程"></a>2.7  分类器训练过程</h2><p><a href="http://blog.csdn.net/zy1034092330/article/details/54600014" target="_blank" rel="external">分类器训练过程</a></p>
<p><strong>Tips:</strong></p>
<p>整个分类器的训练过程可以分为以下几个步骤：</p>
<ol>
<li>寻找TP和FP作为训练样本</li>
<li>计算每个Haar特征在当前权重下的Best split threshold+leftvalue+rightvalue，组成了一个个弱分类器</li>
<li>通过WSE寻找最优的弱分类器</li>
<li>更新权重</li>
<li>按照minHitRate估计stageThreshold</li>
<li>重复上述1-5步骤，直到falseAlarmRate到达要求，或弱分类器数量足够。停止循环，输出stage。</li>
<li>进入下一个stage训练</li>
</ol>
<h1 id="3-代码分析"><a href="#3-代码分析" class="headerlink" title="3 代码分析"></a>3 代码分析</h1><p><a href="http://blog.csdn.net/zhaocj/article/details/54291762" target="_blank" rel="external">Opencv2.4.9源码分析——Cascade Classification（二）</a></p>
]]></content>
      
        <categories>
            
            <category> Machine Learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Algorithm </tag>
            
            <tag> Image Processing </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
