<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      Yolo2代码解析 | Kuekua&#39;s blog 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="Kuekua Wu">
    
    

    <meta name="description" content="1. 激活层123456789101112131415161718192021222324252627282930/*计算激活函数对加权输入的导数，并乘以delta，得到当前层最终的delta（敏感度图）输入： x  当前层的所有输出n    l.output的维度，即为l.batch * l.out_c * l.out_w * l.out_h（包含整个batch的）ACTIVATION    激">
<meta name="keywords" content="Machine Learning,Algorithm,Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Yolo2代码解析 | Kuekua&#39;s blog">
<meta property="og:url" content="http://yoursite.com/2017/11/11/Yolo2代码解析/index.html">
<meta property="og:site_name" content="Kuekua&#39;s blog">
<meta property="og:description" content="1. 激活层123456789101112131415161718192021222324252627282930/*计算激活函数对加权输入的导数，并乘以delta，得到当前层最终的delta（敏感度图）输入： x  当前层的所有输出n    l.output的维度，即为l.batch * l.out_c * l.out_w * l.out_h（包含整个batch的）ACTIVATION    激">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2017-11-14T15:36:12.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Yolo2代码解析 | Kuekua&#39;s blog">
<meta name="twitter:description" content="1. 激活层123456789101112131415161718192021222324252627282930/*计算激活函数对加权输入的导数，并乘以delta，得到当前层最终的delta（敏感度图）输入： x  当前层的所有输出n    l.output的维度，即为l.batch * l.out_c * l.out_w * l.out_h（包含整个batch的）ACTIVATION    激">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.ico">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css">

</head>
<body>

    <span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">Kuekua&#39;s blog</a></h1>
        <hr class="panel-cover__divider" />

        
        <p class="panel-cover__description">
          YesterDay you said tomorrow!
        </p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">首页</a></li>
              
                
                <li class="navigation__item"><a href="/archive" title="" class="">分类</a></li>
              
                
                <li class="navigation__item"><a href="/about" title="" class="">关于</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">Yolo2代码解析</h1>

    

    <div class="post-meta">
      <time datetime="2017-11-11" class="post-meta__date date">2017-11-11</time> 

      <span class="post-meta__tags tags">

          
            <font class="categories">
            &#8226; 分类:
            <a class="categories-link" href="/categories/Deep-Learning/">Deep Learning</a>
            </font>
          

          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-link" href="/tags/Algorithm/">Algorithm</a>, <a class="tags-link" href="/tags/Deep-Learning/">Deep Learning</a>, <a class="tags-link" href="/tags/Machine-Learning/">Machine Learning</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <h2 id="1-激活层"><a href="#1-激活层" class="headerlink" title="1. 激活层"></a>1. 激活层</h2><figure class="highlight ada"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">/*</div><div class="line">计算激活函数对加权输入的导数，并乘以<span class="keyword">delta</span>，得到当前层最终的<span class="keyword">delta</span>（敏感度图）</div><div class="line">输入： x  当前层的所有输出</div><div class="line">n    l.output的维度，即为l.batch * l.out_c * l.out_w * l.out_h（包含整个batch的）</div><div class="line">ACTIVATION    激活函数类型</div><div class="line"><span class="keyword">delta</span>     当前层敏感度图（与当前层输出x维度一样）</div><div class="line"></div><div class="line">说明<span class="number">1</span>： 该函数不但计算了激活函数对于加权输入的导数，还将该导数乘以了之前完成大部分计算的敏感度图<span class="keyword">delta</span>（对应元素相乘），</div><div class="line">因此调用改函数之后，将得到该层最终的敏感度图</div><div class="line"></div><div class="line">说明<span class="number">2</span>： 这里直接利用输出值求激活函数关于输入的导数值是因为神经网络中所使用的绝大部分激活函数，</div><div class="line">其关于输入的导数值都可以描述为输出值的函数表达式，比如对于Sigmoid激活函数（记作f(x)），其导数值为f(x)'=f(x)*(<span class="number">1</span>-f(x)),</div><div class="line">因此如果给出y=f(x)，那么f(x)'=y*(<span class="number">1</span>-y)，只需要输出值y就可以了，不需要输入x的值，</div><div class="line">（暂时不确定darknet中有没有使用特殊的激活函数，以致于必须要输入值才能够求出导数值，</div><div class="line">在activiation.c文件中，有几个激活函数暂时没看懂，也没在网上查到）。</div><div class="line"></div><div class="line">说明<span class="number">3</span>： 关于l.<span class="keyword">delta</span>的初值，可能你有注意到在看某一类型网络层的时候，比如卷积层中的backward_convolutional_layer()函数，</div><div class="line">没有发现在此之前对l.<span class="keyword">delta</span>赋初值的语句，只是用calloc为其动态分配了内存，这样的l.<span class="keyword">delta</span>其所有元素的值都为<span class="number">0</span>,</div><div class="line">那么这里使用*=运算符得到的值将恒为<span class="number">0</span>。是的，如果只看某一层，或者说某一类型的层，的确有这个疑惑，但是整个网络是有很多层的，</div><div class="line">且有多种类型，一般来说，不会以卷积层为最后一层，而回以COST或者REGION为最后一层，这些层中，会对l.<span class="keyword">delta</span>赋初值，</div><div class="line">又由于l.<span class="keyword">delta</span>是由后网前逐层传播的，因此，当反向运行到某一层时，l.<span class="keyword">delta</span>的值将都不会为<span class="number">0</span>.</div><div class="line">*/</div><div class="line"></div><div class="line">void gradient_array(const float *x, const int n, const ACTIVATION a, float *<span class="keyword">delta</span>)</div><div class="line">&#123;</div><div class="line">    int i;</div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</div><div class="line">        <span class="keyword">delta</span>[i] *= gradient(x[i], a);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="2-Softmax层"><a href="#2-Softmax层" class="headerlink" title="2. Softmax层"></a>2. Softmax层</h2><h3 id="2-1-前向传播函数"><a href="#2-1-前向传播函数" class="headerlink" title="2.1 前向传播函数"></a>2.1 前向传播函数</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** softmax层前向传播函数</span></div><div class="line"><span class="comment">** 输入： l   当前softmax层</span></div><div class="line"><span class="comment">**       net 整个网络</span></div><div class="line"><span class="comment">** 说明：softmax层的前向比较简单，只需要对输入中的每个元素做softmax处理就可以，</span></div><div class="line"><span class="comment">**      但是darknet的实现引入了softmax_tree，这个参数的用法尚需要去推敲。</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">forward_softmax_layer</span><span class="params">(<span class="keyword">const</span> softmax_layer l, network net)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="keyword">if</span>(l.softmax_tree)&#123;</div><div class="line">        <span class="keyword">int</span> i;</div><div class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</div><div class="line">        <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; l.softmax_tree-&gt;groups; ++i) &#123;</div><div class="line">            <span class="keyword">int</span> group_size = l.softmax_tree-&gt;group_size[i];</div><div class="line">            softmax_cpu(net.input + count, group_size, l.batch, l.inputs, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, l.temperature, l.output + count);</div><div class="line">            count += group_size;</div><div class="line">        &#125;</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// 调用softmax_cpu()对输入的每一个元素进行softmax处理</span></div><div class="line">        softmax_cpu(net.input, l.inputs/l.groups, l.batch, l.inputs, l.groups, </div><div class="line">        l.inputs/l.groups, <span class="number">1</span>, l.temperature, l.output);</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 输入： input   一组输入图片数据（含义见下面softmax_cpu()注释，下同）</span></div><div class="line"><span class="comment">**       n       一组输入数据中含有的元素个数n=l.inputs/l.groups</span></div><div class="line"><span class="comment">**       temp    温度参数，关于softmax的温度参数，可以搜索一下softmax with temperature，应该会有很多的</span></div><div class="line"><span class="comment">**       stride  跨度</span></div><div class="line"><span class="comment">**       output  这一组输入图片数据对应的输出（也即l.output中与这一组输入对应的某一部分）</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">** 说明：本函数实现的就是标准的softmax函数处理，唯一有点变化的就是在做指数运算之前，</span></div><div class="line"><span class="comment">将每个输入元素减去了该组输入元素中的最大值，以增加数值稳定性，</span></div><div class="line"><span class="comment">**关于此，可以参考博客：</span></div><div class="line"><span class="comment">http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/，</span></div><div class="line"><span class="comment">**这篇博客写的不错，博客中还提到了softmax-loss，此处没有实现（此处实现的也即博客中提到的softmax函数，将softmax-loss分开实现了）。</span></div><div class="line"><span class="comment">*/</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">softmax</span><span class="params">(<span class="keyword">float</span> *input, <span class="keyword">int</span> n, <span class="keyword">float</span> temp, <span class="keyword">int</span> stride, <span class="keyword">float</span> *output)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="keyword">int</span> i;</div><div class="line">    <span class="keyword">float</span> sum = <span class="number">0</span>;</div><div class="line">    <span class="comment">// 赋初始最大值为float中的最小值-FLT_MAX（定义在float.h中）</span></div><div class="line">    <span class="keyword">float</span> largest = -FLT_MAX;</div><div class="line">    <span class="comment">// 寻找输入中的最大值，至于为什么要找出最大值，是为了数值计算上的稳定，详细请戳：</span></div><div class="line">    <span class="comment">//http://freemind.pluskid.org/machine-learning/softmax-vs-softmax-loss-numerical-stability/</span></div><div class="line">    <span class="comment">// 这篇博客写的不错，博客在接近尾声的时候，提到了为什么要减去输入中的最大值。</span></div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</div><div class="line">        <span class="keyword">if</span>(input[i*stride] &gt; largest) largest = input[i*stride];</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</div><div class="line"> <span class="comment">// 在进行指数运算之间，如上面博客所说，首先减去最大值（当然温度参数也要除）</span></div><div class="line">        <span class="keyword">float</span> e = <span class="built_in">exp</span>(input[i*stride]/temp - largest/temp);</div><div class="line">        sum += e;                       <span class="comment">// 求和</span></div><div class="line">        output[i*stride] = e;          </div><div class="line">        <span class="comment">// 并将每一个输入的结果保存在相应的输出中</span></div><div class="line">    &#125;</div><div class="line">    <span class="comment">// 最后一步：归一化转换为概率（就是softmax函数的原型～），最后的输出结果保存在output中</span></div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</div><div class="line">        output[i*stride] /= sum;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="2-2-反向传播函数"><a href="#2-2-反向传播函数" class="headerlink" title="2.2 反向传播函数"></a>2.2 反向传播函数</h3><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">/<span class="strong">*</span></div><div class="line"><span class="strong">*</span>* softmax层反向传播函数</div><div class="line"><span class="bullet">** </span>输入： l   当前softmax层</div><div class="line"><span class="bullet">**       </span>net  整个网络</div><div class="line"><span class="bullet">** </span>说明：</div><div class="line"><span class="comment">//      下面注释理解应该有错，另外，对此处softmax反向的实现存在很大的疑问？？？？？？？</span></div><div class="line"><span class="bullet">**      </span>softmax层的反向很简单，由于自身没有训练参数，虽然有激活函数（即softmax函数），</div><div class="line"><span class="bullet">**      </span>但是又因其所处位置特殊，一般处于网络导数第二层，下一层就是cost层，</div><div class="line"><span class="bullet">**      </span>其自身的敏感度图l.delta已经计算完成（如果此处不太明白再说什么，可以参看卷积层的注释，</div><div class="line"><span class="bullet">**      </span>（完成大部分计算，或者全连接层的注释，以及最大池化层的注释），</div><div class="line"><span class="comment">//</span></div><div class="line"><span class="bullet">**      </span>剩下要做的仅剩下利用自身的l.delta计算其上一层的敏感度图</div><div class="line"><span class="bullet">**      </span>还差乘以上一层激活函数关于其加权输入的导数值），</div><div class="line"><span class="bullet">**      </span>即将l.delta中的元素乘以对应的当前层与上一次层之间的权重，</div><div class="line"><span class="bullet">**      </span>而softmax层与上一层输出之间没有权重或者说权重都为1（因为是将输入直接送进softmax函数处理的，</div><div class="line"><span class="bullet">**      </span>并没有加权什么的），且softmax的输出与上一层的输出存在一一对应的关系，</div><div class="line"><span class="bullet">**      </span>所以求取上一层的敏感度图也是很简单，很直接，详见下面的注释。</div><div class="line"><span class="strong">*/</span></div><div class="line"><span class="strong">void backward_softmax_layer(const softmax_layer l, network net)</span></div><div class="line"><span class="strong">&#123;</span></div><div class="line"><span class="strong">    // 由当前softmax层的敏感度图l.delta计算上一层的敏感度图net.delta，调用的函数为axpy_cpu()，</span></div><div class="line"><span class="strong">    // 为什么调用axpy_cpu()函数，因为softmax层的输出元素与上一层的输出元素存在一一对应的关系</span></div><div class="line"><span class="strong">    //（由此可以看出softmax的stride取值为1是必然的，</span></div><div class="line"><span class="strong">    // 再次照应blas.c中softmax_cpu()的注释，如果不为1,肯定不能是一一对应关系），</span></div><div class="line"><span class="strong">    //所以由softmax层的敏感度图计算上一层的敏感度图，</span></div><div class="line"><span class="strong">    // 可以逐个逐个元素计算，不需要类似全连接层中的矩阵乘法，更没有卷积层中的那般复杂。</span></div><div class="line"><span class="strong">    axpy_cpu(l.inputs*</span>l.batch, 1, l.delta, 1, net.delta, 1);</div><div class="line">&#125;</div><div class="line"></div><div class="line">/<span class="strong">*</span></div><div class="line"><span class="strong">*</span>* axpy是线性代数中一种基本操作，完成y= alpha*x + y操作，其中x,y为矢量，alpha为实数系数</div><div class="line"><span class="bullet">** </span>可以参考：</div><div class="line"></div><div class="line"><span class="bullet">** </span>输入：  N       X中包含的有效元素个数</div><div class="line"><span class="bullet">**        </span>ALPHA   系数alpha</div><div class="line"><span class="bullet">**        </span>X       参与运算的矢量X</div><div class="line"><span class="bullet">**        </span>INCX    步长（倍数步长），即X中凡是INCX的倍数编号参与运算</div><div class="line"><span class="bullet">**        </span>Y       参与运算的矢量，也相当于是输出</div><div class="line"><span class="strong">*/</span></div><div class="line"><span class="strong">void axpy_cpu(int N, float ALPHA, float *</span>X, int INCX, float <span class="strong">*Y, int INCY)</span></div><div class="line"><span class="strong">&#123;</span></div><div class="line"><span class="strong">    int i;</span></div><div class="line"><span class="strong">    for(i = 0; i &lt; N; ++i) Y[i*</span>INCY] += ALPHA*X[i*INCX];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="3-全连接层"><a href="#3-全连接层" class="headerlink" title="3. 全连接层"></a>3. 全连接层</h2><h3 id="3-1-全连接层前向传播函数"><a href="#3-1-全连接层前向传播函数" class="headerlink" title="3.1 全连接层前向传播函数"></a>3.1 全连接层前向传播函数</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 全连接层前向传播函数</span></div><div class="line"><span class="comment">** 输入： l     当前全连接层</span></div><div class="line"><span class="comment">**       net   整个网络</span></div><div class="line"><span class="comment">** 流程： 全连接层的前向传播相对简单，首先初始化输出l.output全为0,在进行相关参数赋值之后，直接调用gemm_nt()完成Wx操作，</span></div><div class="line"><span class="comment">**       而后根据判断是否需要BN，如果需要，则进行BN操作，完了之后为每一个输出元素添加偏置得到Wx+b，最后使用激活函数处理</span></div><div class="line"><span class="comment">**       每一个输出元素，得到f(Wx+b)</span></div><div class="line"><span class="comment">*/</span></div><div class="line">void forward_connected_layer(connected_layer l, network net)</div><div class="line">&#123;</div><div class="line">    int i;</div><div class="line">    <span class="comment">// 初始化全连接层的所有输出（包含所有batch）为0值</span></div><div class="line">    fill_cpu(l.outputs*l.batch, <span class="number">0</span>, l.output, <span class="number">1</span>);</div><div class="line"></div><div class="line">    <span class="comment">// m：全连接层接收的一个batch的图片张数</span></div><div class="line">    <span class="comment">// k：全连接层单张输入图片元素个数</span></div><div class="line">    <span class="comment">// n：全连接层对应单张输入图片的输出元素个数</span></div><div class="line">    int m = l.batch;</div><div class="line">    int k = l.inputs;</div><div class="line">    int n = l.outputs;</div><div class="line"></div><div class="line">    <span class="type">float</span> *a = net.input;</div><div class="line">    <span class="type">float</span> *b = l.weights;</div><div class="line">    <span class="type">float</span> *c = l.output;</div><div class="line"></div><div class="line">    <span class="comment">// a：全连接层的输入数据，维度为l.batch*l.inputs（包含整个batch的输入），可视作l.batch行，l.inputs列，每行就是一张输入图片</span></div><div class="line">    <span class="comment">// b：全连接层的所有权重，维度为l.outputs*l.inputs(见make_connected_layer())</span></div><div class="line">    <span class="comment">// c：全连接层的所有输出（包含所有batch），维度为l.batch*l.outputs（包含整个batch的输出）</span></div><div class="line">    <span class="comment">// 根据维度匹配规则，显然需要对b进行转置，故而调用gemm_nt()函数，最终计算得到的c的维度为l.batch*l.outputs,</span></div><div class="line">    <span class="comment">// 全连接层的的输出很好计算，直接矩阵相承就可以了，所谓全连接，就是全连接层的输出与输入的每一个元素都有关联（当然是同一张图片内的，</span></div><div class="line">    <span class="comment">// 最中得到的c有l.batch行,l.outputs列，每行就是一张输入图片对应的输出）</span></div><div class="line">    <span class="comment">// m：a的行，值为l.batch，含义为全连接层接收的一个batch的图片张数</span></div><div class="line">    <span class="comment">// n：b'的列数，值为l.outputs，含义为全连接层对应单张输入图片的输出元素个数</span></div><div class="line">    <span class="comment">// k：a的列数，值为l.inputs，含义为全连接层单张输入图片元素个数</span></div><div class="line">    gemm(<span class="number">0</span>,<span class="number">1</span>,m,n,k,<span class="number">1</span>,a,k,b,k,<span class="number">1</span>,c,n);</div><div class="line"></div><div class="line">    if(l.batch_normalize)&#123;</div><div class="line">        if(net.train)&#123;</div><div class="line">            <span class="comment">// 计算全连接层l.output中每个元素的的均值，得到的l.mean是一个维度为l.outputs的矢量，</span></div><div class="line">            <span class="comment">// 也即全连接层每一个输出元素都有一个平均值（有batch张输入图片，需要计算这batch图片对应输出元素的平均值），</span></div><div class="line">            <span class="comment">// 对全连接层而言，每个输出就是一个通道，且每张特征图的维度为1*1</span></div><div class="line">            mean_cpu(l.output, l.batch, l.outputs, <span class="number">1</span>, l.mean);</div><div class="line">            <span class="comment">// 计算全连接层每个输出元素的方差l,variance，其维度与l.mean一样</span></div><div class="line">            variance_cpu(l.output, l.mean, l.batch, l.outputs, <span class="number">1</span>, l.variance);</div><div class="line"></div><div class="line">            scal_cpu(l.outputs, <span class="number">.95</span>, l.rolling_mean, <span class="number">1</span>);</div><div class="line">            axpy_cpu(l.outputs, <span class="number">.05</span>, l.mean, <span class="number">1</span>, l.rolling_mean, <span class="number">1</span>);</div><div class="line">            scal_cpu(l.outputs, <span class="number">.95</span>, l.rolling_variance, <span class="number">1</span>);</div><div class="line">            axpy_cpu(l.outputs, <span class="number">.05</span>, l.variance, <span class="number">1</span>, l.rolling_variance, <span class="number">1</span>);</div><div class="line"></div><div class="line">            copy_cpu(l.outputs*l.batch, l.output, <span class="number">1</span>, l.x, <span class="number">1</span>);</div><div class="line">            normalize_cpu(l.output, l.mean, l.variance, l.batch, l.outputs, <span class="number">1</span>);   </div><div class="line">            copy_cpu(l.outputs*l.batch, l.output, <span class="number">1</span>, l.x_norm, <span class="number">1</span>);</div><div class="line">        &#125; else &#123;</div><div class="line">            normalize_cpu(l.output, l.rolling_mean, l.rolling_variance, l.batch, l.outputs, <span class="number">1</span>);</div><div class="line">        &#125;</div><div class="line">        scale_bias(l.output, l.scales, l.batch, l.outputs, <span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// 前面得到的是全连接层每个输出元素的加权输入Wx，下面这个循环就是为每个元素加上偏置，最终得到每个输出元素上的加权输入：Wx+b</span></div><div class="line">    <span class="comment">// 循环次数为l.batch，不是l.outputs，是因为对于全连接层来说，l.batch = l.outputs，无所谓了～</span></div><div class="line">    for(i = <span class="number">0</span>; i &lt; l.batch; ++i)&#123;</div><div class="line">        <span class="comment">// axpy_cpu()完成l.output + i*l.outputs = l.biases + (l.output + i*l.outputs)操作</span></div><div class="line">        <span class="comment">// l.biases的维度为l.outputs;l.output的维度为l.batch*l.outputs，包含整个batch的输出，所以需要注意移位</span></div><div class="line">        axpy_cpu(l.outputs, <span class="number">1</span>, l.biases, <span class="number">1</span>, l.output + i*l.outputs, <span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="comment">// 前向传播最后一步：前面得到每一个输出元素的加权输入Wx+b,这一步利用激活函数处理l.output中的每一个输出元素，</span></div><div class="line">    <span class="comment">// 最终得到全连接层的输出f(Wx+b)</span></div><div class="line">    activate_array(l.output, l.outputs*l.batch, l.activation);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="3-2-全连接层反向传播函数"><a href="#3-2-全连接层反向传播函数" class="headerlink" title="3.2 全连接层反向传播函数"></a>3.2 全连接层反向传播函数</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 全连接层反向传播函数</span></div><div class="line"><span class="comment">** 输入： l     当前全连接层</span></div><div class="line"><span class="comment">**       net   整个网络</span></div><div class="line"><span class="comment">** 流程：先完成之前为完成的计算：计算当前层的敏感度图l.delta（注意是反向传播），</span></div><div class="line"><span class="comment">//而后调用axpy_cpu()函数计算当前全连接层的偏置更新值（基于完全计算完的l.delta），</span></div><div class="line"><span class="comment">**      然后判断是否进行BN，如果进行，则完成BN操作，再接着计算当前层权重更新值，最后计算上一层的敏感度图（完成大部分计算）。</span></div><div class="line"><span class="comment">//相比于卷积神经网络，全连接层很多的计算变得更为直接，不需要调用诸如im2col_cpu()或者col2im_cpu()函数</span></div><div class="line"><span class="comment">//对数据重排来重排去，直接矩阵相乘就可以搞定。</span></div><div class="line"><span class="comment">*/</span></div><div class="line">void backward_connected_layer(connected_layer l, network net)</div><div class="line">&#123;</div><div class="line">    int i;</div><div class="line">    <span class="comment">// 完成当前层敏感度图的计算：当前全连接层下一层不管是什么类型的网络，都会完成当前层敏感度图的绝大部分计算（上一层敏感度乘以上一层与当前层之间的权重）</span></div><div class="line">    <span class="comment">// （注意是反向传播），此处只需要再将l.delta中的每一个元素乘以激活函数对加权输入的导数即可</span></div><div class="line">    <span class="comment">// gradient_array()函数完成激活函数对加权输入的导数，并乘以之前得到的l.delta，得到当前层最终的l.delta（误差函数对加权输入的导数）</span></div><div class="line">    gradient_array(l.output, l.outputs*l.batch, l.activation, l.delta);</div><div class="line"></div><div class="line">    <span class="comment">// 计算当前全连接层的偏置更新值</span></div><div class="line">    <span class="comment">// 相比于卷积层的偏置更新值，此处更为简单（卷积层中有专门的偏置更新值计算函数，主要原因是卷积核在图像上做卷积即权值共享增加了复杂度，</span></div><div class="line">    <span class="comment">//而全连接层没有权值共享），只需调用axpy_cpu()函数就可以完成。误差函数对偏置的导数实际就等于以上刚求完的敏感度值，</span></div><div class="line">    <span class="comment">//因为有多张图片，需要将多张图片的效果叠加，故而循环调用axpy_cpu()函数，</span></div><div class="line">    <span class="comment">// 不同于卷积层每个卷积核才有一个偏置参数，全连接层是每个输出元素就对应有一个偏置参数，共有l.outputs个，</span></div><div class="line">    <span class="comment">//每次循环将求完一张图片所有输出的偏置更新值。</span></div><div class="line">    <span class="comment">// l.bias_updates虽然没有明显的初始化操作，但其在make_connected_layer()中是用calloc()动态分配内存的，</span></div><div class="line">    <span class="comment">//因此其已经全部初始化为0值。</span></div><div class="line">    <span class="comment">// 循环结束后，最终会把每一张图的偏置更新值叠加，因此，最终l.bias_updates中每一个元素的值是batch中</span></div><div class="line">    <span class="comment">//所有图片对应输出元素偏置更新值的叠加。</span></div><div class="line">    for(i = <span class="number">0</span>; i &lt; l.batch; ++i)&#123;</div><div class="line">        axpy_cpu(l.outputs, <span class="number">1</span>, l.delta + i*l.outputs, <span class="number">1</span>, l.bias_updates, <span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">    if(l.batch_normalize)&#123;</div><div class="line">        backward_scale_cpu(l.x_norm, l.delta, l.batch, l.outputs, <span class="number">1</span>, l.scale_updates);</div><div class="line"></div><div class="line">        scale_bias(l.delta, l.scales, l.batch, l.outputs, <span class="number">1</span>);</div><div class="line"></div><div class="line">        mean_delta_cpu(l.delta, l.variance, l.batch, l.outputs, <span class="number">1</span>, l.mean_delta);</div><div class="line">        variance_delta_cpu(l.x, l.delta, l.mean, l.variance, l.batch, l.outputs, <span class="number">1</span>, l.variance_delta);</div><div class="line">        normalize_delta_cpu(l.x, l.mean, l.variance, l.mean_delta, l.variance_delta, l.batch, l.outputs, <span class="number">1</span>, l.delta);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 计算当前全连接层的权重更新值</span></div><div class="line">    int m = l.outputs;</div><div class="line">    int k = l.batch;</div><div class="line">    int n = l.inputs;</div><div class="line">    <span class="type">float</span> *a = l.delta;</div><div class="line">    <span class="type">float</span> *b = net.input;</div><div class="line">    <span class="type">float</span> *c = l.weight_updates;</div><div class="line"></div><div class="line">    <span class="comment">// a：当前全连接层敏感度图，维度为l.batch*l.outputs</span></div><div class="line">    <span class="comment">// b：当前全连接层所有输入，维度为l.batch*l.inputs</span></div><div class="line">    <span class="comment">// c：当前全连接层权重更新值，维度为l.outputs*l.inputs（权重个数）</span></div><div class="line">    <span class="comment">// 由行列匹配规则可知，需要将a转置，故而调用gemm_tn()函数，转置a实际上是想把batch中所有图片的影响叠加。</span></div><div class="line">    <span class="comment">// 全连接层的权重更新值的计算也相对简单，简单的矩阵乘法即可完成：当前全连接层的敏感度图乘以当前层的输入即可得到当前全连接层的权重更新值，</span></div><div class="line">    <span class="comment">// （当前层的敏感度是误差函数对于加权输入的导数，所以再乘以对应输入值即可得到权重更新值）</span></div><div class="line">    <span class="comment">// m：a'的行，值为l.outputs，含义为每张图片输出的元素个数</span></div><div class="line">    <span class="comment">// n：b的列数，值为l.inputs，含义为每张输入图片的元素个数</span></div><div class="line">    <span class="comment">// k：a’的列数，值为l.batch，含义为一个batch中含有的图片张数</span></div><div class="line">    <span class="comment">// 最终得到的c维度为l.outputs*l.inputs，对应所有权重的更新值</span></div><div class="line">    gemm(<span class="number">1</span>,<span class="number">0</span>,m,n,k,<span class="number">1</span>,a,m,b,n,<span class="number">1</span>,c,n);</div><div class="line"></div><div class="line">    <span class="comment">// 由当前全连接层计算上一层的敏感度图（完成绝大部分计算：当前全连接层敏感度图乘以当前层还未更新的权重）</span></div><div class="line">    m = l.batch;</div><div class="line">    k = l.outputs;</div><div class="line">    n = l.inputs;</div><div class="line"></div><div class="line">    a = l.delta;</div><div class="line">    b = l.weights;</div><div class="line">    c = net.delta;</div><div class="line"></div><div class="line">    <span class="comment">// 一定注意此时的c等于net.delta，已经在network.c中的backward_network()函数中赋值为上一层的delta</span></div><div class="line">    <span class="comment">// a：当前全连接层敏感度图，维度为l.batch*l.outputs</span></div><div class="line">    <span class="comment">// b：当前层权重（连接当前层与上一层），维度为l.outputs*l.inputs</span></div><div class="line">    <span class="comment">// c：上一层敏感度图（包含整个batch），维度为l.batch*l.inputs</span></div><div class="line">    <span class="comment">// 由行列匹配规则可知，不需要转置。由全连接层敏感度图计算上一层的敏感度图也很简单，直接利用矩阵相乘，</span></div><div class="line">    <span class="comment">//将当前层l.delta与当前层权重相乘就可以了，只需要注意要不要转置，拿捏好就可以，不需要像卷积层一样，需要对权重或者输入重排！</span></div><div class="line">    <span class="comment">// m：a的行，值为l.batch，含义为一个batch中含有的图片张数</span></div><div class="line">    <span class="comment">// n：b的列数，值为l.inputs，含义为每张输入图片的元素个数</span></div><div class="line">    <span class="comment">// k：a的列数，值为l.outputs，含义为每张图片输出的元素个数</span></div><div class="line">    <span class="comment">// 最终得到的c维度为l.bacth*l.inputs（包含所有batch）</span></div><div class="line">    if(c) gemm(<span class="number">0</span>,<span class="number">0</span>,m,n,k,<span class="number">1</span>,a,k,b,n,<span class="number">1</span>,c,n);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="4-卷积层"><a href="#4-卷积层" class="headerlink" title="4. 卷积层"></a>4. 卷积层</h2><h3 id="4-1-卷积运算的加速实现"><a href="#4-1-卷积运算的加速实现" class="headerlink" title="4.1  卷积运算的加速实现"></a>4.1  卷积运算的加速实现</h3><p>将图像平铺成一行，每一段均可以直接与卷积核相乘，根据Stride大小可能有重复元素，因此元素总个数也可能变多，这样做是为了更方便快捷的并行进行卷积运算！<br><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">**  从输入的多通道数组im（存储图像数据）中获取指定行、列、、通道数处的元素值</span></div><div class="line"><span class="comment">**  输入： im      输入，所有数据存成一个一维数组，例如对于3通道的二维图像而言，</span></div><div class="line"><span class="comment">**                每一通道按行存储（每一通道所有行并成一行），三通道依次再并成一行</span></div><div class="line"><span class="comment">**        height  每一通道的高度（即输入图像的真正的高度，补0之前）</span></div><div class="line"><span class="comment">**        width   每一通道的宽度（即输入图像的宽度，补0之前）</span></div><div class="line"><span class="comment">**        channels 输入im的通道数，比如彩色图为3通道，之后每一卷积层的输入的通道数等于上一卷积层卷积核的个数</span></div><div class="line"><span class="comment">**        row     要提取的元素所在的行（二维图像补0之后的行数）</span></div><div class="line"><span class="comment">**        col     要提取的元素所在的列（二维图像补0之后的列数）</span></div><div class="line"><span class="comment">**        channel 要提取的元素所在的通道</span></div><div class="line"><span class="comment">**        pad     图像左右上下各补0的长度（四边补0的长度一样）</span></div><div class="line"><span class="comment">**  返回： float类型数据，为im中channel通道，row-pad行，col-pad列处的元素值</span></div><div class="line"><span class="comment">而row与col则是补0之后，元素所在的行列，因此，要准确获取在im中的元素值，首先需要减去pad以获取在im中真实的行列数</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="keyword">float</span> im2col_get_pixel(<span class="keyword">float</span> *im, <span class="keyword">int</span> <span class="built_in">height</span>, <span class="keyword">int</span> <span class="built_in">width</span>, <span class="keyword">int</span> channels,<span class="keyword">int</span> row, <span class="keyword">int</span> col, <span class="keyword">int</span> channel, <span class="keyword">int</span> pad)</div><div class="line">&#123;</div><div class="line">    <span class="comment">// 减去补0长度，获取元素真实的行列数</span></div><div class="line">    row -= pad;</div><div class="line">    col -= pad;</div><div class="line"></div><div class="line">    <span class="comment">// 如果行列数小于0,则返回0（刚好是补0的效果）</span></div><div class="line">    <span class="built_in">if</span> (row &lt; <span class="number">0</span> || col &lt; <span class="number">0</span> ||</div><div class="line">        row &gt;= <span class="built_in">height</span> || col &gt;= <span class="built_in">width</span>) <span class="built_in">return</span> <span class="number">0</span>;</div><div class="line">    </div><div class="line">    <span class="comment">// im存储多通道二维图像的数据的格式为：各通道所有行并成一行，再多通道依次并成一行，</span></div><div class="line">    <span class="comment">// 因此width*height*channel首先移位到所在通道的起点位置，加上width*row移位到所在指定通道所在行，再加上col移位到所在列</span></div><div class="line">    <span class="built_in">return</span> im[col + <span class="built_in">width</span>*(row + <span class="built_in">height</span>*channel)];</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">//From Berkeley Vision's Caffe!</span></div><div class="line"><span class="comment">//https://github.com/BVLC/caffe/blob/master/LICENSE</span></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 将输入图片转为便于计算的数组格式，可以参考https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/</span></div><div class="line"><span class="comment">** 进行辅助理解（但执行方式并不同，只是用于概念上的辅助理解），由作者的注释可知，这是直接从caffe移植过来的</span></div><div class="line"><span class="comment">** 输入： data_im    输入图像</span></div><div class="line"><span class="comment">**       channels   输入图像的通道数（对于第一层，一般是颜色图，3通道，中间层通道数为上一层卷积核个数）</span></div><div class="line"><span class="comment">**       height     输入图像的高度（行）</span></div><div class="line"><span class="comment">**       width      输入图像的宽度（列）</span></div><div class="line"><span class="comment">**       ksize      卷积核尺寸</span></div><div class="line"><span class="comment">**       stride     卷积核跨度</span></div><div class="line"><span class="comment">**       pad        四周补0长度</span></div><div class="line"><span class="comment">**       data_col   相当于输出，为进行格式重排后的输入图像数据</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">** 说明：1）此函数个人感觉在实现上存在不足，传入参数没有必要这么多，只需传入当前卷积层的指针即可，这样函数中的一些代码就会变的多余</span></div><div class="line"><span class="comment">**      2）输出data_col的元素个数与data_im元素个数不相等，一般比data_im的元素个数多，因为stride较小，</span></div><div class="line"><span class="comment">各个卷积核之间有很多重叠，实际data_col中的元素个数为channels*ksize*ksize*height_col* width_col，</span></div><div class="line"><span class="comment">其中channels为data_im的通道数，ksize为卷积核大小，height_col和width_col如下所注。data_col的还是按行排列，只是行数为</span></div><div class="line"><span class="comment">channels*ksize*ksize,列数为height_col*width_col，即一张特征图总的元素个数，每整列包含与某个位置处的卷积核计算的所有通道上的像素</span></div><div class="line"><span class="comment">（比如输入图像通道数为3,卷积核尺寸为3*3，则共有27行，每列有27个元素），不同列对应卷积核在图像上的不同位置做卷积</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="keyword">void</span> im2col_cpu(<span class="keyword">float</span>* data_im,</div><div class="line">     <span class="keyword">int</span> channels,  <span class="keyword">int</span> <span class="built_in">height</span>,  <span class="keyword">int</span> <span class="built_in">width</span>,</div><div class="line">     <span class="keyword">int</span> ksize,  <span class="keyword">int</span> stride, <span class="keyword">int</span> pad, <span class="keyword">float</span>* data_col) </div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> c,h,w;</div><div class="line">    <span class="comment">// 计算该层神经网络的输出图像尺寸（其实没有必要再次计算的，因为在构建卷积层时，make_convolutional_layer()函数已经调用</span></div><div class="line"><span class="comment">//函数参数只要传入该层网络指针就可了，没必要弄这么多参数）</span></div><div class="line">    <span class="keyword">int</span> height_col = (<span class="built_in">height</span> + <span class="number">2</span>*pad - ksize) / stride + <span class="number">1</span>;</div><div class="line">    <span class="keyword">int</span> width_col = (<span class="built_in">width</span> + <span class="number">2</span>*pad - ksize) / stride + <span class="number">1</span>;</div><div class="line"></div><div class="line">    <span class="comment">// 卷积核大小：ksize*ksize是一个卷积核的大小，之所以乘以通道数channels，是因为输入图像有多通道，</span></div><div class="line">    <span class="comment">//每个卷积核在做卷积时，是同时对同一位置处多通道的图像进行卷积运算，这里为了实现这一目的，将三通道上的卷积核并在一起以便进行计算，</span></div><div class="line">    <span class="comment">//实际就是同一个卷积核的复制，比如对于3通道图像，卷积核尺寸为3*3，该卷积核将同时作用于三通道图像上，</span></div><div class="line">    <span class="comment">//这样并起来就得到含有27个元素的卷积核,能不能使得作用在不同通道上的卷积核有不同参数呢？</span></div><div class="line">    <span class="comment">//不知道有没有这样的做法？可以思考下，当然这样做肯定会是参数剧增！！</span></div><div class="line">    <span class="keyword">int</span> channels_col = channels * ksize * ksize;</div><div class="line">    </div><div class="line">    <span class="comment">// 这三层循环之间的逻辑关系，决定了输入图像重排后的格式，更为详细/形象的说明可参考博客</span></div><div class="line">    <span class="comment">// 外循环次数为一个卷积核的尺寸数，循环次数即为最终得到的data_col的总行数</span></div><div class="line">    <span class="built_in">for</span> (c = <span class="number">0</span>; c &lt; channels_col; ++c) &#123;</div><div class="line">        <span class="comment">// 列偏移，卷积核是一个二维矩阵，并按行存储在一维数组中，利用求余运算获取对应在卷积核中的列数，</span></div><div class="line">        <span class="comment">//比如对于3*3的卷积核（3通道），当c=0时，显然在第一列，当c=5时，显然在第2列，</span></div><div class="line">        <span class="comment">//当c=9时，在第二通道上的卷积核的第一列，当c=26时，在第三列（第三通道上）</span></div><div class="line">        <span class="keyword">int</span> w_offset = c % ksize;</div><div class="line">        <span class="comment">// 行偏移，卷积核是一个二维的矩阵，且是按行（卷积核所有行并成一行）存储在一维数组中的，</span></div><div class="line">        <span class="comment">// 比如对于3*3的卷积核，处理3通道的图像，那么一个卷积核具有27个元素，每9个元素对应一个通道上的卷积核（互为一样），</span></div><div class="line">        <span class="comment">// 每当c为3的倍数，就意味着卷积核换了一行，h_offset取值为0,1,2，对应3*3卷积核中的第1, 2, 3行</span></div><div class="line">        <span class="keyword">int</span> h_offset = (c / ksize) % ksize;</div><div class="line">        <span class="comment">// 通道偏移，channels_col是多通道的卷积核并在一起的，比如对于3通道，3*3卷积核，每过9个元素就要换一通道数，</span></div><div class="line">        <span class="comment">// 当c=0~8时，c_im=0;c=9~17时，c_im=1;c=18~26时，c_im=2</span></div><div class="line">        <span class="keyword">int</span> c_im = c / ksize / ksize;</div><div class="line"></div><div class="line">        <span class="comment">// 中循环次数等于该层输出图像行数height_col，说明data_col中的每一行存储了一张特征图，</span></div><div class="line">        <span class="comment">//这张特征图又是按行存储在data_col中的某行中</span></div><div class="line">        <span class="built_in">for</span> (h = <span class="number">0</span>; h &lt; height_col; ++h) &#123;</div><div class="line">            <span class="comment">// 内循环等于该层输出图像列数width_col，说明最终得到的data_col总有channels_col行，height_col*width_col列</span></div><div class="line">            <span class="built_in">for</span> (w = <span class="number">0</span>; w &lt; width_col; ++w) &#123;</div><div class="line">                <span class="comment">// 由上面可知，对于3*3的卷积核，h_offset取值为0,1,2,当h_offset=0时，会提取出所有与卷积核第一行元素进行运算的像素，</span></div><div class="line">                <span class="comment">// 依次类推；加上h*stride是对卷积核进行行移位操作，比如卷积核从图像(0,0)位置开始做卷积，</span></div><div class="line">                <span class="comment">//那么最先开始涉及(0,0)~(3,3)之间的像素值，若stride=2，那么卷积核进行一次行移位时，</span></div><div class="line">                <span class="comment">//下一行的卷积操作是从元素(2,0)（2为图像行号，0为列号）开始</span></div><div class="line">                <span class="keyword">int</span> im_row = h_offset + h * stride;</div><div class="line">                <span class="comment">// 对于3*3的卷积核，w_offset取值也为0,1,2，当w_offset取1时，会提取出所有与卷积核中第2列元素进行运算的像素，</span></div><div class="line">                <span class="comment">// 实际在做卷积操作时，卷积核对图像逐行扫描做卷积，加上w*stride就是为了做列移位，</span></div><div class="line">                <span class="comment">// 比如前一次卷积其实像素元素为(0,0)，若stride=2,那么下次卷积元素起始像素位置为(0,2)（0为行号，2为列号）</span></div><div class="line">                <span class="keyword">int</span> im_col = w_offset + w * stride;</div><div class="line"></div><div class="line">                <span class="comment">// col_index为重排后图像中的像素索引，等于c * height_col * width_col + h * width_col +w（还是按行存储，所有通道再并成一行），</span></div><div class="line">                <span class="comment">// 对应第c通道，h行，w列的元素</span></div><div class="line">                <span class="keyword">int</span> col_index = (c * height_col + h) * width_col + w;</div><div class="line">                </div><div class="line">                <span class="comment">// im2col_get_pixel函数获取输入图像data_im中第c_im通道，im_row,im_col的像素值并赋值给重排后的图像，</span></div><div class="line">                <span class="comment">// height和width为输入图像data_im的真实高、宽，pad为四周补0的长度（注意im_row,im_col是补0之后的行列号，</span></div><div class="line">                <span class="comment">// 不是真实输入图像中的行列号，因此需要减去pad获取真实的行列号）</span></div><div class="line">                data_col[col_index] = im2col_get_pixel(data_im, <span class="built_in">height</span>, <span class="built_in">width</span>, channels,im_row, im_col, c_im, pad);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="4-2-更新卷积核的偏置"><a href="#4-2-更新卷积核的偏置" class="headerlink" title="4.2 更新卷积核的偏置"></a>4.2 更新卷积核的偏置</h3><figure class="highlight arduino"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 计算每个卷积核的偏置更新值，所谓偏置更新值，就是bias = bias - alpha * bias_update中的bias_update</span></div><div class="line"><span class="comment">** 输入： bias_updates     当前层所有偏置的更新值，维度为l.n（即当前层卷积核的个数）</span></div><div class="line"><span class="comment">**       delta            当前层的敏感度图（即l.delta）</span></div><div class="line"><span class="comment">**       batch            一个batch含有的图片张数（即l.batch）</span></div><div class="line"><span class="comment">**       n                当前层卷积核个数（即l.h）</span></div><div class="line"><span class="comment">**       k                当前层输入特征图尺寸（即l.out_w*l.out_h）</span></div><div class="line"><span class="comment">** 原理：当前层的敏感度图l.delta是误差函数对加权输入的导数，也就是偏置更新值，只是其中每l.out_w*l.out_h个元素都对应同一个</span></div><div class="line"><span class="comment">偏置，因此需要将其加起来，得到的和就是误差函数对当前层各偏置的导数（l.delta的维度为l.batch*l.n*l.out_h*l.out_w,</span></div><div class="line"><span class="comment">可理解成共有l.batch行，每行有l.n*l.out_h*l.out_w列，而这一大行又可以理解成有l.n，l.out_h*l.out_w列，</span></div><div class="line"><span class="comment">这每一小行就对应同一个卷积核也即同一个偏置）</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="keyword">void</span> backward_bias(<span class="keyword">float</span> *bias_updates, <span class="keyword">float</span> *delta, <span class="keyword">int</span> batch, <span class="keyword">int</span> n, <span class="keyword">int</span> <span class="built_in">size</span>)</div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> i,b;</div><div class="line">    <span class="comment">// 遍历batch中每张输入图片</span></div><div class="line">    <span class="comment">// 注意，最后的偏置更新值是所有输入图片的总和（多张图片无非就是重复一张图片的操作，求和即可）。</span></div><div class="line">    <span class="comment">// 总之：一个卷积核对应一个偏置更新值，该偏置更新值等于batch中所有输入图片累积的偏置更新值，</span></div><div class="line">    <span class="comment">// 而每张图片也需要进行偏置更新值求和（因为每个卷积核在每张图片多个位置做了卷积运算，这都对偏置更新值有贡献）</span></div><div class="line">    <span class="comment">//以得到每张图片的总偏置更新值。</span></div><div class="line">    <span class="built_in">for</span>(b = <span class="number">0</span>; b &lt; batch; ++b)&#123;</div><div class="line">        <span class="comment">// 求和得一张输入图片的总偏置更新值</span></div><div class="line">        <span class="built_in">for</span>(i = <span class="number">0</span>; i &lt; n; ++i)&#123;</div><div class="line">            bias_updates[i] += sum_array(delta+<span class="built_in">size</span>*(i+b*n), <span class="built_in">size</span>);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 将以a为首地址此后n个元素相加，返回总和</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="keyword">float</span> sum_array(<span class="keyword">float</span> *a, <span class="keyword">int</span> n)</div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> i;</div><div class="line">    <span class="keyword">float</span> sum = <span class="number">0</span>;</div><div class="line">    <span class="built_in">for</span>(i = <span class="number">0</span>; i &lt; n; ++i) sum += a[i];</div><div class="line">    <span class="built_in">return</span> sum;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="4-3-卷积层前向传播函数"><a href="#4-3-卷积层前向传播函数" class="headerlink" title="4.3 卷积层前向传播函数"></a>4.3 卷积层前向传播函数</h3><figure class="highlight processing"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">void</span> forward_convolutional_layer(convolutional_layer l, network net)</div><div class="line">&#123;</div><div class="line">    <span class="built_in">int</span> out_h = l.out_h;</div><div class="line">    <span class="built_in">int</span> out_w = l.out_w;</div><div class="line">    <span class="built_in">int</span> i;</div><div class="line">    <span class="comment">/*l.outputs = l.out_h * l.out_w * l.out_c在make各网络层函数中赋值（比如make_convolutional_layer()），</span></div><div class="line"><span class="comment">    对应每张输入图片的所有输出特征图的总元素个数（每张输入图片会得到n也即l.out_c张特征图）</span></div><div class="line"><span class="comment">    初始化输出l.output全为0.0；输入l.outputs*l.batch为输出的总元素个数，其中l.outputs为batch</span></div><div class="line"><span class="comment">    中一个输入对应的输出的所有元素的个数，l.batch为一个batch输入包含的图片张数；0表示初始化所有输出为0；*/</span></div><div class="line">    fill_cpu(l.outputs*l.batch, <span class="number">0</span>, l.output, <span class="number">1</span>);</div><div class="line"></div><div class="line">   <span class="comment">/*是否进行二值化操作（这个操作应该只有第一个卷积层使用吧？因为下面直接对net.input操作，这个理解是错误的，</span></div><div class="line"><span class="comment">   因为在forward_network()含中，每进行一层都会将net.input = l.output，即下一层的输入被设置为当前层的输出）*/</span></div><div class="line">    <span class="keyword">if</span>(l.xnor)&#123;</div><div class="line">        binarize_weights(l.weights, l.n, l.c*l.<span class="built_in">size</span>*l.<span class="built_in">size</span>, l.binary_weights);</div><div class="line">        swap_binary(&amp;l);</div><div class="line">        binarize_cpu(net.input, l.c*l.h*l.w*l.batch, l.binary_input);</div><div class="line">        net.input = l.binary_input;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="built_in">int</span> m = l.n;                <span class="comment">// 该层卷积核个数</span></div><div class="line">    <span class="built_in">int</span> k = l.<span class="built_in">size</span>*l.<span class="built_in">size</span>*l.c;  <span class="comment">// 该层每个卷积核的参数元素个数</span></div><div class="line">    <span class="built_in">int</span> n = out_h*out_w;        <span class="comment">// 该层每个特征图的尺寸（元素个数）</span></div><div class="line"></div><div class="line"></div><div class="line">    <span class="built_in">float</span> *a = l.weights;       <span class="comment">// 所有卷积核（也即权重），元素个数为l.n*l.c*l.size*l.size，按行存储，共有l*n行，l.c*l.size*l.size列</span></div><div class="line">    <span class="built_in">float</span> *b = net.workspace;   <span class="comment">// 对输入图像进行重排之后的图像数据</span></div><div class="line">    <span class="built_in">float</span> *c = l.output;        <span class="comment">// 存储一张输入图片（多通道）所有的输出特征图（输入图片是多通道的，输出图片也是多通道的，有多少个卷积核就有多少个通道，每个卷积核得到一张特征图即为一个通道）</span></div><div class="line"></div><div class="line">    <span class="comment">/*该循环即为卷积计算核心代码：所有卷积核对batch中每张图片进行卷积运算！！</span></div><div class="line"><span class="comment">    可以参考：https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/</span></div><div class="line"><span class="comment">    进行辅助理解（主要是辅助理解，实际执行并不一样）。每次循环处理一张输入图片（所有卷积核对batch中一张图片做卷积运算）*/</span></div><div class="line">    </div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; l.batch; ++i)&#123;</div><div class="line">        <span class="comment">/*将多通道二维图像net.input变成按一定存储规则排列的数组b，以方便、高效地进行矩阵（卷积）计算，</span></div><div class="line"><span class="comment">        详细查看该函数注释（比较复杂).注意net.input包含batch中所有图片的数据，但是每次循环只处理一张</span></div><div class="line"><span class="comment">        （本循环最后一句对net.input进行了移位），因此在im2col_cpu仅会对其中一张图片</span></div><div class="line"><span class="comment">        进行重排，l.c为每张图片的通道数，l.h为每张图片的高度，l.w为每张图片的宽度，l.size为卷积核尺寸，l.stride为跨度</span></div><div class="line"><span class="comment">        得到的b为一张图片重排后的结果，也是按行存储的一维数组（共有l.c*l.size*l.size行，l.out_w*l.out_h列），*/</span></div><div class="line">        im2col_cpu(net.input, l.c, l.h, l.w, </div><div class="line">                l.<span class="built_in">size</span>, l.stride, l.pad, b);</div><div class="line"></div><div class="line">        <span class="comment">/*GEneral Matrix to Matrix Multiplication</span></div><div class="line"><span class="comment">        // 此处在im2col_cpu操作基础上，利用矩阵乘法c=alpha*a*b+beta*c完成对图像卷积的操作</span></div><div class="line"><span class="comment">        // 0,0表示不对输入a,b进行转置，</span></div><div class="line"><span class="comment">        // m是输入a,c的行数，具体含义为每个卷积核的个数，</span></div><div class="line"><span class="comment">        // n是输入b,c的列数，具体含义为每个输出特征图的元素个数(out_h*out_w)，</span></div><div class="line"><span class="comment">        // k是输入a的列数也是b的行数，具体含义为卷积核元素个数乘以输入图像的通道数（l.size*l.size*l.c），</span></div><div class="line"><span class="comment">        // a,b,c即为三个参与运算的矩阵（用一维数组存储）,alpha=beta=1为常系数，</span></div><div class="line"><span class="comment">        // a为所有卷积核集合,元素个数为l.n*l.c*l.size*l.size，按行存储，共有l*n行，l.c*l.size*l.size列，</span></div><div class="line"><span class="comment">        // 即a中每行代表一个可以作用在3通道上的卷积核，</span></div><div class="line"><span class="comment">        // b为一张输入图像经过im2col_cpu重排后的图像数据（共有l.c*l.size*l.size行，l.out_w*l.out_h列），</span></div><div class="line"><span class="comment">        // c为gemm()计算得到的值，包含一张输入图片得到的所有输出特征图（每个卷积核得到一张特征图），c中一行代表一张特征图，</span></div><div class="line"><span class="comment">        // 各特征图铺排开成一行后，再将所有特征图并成一大行，存储在c中，因此c可视作有l.n行，l.out_h*l.out_w列。</span></div><div class="line"><span class="comment">        // 详细查看该函数注释（比较复杂）*/</span></div><div class="line">        gemm(<span class="number">0</span>,<span class="number">0</span>,m,n,k,<span class="number">1</span>,a,k,b,n,<span class="number">1</span>,c,n);</div><div class="line"></div><div class="line">        <span class="comment">/*// 对c进行指针偏移：移到batch中下一张图片对应输出的起始位置（每循环一次，将完成对一张图片的卷积操作，</span></div><div class="line"><span class="comment">        // 产生的所有特征图的元素个数总和为n*m）*/</span></div><div class="line">        c += n*m;</div><div class="line">        <span class="comment">// 同样，输入也进行指针偏移，移动到下一张图片元素的起始位置，以便下一次循环处理</span></div><div class="line">        <span class="comment">// （batch中每张图片的元素个数为通道数*高度*宽度，即l.c*l.h*l.w）</span></div><div class="line">        net.input += l.c*l.h*l.w;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">//如需要规范化（BN在非线性激活函数处理之前完成）</span></div><div class="line">    <span class="keyword">if</span>(l.batch_normalize)&#123;</div><div class="line">        forward_batchnorm_layer(l, net);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        add_bias(l.output, l.biases, l.batch, l.n, out_h*out_w);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    activate_array(l.output, m*n*l.batch, l.activation);</div><div class="line">    <span class="keyword">if</span>(l.<span class="built_in">binary</span> || l.xnor) swap_binary(&amp;l);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="4-4-卷积层反向传播函数"><a href="#4-4-卷积层反向传播函数" class="headerlink" title="4.4 卷积层反向传播函数"></a>4.4 卷积层反向传播函数</h3><figure class="highlight mel"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 卷积神经网络反向传播核心函数</span></div><div class="line"><span class="comment">** 主要流程：</span></div><div class="line"><span class="comment">1） 调用gradient_array()计算当前层l所有输出元素关于加权输入的导数值（也即激活函数关于输入的导数值），</span></div><div class="line"><span class="comment">并乘上上一次调用backward_convolutional_layer()还没计算完的l.delta，得到当前层最终的敏感度图；</span></div><div class="line"><span class="comment">2） 如果网络进行了BN，则需要进行BN的梯度计算；</span></div><div class="line"><span class="comment">3） 如果网络没有进行BN，则直接调用 backward_bias()计算当前层所有卷积核的偏置更新值；</span></div><div class="line"><span class="comment">4） 依次调用im2col_cpu()，gemm_nt()函数计算当前层权重系数更新值；</span></div><div class="line"><span class="comment">5） 如果上一层的delta已经动态分配了内存，则依次调用gemm_tn(), </span></div><div class="line"><span class="comment">col2im_cpu()计算上一层的敏感度图（并未完成所有计算，还差一个步骤）；</span></div><div class="line"><span class="comment"></span></div><div class="line"><span class="comment">** 强调：每次调用本函数会计算完成当前层的敏感度计算，同时计算当前层的偏置、权重更新值，除此之外，还会计算上一层的敏感度图，</span></div><div class="line"><span class="comment">但是要注意的是，并没有完全计算完，还差一步：乘上激活函数对加权输入的导数值。这一步在下一次调用本函数时完成。</span></div><div class="line"><span class="comment">*/</span></div><div class="line"></div><div class="line">void backward_convolutional_layer(convolutional_layer l, network net)</div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> i;</div><div class="line">    <span class="keyword">int</span> m = l.n;                <span class="comment">// 卷积核个数</span></div><div class="line">    <span class="comment">// 每一个卷积核元素个数（包括l.c（l.c为该层网络接受的输入图片的通道数）个通道上的卷积核元素个数总数，比如卷积核尺寸为3*3,</span></div><div class="line">    <span class="comment">// 输入图片有3个通道，因为要同时作用于3个通道上，所以需要额外复制两次这个卷积核，那么一个卷积核共有27个元素）</span></div><div class="line">    <span class="keyword">int</span> n = l.<span class="keyword">size</span>*l.<span class="keyword">size</span>*l.c;</div><div class="line">    <span class="keyword">int</span> k = l.out_w*l.out_h;    <span class="comment">// 每张输出特征图的元素个数：out_w，out_h是输出特征图的宽高</span></div><div class="line"></div><div class="line">    <span class="comment">// 计算当前层激活函数对加权输入的导数值并乘以l.delta相应元素，从而彻底完成当前层敏感度图的计算，得到当前层的敏感度图l.delta。</span></div><div class="line">    <span class="comment">// l.output存储了该层网络的所有输出：该层网络接受一个batch的输入图片，其中每张图片经卷积处理后得到的特征图尺寸为：l.out_w,l.out_h，</span></div><div class="line">    <span class="comment">// 该层卷积网络共有l.n个卷积核，因此一张输入图片共输出l.n张宽高为l.out_w,l.out_h的特征图（l.outputs为一张图所有输出特征图的总元素个数），</span></div><div class="line">    <span class="comment">// 所以所有输入图片也即l.output中的总元素个数为：l.n*l.out_w*l.out_h*l.batch；</span></div><div class="line">    <span class="comment">// l.activation为该卷积层的激活函数类型，l.delta就是gradient_array()函数计算得到的</span></div><div class="line">   <span class="comment">//l.output中每一个元素关于激活函数函数输入的导数值，</span></div><div class="line">    <span class="comment">// 注意，这里直接利用输出值求得激活函数关于输入的导数值是因为神经网络中所使用的绝大部分激活函数关于输入的导数值</span></div><div class="line">    <span class="comment">//都可以描述为输出值的函数表达式，比如对于Sigmoid激活函数（记作f(x)），其导数值为f(x)'=f(x)*(1-f(x)),</span></div><div class="line">    <span class="comment">//因此如果给出y=f(x)，那么f(x)'=y*(1-y)，只需要输出值y就可以了，不需要输入x的值，</span></div><div class="line">    <span class="comment">// （暂时不确定darknet中有没有使用特殊的激活函数，以致于必须要输入值才能够求出导数值，</span></div><div class="line">    <span class="comment">//在activiation.c文件中，有几个激活函数暂时没看懂，也没在网上查到）。</span></div><div class="line">    <span class="comment">// l.delta是一个一维数组，长度为l.batch * l.outputs（其中l.outputs = l.out_h * l.out_w * l.out_c），</span></div><div class="line">    <span class="comment">//在make_convolutional_layer()动态分配内存；</span></div><div class="line">    <span class="comment">// 再强调一次：gradient_array()不单单是完成激活函数对输入的求导运算，还完成计算当前层敏感度图的最后一步：</span></div><div class="line">    <span class="comment">//l.delta中每个元素乘以激活函数对输入的导数（注意gradient_arry中使用的是*=运算符）。</span></div><div class="line">    <span class="comment">// 每次调用backward_convolutional_laye时，都会完成当前层敏感度图的计算，同时会计算上一层的敏感度图，</span></div><div class="line">    <span class="comment">//但对于上一层，其敏感度图并没有完全计算完成，还差一步，</span></div><div class="line">    <span class="comment">// 需要等到下一次调用backward_convolutional_layer()时来完成，诚如col2im_cpu()中注释一样。</span></div><div class="line">    </div><div class="line">    gradient_array(l.output, m*k*l.batch, l.activation, l.delta);</div><div class="line"></div><div class="line">    <span class="keyword">if</span>(l.batch_normalize)&#123;</div><div class="line">        backward_batchnorm_layer(l, net);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// 计算偏置的更新值：每个卷积核都有一个偏置，偏置的更新值也即误差函数对偏置的导数，这个导数的计算很简单，</span></div><div class="line">        <span class="comment">//实际所有的导数已经求完了，都存储在l.delta中，</span></div><div class="line">        <span class="comment">// 接下来只需把l.delta中对应同一个卷积核的项加起来就可以（卷积核在图像上逐行逐列跨步移动做卷积，</span></div><div class="line">        <span class="comment">//每个位置处都有一个输出，共有l.out_w*l.out_h个，</span></div><div class="line">        <span class="comment">// 这些输出都与同一个偏置关联，因此将l.delta中对应同一个卷积核的项加起来即得误差函数对这个偏置的导数）</span></div><div class="line">        backward_bias(l.bias_updates, l.delta, l.batch, l.n, k);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="comment">// 遍历batch中的每张照片，对于l.delta来说，每张照片是分开存的，因此其维度会达到：l.batch*l.n*l.out_w*l.out_h，</span></div><div class="line">    <span class="comment">// 对于l.weights,l.weight_updates以及上面提到的l.bias,l.bias_updates，是将所有照片对应元素叠加起来</span></div><div class="line">    <span class="comment">// （循环的过程就是叠加的过程，注意gemm()这系列函数含有叠加效果，不是覆盖输入C的值，而是叠加到之前的C上），</span></div><div class="line">    <span class="comment">// 因此l.weights与l.weight_updates维度为l.n*l.size*l.size，l.bias与</span></div><div class="line">    <span class="comment">//l.bias_updates的维度为l.h，都与l.batch无关</span></div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; l.batch; ++i)&#123;</div><div class="line">        <span class="keyword">float</span> *a = l.delta + i*m*k;</div><div class="line">        <span class="comment">// net.workspace的元素个数为所有层中最大的l.workspace_size（在make_convolutional_layer()</span></div><div class="line">        <span class="comment">//计算得到workspace_size的大小，在parse_network_cfg()中动态分配内存，此值对应未使用gpu时的情况）,</span></div><div class="line">        <span class="comment">// net.workspace充当一个临时工作空间的作用，存储临时所需要的计算参数，比如每层单张图片重排后的结果</span></div><div class="line">        <span class="comment">//（这些参数马上就会参与卷积运算），一旦用完，就会被马上更新（因此该变量的值的更新频率比较大）</span></div><div class="line">        <span class="keyword">float</span> *b = net.<span class="keyword">workspace</span>;</div><div class="line">        <span class="keyword">float</span> *c = l.weight_updates;</div><div class="line"></div><div class="line">        <span class="comment">// 进入本函数之前，在backward_network()函数中，已经将net.input赋值为prev.output，</span></div><div class="line">        <span class="comment">//也即若当前层为第l层，net.input此时已经是第l-1层的输出</span></div><div class="line">        <span class="keyword">float</span> *im = net.input+i*l.c*l.h*l.w;</div><div class="line"></div><div class="line">        <span class="comment">// 下面两步：im2col_cpu()与gemm()是为了计算当前层的权重更新值（其实也就是误差函数对当前成权重的导数）</span></div><div class="line">        <span class="comment">// 将多通道二维图像net.input变成按一定存储规则排列的数组b，以方便、高效地进行矩阵（卷积）计算，详细查看该函数注释（比较复杂），</span></div><div class="line">        <span class="comment">// im2col_cpu每次仅处理net.input（包含整个batch）中的一张输入图片（对于第一层，则就是读入的图片，</span></div><div class="line">        <span class="comment">//对于之后的层，这些图片都是上一层的输出，通道数等于上一层卷积核个数）。</span></div><div class="line">        <span class="comment">// 最终重排的b为l.c * l.size * l.size行，l.out_h * l.out_w列。</span></div><div class="line">        <span class="comment">// 你会发现在前向forward_convolutional_layer()函数中，也为每层的输入进行了重排，</span></div><div class="line">        <span class="comment">//但是很遗憾的是，并没有一个l.workspace把每一层的重排结果保存下来，而是统一存储到net.workspace中，</span></div><div class="line">        <span class="comment">// 并被不断擦除更新，那为什么不保存呢？保存下来不是省掉一大笔额外重复计算开销？原因有两个：</span></div><div class="line">        <span class="comment">//1）net.workspace中只存储了一张输入图片的重排结果，所以重排下张图片时，马上就会被擦除，</span></div><div class="line">        <span class="comment">// 当然你可能会想，那为什么不弄一个l.worspaces将每层所有输入图片的结果保存呢？这引出第二个原因；</span></div><div class="line">        <span class="comment">//2）计算成本是降低了，但存储空间需求急剧增加，想想每一层都有l.batch张图，且每张都是多通道的，</span></div><div class="line">        <span class="comment">// 重排后其元素个数还会增多，这个存储量搁谁都受不了，如果一个batch有128张图，输入图片尺寸为400*400，</span></div><div class="line">        <span class="comment">//3通道，网络有16层（假设每层输入输出尺寸及通道数都一样），那么单单为了存储这些重排结果，</span></div><div class="line">        <span class="comment">// 就需要128*400*400*3*16*4/1024/1024/1024 = 3.66G，所以为了权衡，只能重复计算！</span></div><div class="line">        im2col_cpu(im, l.c, l.h, l.w, </div><div class="line">                l.<span class="keyword">size</span>, l.stride, l.pad, b);</div><div class="line"></div><div class="line">        <span class="comment">// 下面计算当前层的权重更新值，所谓权重更新值就是weight = weight - alpha * weight_update中的weight_update，</span></div><div class="line">        <span class="comment">// 权重更新值等于当前层敏感度图中每个元素乘以相应的像素值，因为一个权重跟当前层多个输出有关联</span></div><div class="line">        <span class="comment">//（权值共享，即卷积核在图像中跨步移动做卷积，每个位置卷积得到的值</span></div><div class="line">        <span class="comment">// 都与该权值相关），所以对每一个权重更新值来说，需要在l.delta中找出所有与之相关的敏感度，</span></div><div class="line">        <span class="comment">//乘以相应像素值，再求和，具体实现的方式依靠im2col_cpu()与gemm_nt()完成。</span></div><div class="line">        <span class="comment">// （backward_convolutional_layer整个函数的代码非常重要，仅靠文字没有公式与图表辅助说明可能很难说清，</span></div><div class="line">        <span class="comment">//所以这部分更为清晰详细的说明，请参考个人博客！）</span></div><div class="line">        <span class="comment">// GEneral Matrix to Matrix Multiplication</span></div><div class="line">        <span class="comment">// 此处在im2col_cpu操作基础上，利用矩阵乘法c=alpha*a*b+beta*c完成对图像卷积的操作；</span></div><div class="line">        <span class="comment">// 0表示不对输入a进行转置，1表示对输入b进行转置；</span></div><div class="line">        <span class="comment">// m是输入a,c的行数，具体含义为卷积核的个数(l.n)；</span></div><div class="line">        <span class="comment">// n是输入b,c的列数，具体含义为每个卷积核元素个数乘以输入图像的通道数(l.size*l.size*l.c)；</span></div><div class="line">        <span class="comment">// k是输入a的列数也是b的行数，具体含义为每个输出特征图的元素个数（l.out_w*l.out_h）；</span></div><div class="line">        <span class="comment">// a,b,c即为三个参与运算的矩阵（用一维数组存储）,alpha=beta=1为常系数；</span></div><div class="line">        <span class="comment">// a为l.delta的一大行。l.delta为本层所有输出元素（包含整个batch中每张图片的所有输出特征图）</span></div><div class="line">        <span class="comment">//关于加权输入的导数（即激活函数的导数值）集合,</span></div><div class="line">        <span class="comment">// 元素个数为l.batch * l.out_h * l.out_w * l.out_c（l.out_c = l.n），</span></div><div class="line">        <span class="comment">//按行存储，共有l.batch行，l.out_c * l.out_h * l.out_w列，</span></div><div class="line">        <span class="comment">// 即l.delta中每行包含一张图的所有输出图，故这么一大行，又可以视作有l.out_c（l.out_c=l.n）小行，</span></div><div class="line">        <span class="comment">//l.out_h*l*out_w小列，而一次循环就是处理l.delta的一大行，</span></div><div class="line">        <span class="comment">// 故可以将a视作l.out_c行，l.out_h*l*out_w列的矩阵；</span></div><div class="line">        <span class="comment">// b为单张输入图像经过im2col_cpu重排后的图像数据；</span></div><div class="line">        <span class="comment">// c为输出，按行存储，可视作有l.n行，l.c*l.size*l.size列（l.c是输入图像的通道数，l.n是卷积核个数），</span></div><div class="line">	        <span class="comment">// 即c就是所谓的误差项（输出关于加权输入的导数），或者敏感度（强烈推荐：https://www.zybuluo.com/hanbingtao/note/485480）</span></div><div class="line">	        <span class="comment">//（一个核有l.c*l.size*l.size个权重，共有l.n个核）。</span></div><div class="line">        <span class="comment">// 由上可知：</span></div><div class="line">        <span class="comment">// a: (l.out_c) * (l.out_h*l*out_w)</span></div><div class="line">        <span class="comment">// b: (l.c * l.size * l.size) * (l.out_h * l.out_w)</span></div><div class="line">        <span class="comment">// c: (l.n) * (l.c*l.size*l.size)（注意：l.n = l.out_c）</span></div><div class="line">        <span class="comment">// 故要进行a * b + c计算，必须对b进行转置（否则行列不匹配），因故调用gemm_nt()函数</span></div><div class="line">        gemm(<span class="number">0</span>,<span class="number">1</span>,m,n,k,<span class="number">1</span>,a,k,b,k,<span class="number">1</span>,c,n);</div><div class="line"></div><div class="line">        <span class="comment">// 接下来，用当前层的敏感度图l.delta以及权重l.weights（还未更新）来获取上一层网络的敏感度图，</span></div><div class="line">        <span class="comment">//BP算法的主要流程就是依靠这种层与层之间敏感度反向递推传播关系来实现。</span></div><div class="line">      <span class="comment">//而每次开始遍历某一层网络之前，都会更新net.input为这一层网络前一层的输出，即prev.output,</span></div><div class="line">        <span class="comment">// 同时更新net.delta为prev.delta，因此，这里的net.delta是当前层前一层的敏感度图。</span></div><div class="line">        <span class="comment">// 已经强调很多次了，再说一次：下面得到的上一层的敏感度并不完整，完整的敏感度图是损失函数对上一层的加权输入的导数，</span></div><div class="line">        <span class="comment">// 而这里得到的敏感度图是损失函数对上一层输出值的导数，还差乘以一个输出值也即激活函数对加权输入的导数。</span></div><div class="line">        <span class="keyword">if</span>(net.delta)&#123;</div><div class="line">            <span class="comment">// 当前层还未更新的权重</span></div><div class="line">            a = l.weights;</div><div class="line"></div><div class="line">            <span class="comment">// 每次循环仅处理一张输入图，注意移位（l.delta的维度为l.batch * l.out_c * l.out_w * l.out_h）（注意l.n = l.out_c，另外提一下，对整个网络来说，每一层的l.batch其实都是一样的）</span></div><div class="line">            b = l.delta + i*m*k;</div><div class="line"></div><div class="line">            <span class="comment">// net.workspace和上面一样，还是一张输入图片的重排，不同的是，此处我们只需要这个容器，</span></div><div class="line">            <span class="comment">//而里面存储的值我们并不需要，在后面的处理过程中，</span></div><div class="line">            <span class="comment">// 会将其中存储的值一一覆盖掉（尺寸维持不变，还是(l.c * l.size * l.size) * (l.out_h * l.out_w）</span></div><div class="line">            c = net.<span class="keyword">workspace</span>;</div><div class="line"></div><div class="line">            <span class="comment">// 相比上一个gemm，此处的a对应上一个的c,b对应上一个的a，c对应上一个的b，即此处a,b,c的行列分别为：</span></div><div class="line">            <span class="comment">// a: (l.n) * (l.c*l.size*l.size)，表示当前层所有权重系数</span></div><div class="line">            <span class="comment">// b: (l.out_c) * (l.out_h*l*out_w)（注意：l.n = l.out_c），表示当前层的敏感度图</span></div><div class="line">            <span class="comment">// c: (l.c * l.size * l.size) * (l.out_h * l.out_w)，表示上一层的敏感度图</span></div><div class="line">            <span class="comment">//（其元素个数等于上一层网络单张输入图片的所有输出元素个数），</span></div><div class="line">            <span class="comment">// 此时要完成a * b + c计算，必须对a进行转置（否则行列不匹配），因故调用gemm_tn()函数。</span></div><div class="line">            <span class="comment">// 此操作含义是用：用当前层还未更新的权重值对敏感度图做卷积，得到包含上一层所有敏感度信息的矩阵，</span></div><div class="line">            <span class="comment">//但这不是上一层最终的敏感度图，因为此时的c，也即net.workspace的尺寸为</span></div><div class="line">            <span class="comment">//(l.c * l.size * l.size)*(l.out_h * l.out_w)，明显不是上一层的输出尺寸l.c*l.w*l.h，</span></div><div class="line">            <span class="comment">// 接下来还需要调用col2im_cpu()函数将其恢复至l.c*l.w*l.h（可视为l.c行，l.w*l.h列），</span></div><div class="line">            <span class="comment">//这才是上一层的敏感度图（实际还差一个环节，这个环节需要等到下一次调用backward_convolutional_layer()才完成：</span></div><div class="line">            <span class="comment">//将net.delta中每个元素乘以激活函数对加权输入的导数值）。完成gemm这一步，如col2im_cpu()中注释，</span></div><div class="line">            <span class="comment">//是考虑了多个卷积核导致的一对多关系（上一层的一个输出元素会流入到下一层多个输出元素中），</span></div><div class="line">            <span class="comment">// 接下来调用col2im_cpu()则是考虑卷积核重叠（步长较小）导致的一对多关系。</span></div><div class="line">            gemm(<span class="number">1</span>,<span class="number">0</span>,n,k,m,<span class="number">1</span>,a,n,b,k,<span class="number">0</span>,c,k);</div><div class="line"></div><div class="line">            <span class="comment">// 对c也即net.workspace进行重排，得到的结果存储在net.delta中，每次循环只会处理一张输入图片，</span></div><div class="line">            <span class="comment">//因此，此处只会得到一张输入图产生的敏感图（注意net.delta的移位）,</span></div><div class="line">            <span class="comment">// 整个循环结束后，net.delta的总尺寸为l.batch * l.h * l.w * l.c，这就是上一层网络整个batch的敏感度图，</span></div><div class="line">            <span class="comment">//可视为有l.batch行，l.h*l.w*l.c列，每行存储了一张输入图片所有输出特征图的敏感度</span></div><div class="line">            <span class="comment">// col2im_cpu()函数中会调用col2im_add_pixel()函数，该函数中使用了+=运算符，</span></div><div class="line">            <span class="comment">//也即该函数要求输入的net.delta的初始值为0,而在gradient_array()中注释到l.delta的元素是不为0（也不能为0）的，</span></div><div class="line">            <span class="comment">// 看上去是矛盾的，实则不然，gradient_array()使用的l.delta是当前层的敏感度图，</span></div><div class="line">            <span class="comment">//而在col2im_cpu()使用的net.delta是上一层的敏感度图，正如gradient_array()中所注释的，</span></div><div class="line">            <span class="comment">// 当前层l.delta之所以不为0,是因为从后面层反向传播过来的，对于上一层，显然还没有反向传播到那，</span></div><div class="line">            <span class="comment">//因此net.delta的初始值都是为0的（注意，每一层在构建时，就为其delta动态分配了内存，</span></div><div class="line">            <span class="comment">// 且在前向传播时，为每一层的delta都赋值为0,可以参考network.c中forward_network()函数）</span></div><div class="line">            col2im_cpu(net.<span class="keyword">workspace</span>, l.c,  l.h,  l.w,  l.<span class="keyword">size</span>,  l.stride, l.pad, net.delta+i*l.c*l.h*l.w);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="5-Dropout层"><a href="#5-Dropout层" class="headerlink" title="5. Dropout层"></a>5. Dropout层</h2><h3 id="5-1-前向传播"><a href="#5-1-前向传播" class="headerlink" title="5.1 前向传播"></a>5.1 前向传播</h3><figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** dropout层前向传播函数</span></div><div class="line"><span class="comment">** 输入： l    当前dropout层网络</span></div><div class="line"><span class="comment">**       net  整个网络</span></div><div class="line"><span class="comment">** 说明：dropout层同样没有训练参数，因此前向传播比较简单，只完成一个事：按指定概率l.probability，</span></div><div class="line"><span class="comment">**      丢弃输入元素，并将保留下来的输入元素乘以比例因子（采用的是inverted dropout，这种方式实现更为方便，</span></div><div class="line"><span class="comment">**      且代码接口比较统一，想想看，如果采用标准的droput，则测试阶段还需要进入forward_dropout_layer()，</span></div><div class="line"><span class="comment">**      使每个输入乘以保留概率，而使用inverted dropout，测试阶段根本就不需要进入到forward_dropout_layer）。</span></div><div class="line"><span class="comment">** 说明2：dropout层输入与输出元素个数相同（即l.intputs=l.outputs）</span></div><div class="line"><span class="comment">** 说明3：关于inverted dropout，在网上随便搜索关于dropout的博客，都会讲到，这里给一个博客链接：https://yq.aliyun.com/articles/68901</span></div><div class="line"><span class="comment">*/</span></div><div class="line">void forward_dropout_layer(dropout_layer <span class="keyword">l</span>, network <span class="keyword">net</span>)</div><div class="line">&#123;</div><div class="line">    int i;</div><div class="line">    <span class="comment">// 如果当前网络不是处于训练阶段而处于测试阶段，则直接返回（使用inverted dropout带来的方便）</span></div><div class="line">    <span class="keyword">if</span> (!<span class="keyword">net</span>.train) <span class="keyword">return</span>;</div><div class="line"></div><div class="line">    <span class="comment">// 遍历dropout层的每一个输入元素（包含整个batch的），按照指定的概率l.probability置为0或者按l.scale缩放</span></div><div class="line">    <span class="keyword">for</span>(i = 0; i &lt; <span class="keyword">l</span>.batch * <span class="keyword">l</span>.inputs; ++i)&#123;</div><div class="line">        <span class="comment">// 产生一个0~1之间均匀分布的随机数</span></div><div class="line">        float r = rand_uniform(0, 1);</div><div class="line"></div><div class="line">        <span class="comment">// 每个输入元素都对应一个随机数，保存在l.rand中</span></div><div class="line">        <span class="keyword">l</span>.rand[i] = r;</div><div class="line"></div><div class="line">        <span class="comment">// 如果r小于l.probability（l.probability是舍弃概率），则舍弃该输入元素，注意，舍弃并不是删除，</span></div><div class="line">        <span class="comment">// 而是将其值置为0,所以输入元素个数总数没变（因故输出元素个数l.outputs等于l.inputs）</span></div><div class="line">        <span class="keyword">if</span>(r &lt; <span class="keyword">l</span>.probability) <span class="keyword">net</span>.<span class="keyword">input</span>[i] = 0;</div><div class="line">        <span class="comment">// 否则保留该输入元素，并乘以比例因子</span></div><div class="line">        <span class="keyword">else</span> <span class="keyword">net</span>.<span class="keyword">input</span>[i] *= <span class="keyword">l</span>.scale;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="5-2-反向传播"><a href="#5-2-反向传播" class="headerlink" title="5.2 反向传播"></a>5.2 反向传播</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** dropout层反向传播函数</span></div><div class="line"><span class="comment">** 输入： l    当前dropout层网络</span></div><div class="line"><span class="comment">**       net  整个网络</span></div><div class="line"><span class="comment">** 说明：dropout层的反向传播相对简单，因为其本身没有训练参数，也没有激活函数，或者说激活函数就为f(x) = x，也</span></div><div class="line"><span class="comment">**      也就是激活函数关于加权输入的导数值为1,因此其自身的敏感度值已经由其下一层网络反向传播时计算完了，</span></div><div class="line"><span class="comment">**      没有必要再乘以激活函数关于加权输入的导数了。剩下要做的就是计算上一层的敏感度图net.delta，这个计算也很简单，详见下面注释。</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">backward_dropout_layer</span><span class="params">(dropout_layer l, network net)</span></span></div><div class="line"><span class="function"></span>&#123;</div><div class="line">    <span class="comment">// 如果进入backward_dropout_layer()函数，那没得说，一定是训练阶段，因为测试阶段压根就没有反向过程，只有前向过程，</span></div><div class="line">    <span class="comment">// 所以就不再需要像forward_dropout_layer()函数中一样判断net.train是不是处于训练阶段了</span></div><div class="line">    <span class="keyword">int</span> i;</div><div class="line"></div><div class="line">    <span class="comment">// 如果net.delta为空，则返回（net.delta为空则说明已经反向到第一层了，此处所指第一层，是net.layers[0]，</span></div><div class="line">    <span class="comment">// 也是与输入层直接相连的第一层隐含层，详细参见：network.c中的forward_network()函数）</span></div><div class="line">    <span class="keyword">if</span>(!net.delta) <span class="keyword">return</span>;</div><div class="line"></div><div class="line">    <span class="comment">// 因为dropout层的输入输出元素个数相等，所以dropout层的敏感度图的维度就为l.batch*l.inputs（每一层的敏感度值与该层的输出维度一致），</span></div><div class="line">    <span class="comment">// 以下循环遍历当前层的敏感度图，并根据l.rand的指示反向计算上一层的敏感度值，由于当前dropout层与上一层之间的连接没有权重，</span></div><div class="line">    <span class="comment">// 或者说连接权重为0（对于舍弃的输入）或固定的l.scale（保留的输入，这个比例因子是固定的，不需要训练），所以计算过程比较简单，</span></div><div class="line">    <span class="comment">// 只需让保留输入对应输出的敏感度值乘以l.scale，其他输入（输入是针对当前dropout层而言，实际为上一层的输出）的敏感度值直接置为0即可</span></div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; l.batch * l.inputs; ++i)&#123;</div><div class="line">        <span class="keyword">float</span> r = l.rand[i];</div><div class="line">        <span class="comment">// 与前向过程forward_dropout_layer照应，根据l.rand指示，如果r小于l.probability，说明是舍弃的输入，其敏感度值为0；</span></div><div class="line">        <span class="comment">// 反之是保留下来的输入元素，其敏感度值为当前层对应输出的敏感度值乘以l.scale</span></div><div class="line">        <span class="keyword">if</span>(r &lt; l.probability) net.delta[i] = <span class="number">0</span>;</div><div class="line">        <span class="keyword">else</span> net.delta[i] *= l.scale;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="6-Maxpool层"><a href="#6-Maxpool层" class="headerlink" title="6. Maxpool层"></a>6. Maxpool层</h2><h3 id="6-1-前向传播"><a href="#6-1-前向传播" class="headerlink" title="6.1 前向传播"></a>6.1 前向传播</h3><figure class="highlight glsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 最大池化层前向传播函数：计算l层的输出</span></div><div class="line"><span class="comment">** 输入： l    当前层（最大池化层）</span></div><div class="line"><span class="comment">**       net  整个网络结构</span></div><div class="line"><span class="comment">** 说明：最大池化层处理图像的方式与卷积层类似，也是将最大池化核在图像平面上按照指定的跨度移动，</span></div><div class="line"><span class="comment">**      并取对应池化核区域中最大元素值为对应输出元素。最大池化层没有训练参数（没有权重以及偏置），</span></div><div class="line"><span class="comment">**      因此，相对与卷积来说，其前向（以及下面的反向）过程比较简单，实现上也是非常直接，不需要什么技巧。</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="type">void</span> forward_maxpool_layer(<span class="keyword">const</span> maxpool_layer l, network net)</div><div class="line">&#123;</div><div class="line">    <span class="type">int</span> b,i,j,k,m,n;</div><div class="line">    <span class="comment">// 初始偏移设定为四周补0长度的负值</span></div><div class="line">    <span class="type">int</span> w_offset = -l.pad;</div><div class="line">    <span class="type">int</span> h_offset = -l.pad;</div><div class="line"></div><div class="line">    <span class="comment">// 获取当前层的输出尺寸</span></div><div class="line">    <span class="type">int</span> h = l.out_h;</div><div class="line">    <span class="type">int</span> w = l.out_w;</div><div class="line"></div><div class="line">    <span class="comment">// 获取当前层输入图像的通道数，为什么是输入通道数？不应该为输出通道数吗？</span></div><div class="line">    <span class="comment">//实际二者没有区别，对于最大池化层来说，输入有多少通道，输出就有多少通道！</span></div><div class="line">    <span class="type">int</span> c = l.c;</div><div class="line"></div><div class="line">    <span class="comment">// 遍历batch中每一张输入图片，计算得到与每一张输入图片具有相同通道数的输出图</span></div><div class="line">    <span class="keyword">for</span>(b = <span class="number">0</span>; b &lt; l.batch; ++b)&#123;</div><div class="line">        <span class="comment">// 对于每张输入图片，将得到通道数一样的输出图，以输出图为基准，按输出图通道，行，列依次遍历</span></div><div class="line">        <span class="comment">// （这对应图像在l.output的存储方式，每张图片按行铺排成一大行，然后图片与图片之间再并成一行）。</span></div><div class="line">        <span class="comment">// 以输出图为基准进行遍历，最终循环的总次数刚好覆盖池化核在输入图片不同位置进行池化操作。</span></div><div class="line">        <span class="keyword">for</span>(k = <span class="number">0</span>; k &lt; c; ++k)&#123;</div><div class="line">            <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; h; ++i)&#123;</div><div class="line">                <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; w; ++j)&#123;</div><div class="line"></div><div class="line">                    <span class="comment">// out_index为输出图中的索引：out_index = b * c * w * h + k * w * h + h * w + w，</span></div><div class="line">                    <span class="comment">//展开写可能更为清晰些</span></div><div class="line">                    <span class="type">int</span> out_index = j + w*(i + h*(k + c*b));</div><div class="line"></div><div class="line">                    <span class="type">float</span> <span class="built_in">max</span> = -FLT_MAX;   </div><div class="line">                    <span class="comment">// FLT_MAX为c语言中float.h定义的对大浮点数，</span></div><div class="line">                    <span class="comment">//此处初始化最大元素值为最小浮点数</span></div><div class="line">					<span class="comment">// 最大元素值的索引初始化为-1</span></div><div class="line"></div><div class="line">                    <span class="comment">// 下面两个循环回到了输入图片，计算得到的cur_h以及cur_w都是在当前层所有输入元素的索引，</span></div><div class="line">                    <span class="comment">//内外循环的目的是找寻输入图像中，</span></div><div class="line">                    <span class="comment">// 以(h_offset + i*l.stride, w_offset + j*l.stride)为左上起点，</span></div><div class="line">                    <span class="comment">//尺寸为l.size池化区域中的最大元素值max及其在所有输入元素中的索引max_i</span></div><div class="line">                    <span class="keyword">for</span>(n = <span class="number">0</span>; n &lt; l.size; ++n)&#123;</div><div class="line">                        <span class="keyword">for</span>(m = <span class="number">0</span>; m &lt; l.size; ++m)&#123;</div><div class="line">                            <span class="comment">// cur_h，cur_w是在所有输入图像中第k通道中的cur_h行与cur_w列，</span></div><div class="line">                            <span class="comment">//index是在所有输入图像元素中的总索引。</span></div><div class="line">                            <span class="comment">// 为什么这里少一层对输入通道数的遍历循环呢？因为对于最大池化层来说</span></div><div class="line">                            <span class="comment">//输入与输出通道数是一样的，并在上面的通道数循环了！</span></div><div class="line">                            <span class="type">int</span> cur_h = h_offset + i*l.stride + n;</div><div class="line">                            <span class="type">int</span> cur_w = w_offset + j*l.stride + m;</div><div class="line">                            <span class="type">int</span> <span class="keyword">index</span> = cur_w + l.w*(cur_h + l.h*(k + b*l.c));</div><div class="line"></div><div class="line">                            <span class="comment">// 边界检查：正常情况下，是不会越界的，但是如果有补0操作，就会越界了，</span></div><div class="line">                            <span class="comment">//这里的处理方式是直接让这些元素值为-FLT_MAX</span></div><div class="line">                            <span class="comment">// （注意虽然称之为补0操作，但实际不是补0），总之，这些补的元素永远不会充当最大元素值。</span></div><div class="line">                            <span class="type">int</span> valid = (cur_h &gt;= <span class="number">0</span> &amp;&amp; cur_h &lt; l.h &amp;&amp;</div><div class="line">                                         cur_w &gt;= <span class="number">0</span> &amp;&amp; cur_w &lt; l.w);</div><div class="line">                            <span class="type">float</span> val = (valid != <span class="number">0</span>) ? net.input[<span class="keyword">index</span>] : -FLT_MAX;</div><div class="line"></div><div class="line">                            <span class="comment">// 记录这个池化区域中的最大的元素值及其在所有输入元素中的总索引</span></div><div class="line">                            max_i = (val &gt; <span class="built_in">max</span>) ? <span class="keyword">index</span> : max_i;</div><div class="line">                            <span class="built_in">max</span>   = (val &gt; <span class="built_in">max</span>) ? val   : <span class="built_in">max</span>;</div><div class="line">                        &#125;</div><div class="line">                    &#125;</div><div class="line">                    <span class="comment">// 由此得到最大池化层每一个输出元素值及其在所有输入元素中的总索引。</span></div><div class="line">                    <span class="comment">// 为什么需要记录每个输出元素值对应在输入元素中的总索引呢？因为在下面的反向过程中需要用到，</span></div><div class="line">                    <span class="comment">//在计算当前最大池化层上一层网络的敏感度时，</span></div><div class="line">                    <span class="comment">// 需要该索引明确当前层的每个元素究竟是取上一层输出（也即上前层输入）的哪一个元素的值，</span></div><div class="line">                    <span class="comment">//具体见下面backward_maxpool_layer()函数的注释。</span></div><div class="line">                    l.output[out_index] = <span class="built_in">max</span>;</div><div class="line">                    l.indexes[out_index] = max_i;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="6-2-反向传播"><a href="#6-2-反向传播" class="headerlink" title="6.2 反向传播"></a>6.2 反向传播</h3><figure class="highlight glsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 最大池化层反向传播传播函数</span></div><div class="line"><span class="comment">** 输入： l     当前最大池化层</span></div><div class="line"><span class="comment">**       net   整个网络</span></div><div class="line"><span class="comment">** 说明：这个函数看上去很简单，比起backward_convolutional_layer()少了很多，这都是有原因的。实际上，在darknet中，不管是什么层，</span></div><div class="line"><span class="comment">**      其反向传播函数都会先后做两件事：</span></div><div class="line"><span class="comment">**      1）计算当前层的敏感度图l.delta、权重更新值以及偏置更新值；</span></div><div class="line"><span class="comment">**      2）计算上一层的敏感度图net.delta（部分计算，要完成计算得等到真正到了这一层再说）。</span></div><div class="line"><span class="comment">**      而这里，显然没有第一步，只有第二步，而且很简单，这是为什么呢？</span></div><div class="line"><span class="comment">**      首先回答为什么没有第一步。注意当前层l是最大池化层，最大池化层没有训练参数，</span></div><div class="line"><span class="comment">**      说的再直白一点就是没有激活函数，或者认为激活函数就是f(x)=x，所以激活函数对于加权输入的导数其实就是1,</span></div><div class="line"><span class="comment">**      正如在backward_convolutional_layer()注释的那样，每一层的反向传播函数的第一步是将之前</span></div><div class="line"><span class="comment">**      （就是下一层计算得到的，注意过程是反向的）未计算完得到的l.delta乘以激活函数对加权输入的导数，</span></div><div class="line"><span class="comment">**      以最终得到当前层的敏感度图，而对于最大池化层来说，每一个输出对于加权输入的导数值都是1,</span></div><div class="line"><span class="comment">**      同时并没有权重及偏置这些需要训练的参数，自然不再需要第一步；对于第二步为什么会如此简单，可以参考：</span></div><div class="line"><span class="comment">**      https://www.zybuluo.com/hanbingtao/note/485480，最大池化层它就是这么简单，剩下的参考下面的注释。</span></div><div class="line"><span class="comment">*/</span></div><div class="line"><span class="type">void</span> backward_maxpool_layer(<span class="keyword">const</span> maxpool_layer l, network net)</div><div class="line">&#123;</div><div class="line">    <span class="type">int</span> i;</div><div class="line">    <span class="comment">// 获取当前最大池化层l的输出尺寸h,w</span></div><div class="line">    <span class="type">int</span> h = l.out_h;</div><div class="line">    <span class="type">int</span> w = l.out_w;</div><div class="line"></div><div class="line">    <span class="comment">// 获取当前层输入的通道数，为什么是输入通道数？不应该为输出通道数吗？实际二者没有区别，</span></div><div class="line">    <span class="comment">//对于最大池化层来说，输入有多少通道，输出就有多少通道！</span></div><div class="line">    <span class="type">int</span> c = l.c;</div><div class="line"></div><div class="line">    <span class="comment">// 计算上一层的敏感度图（未计算完全，还差一个环节，这个环节等真正反向到了那层再执行）</span></div><div class="line">    <span class="comment">// 这个循环很有意思，循环总次数为当前层输出总元素个数（包含所有输入图片的输出，</span></div><div class="line">    <span class="comment">//即维度为l.out_h * l.out_w * l.c * l.batch，注意此处l.c==l.out_c）,</span></div><div class="line">    <span class="comment">// 而不是上一层输出总元素个数，为什么呢？是因为对于最大池化层而言，</span></div><div class="line">    <span class="comment">//其每个输出元素对仅受上一层输出对应池化核区域中最大值元素的影响，所以当前池化层每个输出元素</span></div><div class="line">    <span class="comment">// 对于上一层输出中的很多元素的导数值为0,而对最大值元素，其导数值为1；再乘以当前层的敏感度图，</span></div><div class="line">    <span class="comment">//导数值为0的还是为0,导数值为1则就等于当前层的敏感度值。</span></div><div class="line">    <span class="comment">// 以输出图总元素个数进行遍历，刚好可以找出上一层输出中所有真正起作用（在某个池化区域中充当了最大元素值）</span></div><div class="line">    <span class="comment">//也即敏感度值不为0的元素，而那些没有起作用的元素，</span></div><div class="line">    <span class="comment">// 可以不用理会，保持其初始值0就可以了。</span></div><div class="line">    <span class="comment">// 详细原理推导可以参见：https://www.zybuluo.com/hanbingtao/note/485480</span></div><div class="line">    <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; h*w*c*l.batch; ++i)&#123;</div><div class="line">        <span class="comment">// 遍历的基准是以当前层的输出元素为基准的，l.indexes记录了当前层每一个输出元素与</span></div><div class="line">        <span class="comment">//上一层哪一个输出元素有真正联系（也即上一层对应池化核区域中最大值元素的索引），</span></div><div class="line">        <span class="comment">// 所以index是上一层中所有输出元素的索引，且该元素在当前层某个池化域中充当了最大值元素，</span></div><div class="line">        <span class="comment">//这个元素的敏感度值将直接传承当前层对应元素的敏感度值。</span></div><div class="line">        <span class="comment">// 而net.delta中，剩下没有被index按索引访问到的元素，就是那些没有真正起到作用的元素，</span></div><div class="line">        <span class="comment">//这些元素的敏感度值为0（net.delta已经在前向时将所有元素值初始化为0）</span></div><div class="line">        <span class="comment">// 至于为什么要用+=运算符，原因有两个，和卷积类似：一是池化核由于跨度较小，导致有重叠区域；</span></div><div class="line">        <span class="comment">//二是batch中有多张图片，需要将所有图片的影响加起来。</span></div><div class="line">        <span class="type">int</span> <span class="keyword">index</span> = l.indexes[i];</div><div class="line">        net.delta[<span class="keyword">index</span>] += l.delta[i];</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="7-RNN"><a href="#7-RNN" class="headerlink" title="7. RNN"></a>7. RNN</h2><p>darknet中的RNN是vanilla RNN，RNN层本质上是三个全连接层构成的，具体结构可以参考 <a href="https://pjreddie.com/darknet/rnns-in-darknet/" target="_blank" rel="external">https://pjreddie.com/darknet/rnns-in-darknet/</a></p>
<h3 id="7-1-前向传播层"><a href="#7-1-前向传播层" class="headerlink" title="7.1 前向传播层"></a>7.1 前向传播层</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** RNN层前向传播</span></div><div class="line"><span class="comment">** 输入    l      当前的RNN层</span></div><div class="line"><span class="comment">**        net     当前网络</span></div><div class="line"><span class="comment">**</span></div><div class="line"><span class="comment">** RNN层前向传播与其他网络不同，RNN中的全连接层的当前状态与上一个时间的状态有关，</span></div><div class="line"><span class="comment">** 所以要在每次传播后记录上一个时刻的状态</span></div><div class="line"><span class="comment">*/</span></div><div class="line"></div><div class="line">void forward_rnn_layer(layer l, network net)</div><div class="line">&#123;</div><div class="line">    network s = net;</div><div class="line">    s.train = net.train;</div><div class="line">    int i;</div><div class="line">    layer input_layer = *(l.input_layer);</div><div class="line">    layer self_layer = *(l.self_layer);</div><div class="line">    layer output_layer = *(l.output_layer);</div><div class="line"></div><div class="line">    <span class="comment">/* 开始训练前要将三个全连接层的错误值设置为0 */</span></div><div class="line">    fill_cpu(l.outputs * l.batch * l.steps, <span class="number">0</span>, output_layer.delta, <span class="number">1</span>);</div><div class="line">    fill_cpu(l.hidden * l.batch * l.steps, <span class="number">0</span>, self_layer.delta, <span class="number">1</span>);</div><div class="line">    fill_cpu(l.hidden * l.batch * l.steps, <span class="number">0</span>, input_layer.delta, <span class="number">1</span>);</div><div class="line">    <span class="comment">/* 如果网络处于训练状态，要将state设置为0，因为初始状态的上一时刻状态是不存在的，我们只能假设它存在，并把它赋值为0 */</span></div><div class="line">    if(net.train) fill_cpu(l.hidden * l.batch, <span class="number">0</span>, l.<span class="section">state</span>, <span class="number">1</span>);</div><div class="line"></div><div class="line">    <span class="comment">/*</span></div><div class="line"><span class="comment">    ** 以下是RNN层前向传播的主要过程，该层总共要传播steps次，每次的输入batch个字符。</span></div><div class="line"><span class="comment">    ** Vanilla RNN具体结构参考 https://pjreddie.com/darknet/rnns-in-darknet/，</span></div><div class="line"><span class="comment">    ** 这里我只简单的说明一下。</span></div><div class="line"><span class="comment">    ** Vanilla RNN的RNN层虽然包含三个全连接层，但是只有中间一层（也就是self_layer)与传统的RNN的隐含层一致。</span></div><div class="line"><span class="comment">    **</span></div><div class="line"><span class="comment">    ** 第一层input_layer可以理解为embedding层，它将input编码为一个hidden维的向量，</span></div><div class="line"><span class="comment">    ** 以darknet的字符预测问题为例，网络的input是英文字母</span></div><div class="line"><span class="comment">    ** 采用one-hot编码，是一个256维的向量，embedding后成为了hidden维的向量。</span></div><div class="line"><span class="comment">    **</span></div><div class="line"><span class="comment">    ** 第二层self_layer与普通RNN的隐含层功能相同，它接收输入层和上一时刻的状态作为输入。</span></div><div class="line"><span class="comment">    **</span></div><div class="line"><span class="comment">    ** 第三层output_layer，接收self_layer的输出为输入，需要注意的是这一层的输出并不是最终结果，</span></div><div class="line"><span class="comment">    ** 还需要做进一步处理，还是以darknet的字符预测为例，第三层的输出要进一步转化为一个256维的向量，</span></div><div class="line"><span class="comment">    ** 然后进行归一化，找到概率最大的字符作为预测结果</span></div><div class="line"><span class="comment">    */</span></div><div class="line">    </div><div class="line">    for (i = <span class="number">0</span>; i &lt; l.steps; ++i) &#123;</div><div class="line">        s.input = net.input;</div><div class="line">        forward_connected_layer(input_layer, s);</div><div class="line"></div><div class="line">        s.input = l.<span class="section">state</span>;</div><div class="line">        forward_connected_layer(self_layer, s);</div><div class="line"></div><div class="line">        <span class="type">float</span> *old_state = l.<span class="section">state</span>;                </div><div class="line">         <span class="comment">// 将当前状态存入上一时刻状态</span></div><div class="line">        if(net.train) l.<span class="section">state</span> += l.hidden*l.batch;  </div><div class="line">        <span class="comment">// 如果网络处于训练状态，注意的是上一时刻的状态包含一个batch</span></div><div class="line">        <span class="comment">// 如何设置当前状态，由shortcut的值决定</span></div><div class="line">            copy_cpu(l.hidden * l.batch, old_state, <span class="number">1</span>, l.<span class="section">state</span>, <span class="number">1</span>);</div><div class="line">        &#125;else&#123;</div><div class="line">            fill_cpu(l.hidden * l.batch, <span class="number">0</span>, l.<span class="section">state</span>, <span class="number">1</span>);</div><div class="line">        &#125;</div><div class="line">        axpy_cpu(l.hidden * l.batch, <span class="number">1</span>, input_layer.output, <span class="number">1</span>, l.<span class="section">state</span>, <span class="number">1</span>);</div><div class="line">        axpy_cpu(l.hidden * l.batch, <span class="number">1</span>, self_layer.output, <span class="number">1</span>, l.<span class="section">state</span>, <span class="number">1</span>);</div><div class="line"></div><div class="line">        s.input = l.<span class="section">state</span>;</div><div class="line">        forward_connected_layer(output_layer, s);</div><div class="line"></div><div class="line">        <span class="comment">/* 一次传播结束，将三个层同时向前推移一步 */</span></div><div class="line">        net.input += l.inputs*l.batch;</div><div class="line">        increment_layer(&amp;input_layer, <span class="number">1</span>);</div><div class="line">        increment_layer(&amp;self_layer, <span class="number">1</span>);</div><div class="line">        increment_layer(&amp;output_layer, <span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="7-1-后向传播层"><a href="#7-1-后向传播层" class="headerlink" title="7.1 后向传播层"></a>7.1 后向传播层</h3><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/*</span></div><div class="line"><span class="comment">** 误差后向传播</span></div><div class="line"><span class="comment">** 输入     l       当前RNN层</span></div><div class="line"><span class="comment">**          net     当前网络</span></div><div class="line"><span class="comment">*/</span></div><div class="line">void <span class="keyword">backward_rnn_layer(layer </span>l, network net)</div><div class="line">&#123;</div><div class="line">    network s = net<span class="comment">;</span></div><div class="line">    s.train = net.train<span class="comment">;</span></div><div class="line">    int i<span class="comment">;</span></div><div class="line">    layer input_layer = *(l.input_layer)<span class="comment">;</span></div><div class="line">    layer self_layer = *(l.self_layer)<span class="comment">;</span></div><div class="line">    layer output_layer = *(l.output_layer)<span class="comment">;</span></div><div class="line"></div><div class="line">    <span class="comment">/* 误差传播从网络中最后一步开始 */</span></div><div class="line">    increment_layer(&amp;input_layer, l.steps-1)<span class="comment">;</span></div><div class="line">    increment_layer(&amp;self_layer, l.steps-1)<span class="comment">;</span></div><div class="line">    increment_layer(&amp;output_layer, l.steps-1)<span class="comment">;</span></div><div class="line"></div><div class="line">    l.state += l.hidden*l.<span class="keyword">batch*l.steps;</span></div><div class="line"><span class="keyword"> </span>   for (i = l.steps-1<span class="comment">; i &gt;= 0; --i) &#123;</span></div><div class="line">        copy_cpu(l.hidden * l.<span class="keyword">batch, </span>input_layer.output, <span class="number">1</span>, l.state, <span class="number">1</span>)<span class="comment">;</span></div><div class="line">        axpy_cpu(l.hidden * l.<span class="keyword">batch, </span><span class="number">1</span>, self_layer.output, <span class="number">1</span>, l.state, <span class="number">1</span>)<span class="comment">;</span></div><div class="line"></div><div class="line">        <span class="comment">/* 计算output_layer层的误差 */</span></div><div class="line">        s.input = l.state<span class="comment">;</span></div><div class="line">        s.delta = self_layer.delta<span class="comment">;</span></div><div class="line">        <span class="keyword">backward_connected_layer(output_layer, </span>s)<span class="comment">;</span></div><div class="line"></div><div class="line">        l.state -= l.hidden*l.<span class="keyword">batch;</span></div><div class="line"><span class="keyword"> </span>       <span class="comment">/*</span></div><div class="line"><span class="comment">           if(i &gt; 0)&#123;</span></div><div class="line"><span class="comment">           copy_cpu(l.hidden * l.batch, input_layer.output - l.hidden*l.batch, 1, l.state, 1);</span></div><div class="line"><span class="comment">           axpy_cpu(l.hidden * l.batch, 1, self_layer.output - l.hidden*l.batch, 1, l.state, 1);</span></div><div class="line"><span class="comment">           &#125;else&#123;</span></div><div class="line"><span class="comment">           fill_cpu(l.hidden * l.batch, 0, l.state, 1);</span></div><div class="line"><span class="comment">           &#125;</span></div><div class="line"><span class="comment">         */</span></div><div class="line"></div><div class="line">        <span class="comment">/* 计算self_layer层的误差 */</span></div><div class="line">        s.input = l.state<span class="comment">;</span></div><div class="line">        s.delta = self_layer.delta - l.hidden*l.<span class="keyword">batch;</span></div><div class="line"><span class="keyword"> </span>       if (i == <span class="number">0</span>) s.delta = <span class="number">0</span><span class="comment">;</span></div><div class="line">        <span class="keyword">backward_connected_layer(self_layer, </span>s)<span class="comment">;</span></div><div class="line"></div><div class="line">        copy_cpu(l.hidden*l.<span class="keyword">batch, </span>self_layer.delta, <span class="number">1</span>, input_layer.delta, <span class="number">1</span>)<span class="comment">;</span></div><div class="line">        if (i &gt; <span class="number">0</span> &amp;&amp; l.<span class="keyword">shortcut) </span>axpy_cpu(l.hidden*l.<span class="keyword">batch, </span><span class="number">1</span>, self_layer.delta,</div><div class="line">         <span class="number">1</span>, self_layer.delta -l.hidden*l.<span class="keyword">batch, </span><span class="number">1</span>)<span class="comment">;</span></div><div class="line">        s.input = net.input + i*l.inputs*l.<span class="keyword">batch;</span></div><div class="line"><span class="keyword"> </span>       if(net.delta) s.delta = net.delta + i*l.inputs*l.<span class="keyword">batch;</span></div><div class="line"><span class="keyword"> </span>       else s.delta = <span class="number">0</span><span class="comment">;</span></div><div class="line">        <span class="comment">/* 计算input_layer层的误差 */</span></div><div class="line">        <span class="keyword">backward_connected_layer(input_layer, </span>s)<span class="comment">;</span></div><div class="line"></div><div class="line">        <span class="comment">/* 误差传播一步之后，需要重新调整各个连接层， 向后移动一步 */</span></div><div class="line">        increment_layer(&amp;input_layer, -<span class="number">1</span>)<span class="comment">;</span></div><div class="line">        increment_layer(&amp;self_layer, -<span class="number">1</span>)<span class="comment">;</span></div><div class="line">        increment_layer(&amp;output_layer, -<span class="number">1</span>)<span class="comment">;</span></div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>工程地址：</strong><br><a href="https://github.com/hgpvision/darknet" target="_blank" rel="external">https://github.com/hgpvision/darknet</a></p>

  </section>

  <section class="post-comments">


<section id="comment">
<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2147613"></script>
<!-- UY END -->
</section>

    
</section>


</article>


            <footer class="footer">
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
    <span class="footer__copyright">&copy; 2017-2020. 个人邮箱：wj.bob@163.com</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    
  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$$','$$'], ["\\(","\\)"] ],
      displayMath: [ ['$','$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
			    extensions: ["tex2jax.js"],
                jax: ["input/TeX", "output/HTML-CSS"],
                tex2jax: {inlineMath: [['[latex]','[/latex]'],['\\(','\\)']],
				          displayMath: [ ['$','$'], ["\\[","\\]"] ],
						  processEscapes: true
						  },
                "HTML-CSS": { availableFonts: ["TeX"] }						  
            });
        });
    </script>
	


    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
				autoDetectHeadings: true,
				enableToTopButton: true,
				displayNow: true,
				title: "文章目录",
				css: {
					fontSize: "16px",
					largeFontSize: "20px",
					},
            });
        });
    </script>


    
    

    <script src="/js/jquery.githubRepoWidget.min.js"></script>


    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]-->

</body>
</html>
