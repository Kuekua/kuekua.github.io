<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      CS231n学习笔记--9.CNN Architectures | Kuekua&#39;s blog 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="Kuekua Wu">
    
    

    <meta name="description" content="1. AlexNet   Tips：  Trained on GTX 580 GPU with only 3 GB of memory.Network spread across 2 GPUs, half the neurons (feature maps) on eachGPU.所以在CONV1中分为两部分，每部分输出大小为55X55X48！  CONV1, CONV2, CONV4, C">
<meta name="keywords" content="Machine Learning,Algorithm,Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="CS231n学习笔记--9.CNN Architectures | Kuekua&#39;s blog">
<meta property="og:url" content="http://yoursite.com/2017/09/30/CS231n学习笔记--9.CNN Architectures/index.html">
<meta property="og:site_name" content="Kuekua&#39;s blog">
<meta property="og:description" content="1. AlexNet   Tips：  Trained on GTX 580 GPU with only 3 GB of memory.Network spread across 2 GPUs, half the neurons (feature maps) on eachGPU.所以在CONV1中分为两部分，每部分输出大小为55X55X48！  CONV1, CONV2, CONV4, C">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://img.blog.csdn.net/20171006102227749?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006104608122?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006105228083?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006105341267?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006105627204?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006110142088?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006110524369?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006110936604?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006111154381?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006112916918?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006113103546?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006155054290?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006154318957?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006155302440?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006155417500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006155812382?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006160056485?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006160409516?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006160734579?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006161004901?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006161050225?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006161416263?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006161558698?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006161729409?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20171006161816291?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:updated_time" content="2017-10-28T02:34:45.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CS231n学习笔记--9.CNN Architectures | Kuekua&#39;s blog">
<meta name="twitter:description" content="1. AlexNet   Tips：  Trained on GTX 580 GPU with only 3 GB of memory.Network spread across 2 GPUs, half the neurons (feature maps) on eachGPU.所以在CONV1中分为两部分，每部分输出大小为55X55X48！  CONV1, CONV2, CONV4, C">
<meta name="twitter:image" content="http://img.blog.csdn.net/20171006102227749?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.png">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css">

</head>
<body>

    <span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">Kuekua&#39;s blog</a></h1>
        <hr class="panel-cover__divider" />

        
        <p class="panel-cover__description">
          YesterDay you said tomorrow!
        </p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">首页</a></li>
              
                
                <li class="navigation__item"><a href="/archive" title="" class="">分类</a></li>
              
                
                <li class="navigation__item"><a href="/about" title="" class="">关于</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">CS231n学习笔记--9.CNN Architectures</h1>

    

    <div class="post-meta">
      <time datetime="2017-09-30" class="post-meta__date date">2017-09-30</time> 

      <span class="post-meta__tags tags">

          
            <font class="categories">
            &#8226; 分类:
            <a class="categories-link" href="/categories/CS231n/">CS231n</a>
            </font>
          

          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-link" href="/tags/Algorithm/">Algorithm</a>, <a class="tags-link" href="/tags/Deep-Learning/">Deep Learning</a>, <a class="tags-link" href="/tags/Machine-Learning/">Machine Learning</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h2 id="1-AlexNet"><a href="#1-AlexNet" class="headerlink" title="1. AlexNet"></a>1. AlexNet</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20171006102227749?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>Tips：</strong></p>
<ol>
<li><p>Trained on GTX 580 GPU with only 3 GB of memory.Network spread across 2 GPUs, half the neurons (feature maps) on each<br>GPU.所以在CONV1中分为两部分，每部分输出大小为55X55X48！</p>
</li>
<li><p>CONV1, CONV2, CONV4, CONV5: Connections only with feature maps on same GPU</p>
</li>
<li><p>CONV3, FC6, FC7, FC8: Connections with all feature maps in<br>preceding layer, communication across GPUs</p>
</li>
<li><p>heavy data augmentation：</p>
<p> a. 增大训练样本：通过对于图像的变换实现了对于数据集合的enlarge。首先对于输入的图像（size 256<em>256）随机提取224</em>224的图像集合，并对他们做一个horizontal reflections。变换后图像和原图像相差了32个像素，因此主体部分应该都包含在训练集合中，相当于在位置这个维度上丰富了训练数据。对horizontal reflections来说，相当于相机在主轴方向做了镜像，丰富了反方向的图像。数据集合增大了2048倍，直接结果就是降低了overfitting同时降低了网络结构设计的复杂层度。</p>
<p> 在测试阶段，取每一个测试样本四个角以及中间区域，一共5个patch然后再镜像后得到10个样本输入到网络中，最后将10个softmax输出平均后作为最后的输出。</p>
<p> b.使用PCA对于训练数据进行增强：对于每一个RGB图像进行一个PCA的变换，完成去噪功能，同时为了保证图像的多样性，在eigenvalue上加了一个随机的尺度因子，每一轮重新生成一个尺度因子，这样保证了同一副图像中在显著特征上有一定范围的变换，降低了overfitting的概率。</p>
</li>
<li><p>Norm layers:</p>
<p>更常用的是Local Response Normalization：<br>使用ReLU f(x)=max(0,x)后，你会发现激活函数之后的值没有了tanh，sigmoid函数那样有一个值域区间，所以一般在ReLU之后会做一个normalization，LRU就是其中一种方法，在神经科学中有个概念叫“Lateral inhibition”，讲的是活跃的神经元对它周边神经元的影响。</p>
</li>
</ol>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006104608122?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>从这个公式中可以看出，原来的激活值a被加一个归一化权重（分母部分）生成了新的激活b，相当于在同一个位置（x，y），不同的map上的激活进行了平滑，平滑操作大概可以将识别率提高1-2个百分点。<br>之后，这一层已经被其它种的Regularization技术，如drop out, batch normalization取代了。知道了这些，似乎也可以不那么纠结这个LRN了。</p>
<h2 id="2-ZFNet"><a href="#2-ZFNet" class="headerlink" title="2. ZFNet"></a>2. ZFNet</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20171006105228083?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006105341267?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<h2 id="3-VGGNet"><a href="#3-VGGNet" class="headerlink" title="3. VGGNet"></a>3. VGGNet</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20171006105627204?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>Tips：</strong></p>
<ol>
<li>用更小的filters的原因是可以大大减少参数的数量，并通过增加depth，达到与较大filters相同的效果（从参考的邻域点考虑Stack of three 3x3 conv (stride 1) layers has same effective receptive field as<br>one 7x7 conv layer）！</li>
<li>No Local Response Normalisation (LRN)</li>
</ol>
<p>VGGNet消耗资源表：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006110142088?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<h2 id="4-GoogLeNet"><a href="#4-GoogLeNet" class="headerlink" title="4. GoogLeNet"></a>4. GoogLeNet</h2><p>GoogLeNet架构：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006110524369?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>Tips：</strong></p>
<p>“Inception module”: design a good local network topology (network within a network) and then stack these modules on top of each other</p>
<p>初始的Inception module及其存在的问题：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006110936604?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>Solution: “bottleneck” layers that use 1x1 convolutions to reduce feature depth:</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006111154381?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>Tips:</strong></p>
<p>Add 1x1 CONV with 32 filters, preserves spatial dimensions, reduces depth! Projects depth to lower dimension (combination of feature maps).</p>
<p>改进的Inception module：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006112916918?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>Full GoogLeNet architecture：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006113103546?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>Auxiliary classification outputs可以缓解深度过深导致权重梯度过小而无法优化的现象！</p>
<h2 id="5-ResNet"><a href="#5-ResNet" class="headerlink" title="5. ResNet"></a>5. ResNet</h2><p>受到深度的意义的驱使，出现了这样一个问题：是不是更多的堆叠层就一定能学习出更好的网络？这个问题的一大障碍就是臭名昭著的梯度消失/爆炸问题，它从一开始就阻碍了收敛，然而梯度消失/爆炸的问题，很大程度上可以通过标准的初始化和正则化层来基本解决，确保几十层的网络能够收敛（用SGD+反向传播）。</p>
<p>　　然而当开始考虑更深层的网络的收敛问题时，退化问题就暴露了：随着神经网络深度的增加，精确度开始饱和（这是不足为奇的），然后会迅速的变差。出人意料的，这样一种退化，并不是过拟合导致的，并且增加更多的层匹配深度模型，会导致更多的训练误差。如下图所示：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006155054290?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"> <div align="left"> </div></div></p>
<p>　　我们通过引入一个深度残差学习框架，解决了这个退化问题。我们不期望每一层能直接吻合一个映射，我们明确的让这些层去吻合残差映射。形式上看，就是用H(X)来表示最优解映射，但我们让堆叠的非线性层去拟合另一个映射F（X）:=H(X) - X, 此时原最优解映射H（X）就可以改写成F(X)+X，我们假设残差映射跟原映射相比更容易被优化。极端情况下，如果一个映射是可优化的，那也会很容易将残差推至0，<strong>把残差推至0和把此映射逼近另一个非线性层相比要容易的多。</strong></p>
<p>　　F(X)+X的公式可以通过在前馈网络中做一个“快捷连接”来实现 ，快捷连接跳过一个或多个层。在我们的用例中，快捷连接简单的执行自身映射，它们的输出被添加到叠加层的输出中。自身快捷连接既不会添加额外的参数也不会增加计算复杂度。整个网络依然可以用SGD+反向传播来做端到端的训练，并且可以很容易用大众框架来实现（比如Caffe）不用修改slover配置（slover是caffe中的核心slover.prototxt）</p>
<p>ResNet结构图：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006154318957?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>ResNet结构特点：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006155302440?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p>更进一步的改进：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006155417500?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>Training ResNet in practice:</strong></p>
<ul>
<li>Batch Normalization after every CONV layer</li>
<li>Xavier/2 initialization from He et al.</li>
<li>SGD + Momentum (0.9)</li>
<li>Learning rate: 0.1, divided by 10 when validation error plateaus</li>
<li>Mini-batch size 256</li>
<li>Weight decay of 1e-5</li>
<li>No dropout used</li>
</ul>
<p>各网络架构性能（准确率，耗时，占用内存）比较：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006155812382?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>Tips：</strong></p>
<ol>
<li>Inception-v4: Resnet + Inception!</li>
<li>VGG: Highest memory, most operations</li>
<li>GoogLeNet: most efficient</li>
<li>AlexNet: Smaller compute, still memory heavy, lower accuracy</li>
<li>ResNet: Moderate efficiency depending on model, highest accuracy</li>
</ol>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006160056485?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<h2 id="6-Other-architectures"><a href="#6-Other-architectures" class="headerlink" title="6. Other architectures"></a>6. Other architectures</h2><p><strong>Network in Network (NiN)：</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006160409516?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="80%" align="center"> <div align="left"> </div></div></p>
<p><strong>Improving ResNets：</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006160734579?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>更宽的残差网络：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006161004901?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006161050225?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>加入随机概念：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006161416263?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p>Fractal architecture with both shallow and deep paths to output：</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006161558698?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><strong>Beyond ResNets</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006161729409?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><strong>Efficient networks</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20171006161816291?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="70%" align="center"> <div align="left"> </div></div></p>
<p><strong>Summary: CNN Architectures</strong></p>
<ul>
<li>VGG, GoogLeNet, ResNet all in wide use, available in model zoos</li>
<li>ResNet current best default</li>
<li>Trend towards extremely deep networks</li>
<li>Significant research centers around design of layer / skip connections and improving gradient flow</li>
<li>Even more recent trend towards examining necessity of depth vs.<br>width and residual connections</li>
</ul>

  </section>

  <section class="post-comments">

    <!-- 将评论系统（例如Disqus、多说、友言、畅言等）提供的代码片段粘贴在这里 -->
    
</section>


</article>


            <div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kuekua Wu</span>

  
</div>









        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    
  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$$','$$'], ["\\(","\\)"] ],
      displayMath: [ ['$','$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
			    extensions: ["tex2jax.js"],
                jax: ["input/TeX", "output/HTML-CSS"],
                tex2jax: {inlineMath: [['[latex]','[/latex]'],['\\(','\\)']],
				          displayMath: [ ['$','$'], ["\\[","\\]"] ],
						  processEscapes: true
						  },
                "HTML-CSS": { availableFonts: ["TeX"] }						  
            });
        });
    </script>
	


    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
				autoDetectHeadings: true,
				enableToTopButton: true,
				displayNow: true,
				title: "文章目录",
				css: {
					fontSize: "16px",
					largeFontSize: "20px",
					},
            });
        });
    </script>


    
    

    <script src="/js/jquery.githubRepoWidget.min.js"></script>


    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]-->

</body>
</html>
