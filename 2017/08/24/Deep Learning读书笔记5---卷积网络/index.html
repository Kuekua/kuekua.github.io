<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      Deep Learning读书笔记5---卷积网络 | Kuekua&#39;s blog 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="Kuekua Wu">
    
    

    <meta name="description" content="1. 卷积公式：    2.卷积运算特点：稀疏交互：     参数共享： 参数共享是指在一个模型的多个函数中使用相同的参数。 在卷积神经网络中，核的每一个元素都作用在输入的每一位置上（是否考虑边界像素取决于对边界决策的设计）。 卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。 平移等变： 如果一个函数满足输入改变，输出也以同样的方式改变这">
<meta name="keywords" content="Machine Learning,Algorithm,Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning读书笔记5---卷积网络 | Kuekua&#39;s blog">
<meta property="og:url" content="http://yoursite.com/2017/08/24/Deep Learning读书笔记5---卷积网络/index.html">
<meta property="og:site_name" content="Kuekua&#39;s blog">
<meta property="og:description" content="1. 卷积公式：    2.卷积运算特点：稀疏交互：     参数共享： 参数共享是指在一个模型的多个函数中使用相同的参数。 在卷积神经网络中，核的每一个元素都作用在输入的每一位置上（是否考虑边界像素取决于对边界决策的设计）。 卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。 平移等变： 如果一个函数满足输入改变，输出也以同样的方式改变这">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://img.blog.csdn.net/20170927104818444?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/100/fill/I0JBQkFCMA==/dissolve/40/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170927105858854?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/40/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170927111624901?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170927113213889?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170927112647161?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170927114217706?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170927141521499?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170927142822259?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170927143010384?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast">
<meta property="og:updated_time" content="2017-10-28T07:53:47.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep Learning读书笔记5---卷积网络 | Kuekua&#39;s blog">
<meta name="twitter:description" content="1. 卷积公式：    2.卷积运算特点：稀疏交互：     参数共享： 参数共享是指在一个模型的多个函数中使用相同的参数。 在卷积神经网络中，核的每一个元素都作用在输入的每一位置上（是否考虑边界像素取决于对边界决策的设计）。 卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。 平移等变： 如果一个函数满足输入改变，输出也以同样的方式改变这">
<meta name="twitter:image" content="http://img.blog.csdn.net/20170927104818444?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/100/fill/I0JBQkFCMA==/dissolve/40/gravity/SouthEast">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.png">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css">

</head>
<body>

    <span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">Kuekua&#39;s blog</a></h1>
        <hr class="panel-cover__divider" />

        
        <p class="panel-cover__description">
          YesterDay you said tomorrow!
        </p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">首页</a></li>
              
                
                <li class="navigation__item"><a href="/archive" title="" class="">分类</a></li>
              
                
                <li class="navigation__item"><a href="/about" title="" class="">关于</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">Deep Learning读书笔记5---卷积网络</h1>

    

    <div class="post-meta">
      <time datetime="2017-08-24" class="post-meta__date date">2017-08-24</time> 

      <span class="post-meta__tags tags">

          
            <font class="categories">
            &#8226; 分类:
            <a class="categories-link" href="/categories/Deep-Learning-Book/">Deep Learning Book</a>
            </font>
          

          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-link" href="/tags/Algorithm/">Algorithm</a>, <a class="tags-link" href="/tags/Deep-Learning/">Deep Learning</a>, <a class="tags-link" href="/tags/Machine-Learning/">Machine Learning</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h2 id="1-卷积公式："><a href="#1-卷积公式：" class="headerlink" title="1. 卷积公式："></a>1. 卷积公式：</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170927104818444?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/100/fill/I0JBQkFCMA==/dissolve/40/gravity/SouthEast" width="40%" align="center"></div></p>
<div align="left"> 

<h2 id="2-卷积运算特点："><a href="#2-卷积运算特点：" class="headerlink" title="2.卷积运算特点："></a>2.卷积运算特点：</h2><p><strong>稀疏交互：</strong></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170927105858854?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/40/gravity/SouthEast" width="60%" align="center"></div></p>
<div align="left"> 

<p><strong>参数共享：</strong></p>
<p>参数共享是指在一个模型的多个函数中使用相同的参数。 在卷积神经网络中，核的每一个元素都作用在输入的每一位置上（是否考虑边界像素取决于对边界决策的设计）。 卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。</p>
<p><strong>平移等变：</strong></p>
<p>如果一个函数满足输入改变，输出也以同样的方式改变这一性质，我们就说它是等变(equivariant)的。 特别地，如果函数f(x)与g(x)满足$$f(g(x))= g(f(x))$$，我们就说f(x)对于变换g具有等变性。 对于卷积来说，如果令g是输入的任意平移函数，那么卷积函数对于g具有等变性。 </p>
<h2 id="3-池化"><a href="#3-池化" class="headerlink" title="3. 池化"></a>3. 池化</h2><p>池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。 例如，最大池化函数给出相邻矩形区域内的最大值。 其他常用的池化函数包括相邻矩形区域内的平均值、L^2范数以及基于据中心像素距离的加权平均函数。</p>
<p>不管采用什么样的池化函数，当输入作出少量平移时，池化能够帮助输入的表示近似不变。 对于平移的不变性是指当我们对输入进行少量平移时，经过池化函数后的大多数输出并不会发生改变。 </p>
<p><strong>局部平移不变性是一个很有用的性质，尤其是当我们关心某个特征是否出现而不关心它出现的具体位置时。</strong></p>
<p>使用池化可以看作是增加了一个无限强的先验：这一层学得的函数必须具有对少量平移的不变性。 当这个假设成立时，池化可以极大地提高网络的统计效率。</p>
<p>当我们对分离参数的卷积的输出进行池化时，特征能够学得应该对于哪种变换具有不变性，如下图所示。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170927111624901?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="60%" align="center"></div></p>
<div align="left"> 

<h2 id="4-基本卷积函数的变体"><a href="#4-基本卷积函数的变体" class="headerlink" title="4. 基本卷积函数的变体"></a>4. 基本卷积函数的变体</h2><p>我们有时会希望跳过核中的一些位置来降低计算的开销（相应的代价是提取特征没有先前那么好了）。 我们可以把这一过程看作是对全卷积函数输出的下采样(downsampling)。 如果我们只想在输出的每个方向上每间隔s个像素进行采样，那么我们可以定义一个下采样卷积函数c使得$$\begin{equation} Z<em>{i,j,k} = c(K, V, s)</em>{i,j,k} = \sum<em>{l,m,n} [V</em>{l,(j-1)\timess+m, (k-1)\times s +n,} K_{i,l,m,n}].\end{equation}$$，我们把s称为下采样卷积的步幅。 当然也可以对每个移动方向定义不同的步幅。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170927113213889?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast" width="60%" align="center"></div></p>
<div align="left"> 

<p><strong>平铺卷积</strong>对卷积层和局部连接层进行了折衷。 这里并不是对每一个空间位置的权重集合进行学习，我们学习一组核使得当我们在空间移动时它们可以循环利用。 这意味着在近邻的位置上拥有不同的过滤器，就像局部连接层一样，但是对于这些参数的存储需求仅仅会增长常数倍，这个常数就是核的集合的大小，而不是整个输出的特征映射的大小。</p>
<p>局部连接，卷积和全连接的比较 :</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170927112647161?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast" width="60%" align="center"></div></p>
<div align="left"> 

<h2 id="5-结构化输出"><a href="#5-结构化输出" class="headerlink" title="5. 结构化输出"></a>5. 结构化输出</h2><p>对图像逐个像素标记的一种策略是先产生图像标签的原始猜测，然后使用相邻像素之间的交互来修正该原始猜测。 重复这个修正步骤数次对应于在每一步使用相同的卷积，该卷积在深层网络的最后几层之间共享权重。 这使得在层之间共享参数的连续的卷积层所执行的一系列运算，形成了一种特殊的循环神经网络。 下图给出了这样一个循环卷积网络的结构。</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170927114217706?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast" width="60%" align="center"></div></p>
<p><div align="left"> </div></p>
<h2 id="6-数据类型"><a href="#6-数据类型" class="headerlink" title="6. 数据类型"></a>6. 数据类型</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170927141521499?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast" width="60%" align="center"></div></p>
<p><div align="left"> </div></p>
<h2 id="7-高效的卷积算法"><a href="#7-高效的卷积算法" class="headerlink" title="7. 高效的卷积算法"></a>7. 高效的卷积算法</h2><ol>
<li>卷积等效于使用傅立叶变换将输入与核都转换到频域、执行两个信号的逐点相乘，再使用傅立叶逆变换转换回时域。</li>
<li><p>当一个d维的核可以表示成d个向量（每一维一个向量）的外积时，该核被称为可分离的。  它等价于组合d个一维卷积，每个卷积使用这些向量中的一个。 组合方法显著快于使用它们的外积来执行一个d维的卷积。 并且核也只要更少的参数来表示成向量。 如果核在每一维都是w个元素宽，那么朴素的多维卷积需要O(w^d)的运行时间和参数存储空间，而可分离卷积只需要$$O(w\times d)$$的运行时间和参数存储空间。</p>
</li>
<li><p>随机或无监督的特征</p>
</li>
</ol>
<hr>
<p>通常，卷积网络训练中最昂贵的部分是学习特征。 减少卷积网络训练成本的一种方式是使用那些不是由监督方式训练得到的特征。</p>
<p>有三种基本策略可以不通过监督训练而得到卷积核：</p>
<ol>
<li>简单地随机初始化它们。 </li>
<li>手动设计它们，例如设置每个核在一个特定的方向或尺度来检测边缘。 </li>
<li>使用无监督的标准来学习核。 例如，将k均值聚类算法应用于小图像块，然后使用每个学得的中心作为卷积核。  使用无监督的标准来学习特征，允许这些特征的确定与位于网络结构顶层的分类层相分离。 然后只需提取一次全部训练集的特征，构造用于最后一层的新训练集。 假设最后一层类似逻辑回归或者SVM，那么学习最后一层通常是凸优化问题。</li>
</ol>
<h2 id="9-卷积网络的神经科学基础"><a href="#9-卷积网络的神经科学基础" class="headerlink" title="9. 卷积网络的神经科学基础"></a>9. 卷积网络的神经科学基础</h2><p>初级视觉皮层细胞具有由Gabor函数所描述的权重。 Gabor函数描述在图像中的2维点处的权重。我们可以认为图像是2维坐标I(x,y)的函数。 类似地，我们可以认为简单细胞是在图像中的一组位置采样，这组位置由一组x坐标X和一组y坐标Y来定义，并且使用的权重$w(x,y)$也是位置的函数。 从这个观点来看，简单细胞对于图像的响应由下式给出:</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170927142822259?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast" width="60%" align="center"></div></p>
<p><div align="left"><br>这里$$\alpha, \beta_x, \beta_y, f, \phi, x_0, y_0, \tau$$都是控制Gabor函数性质的参数。 下图给出了Gabor函数在不同参数集上的一些例子：</div></p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170927143010384?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dissolve/20/gravity/SouthEast" width="60%" align="center"></div></p>
</div></div></div></div></div>
  </section>

  <section class="post-comments">


<section id="comment">
<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2147613"></script>
<!-- UY END -->
</section>

    
</section>


</article>


            <div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kuekua Wu</span>

  
</div>









        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    
  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$$','$$'], ["\\(","\\)"] ],
      displayMath: [ ['$','$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
			    extensions: ["tex2jax.js"],
                jax: ["input/TeX", "output/HTML-CSS"],
                tex2jax: {inlineMath: [['[latex]','[/latex]'],['\\(','\\)']],
				          displayMath: [ ['$','$'], ["\\[","\\]"] ],
						  processEscapes: true
						  },
                "HTML-CSS": { availableFonts: ["TeX"] }						  
            });
        });
    </script>
	


    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
				autoDetectHeadings: true,
				enableToTopButton: true,
				displayNow: true,
				title: "文章目录",
				css: {
					fontSize: "16px",
					largeFontSize: "20px",
					},
            });
        });
    </script>


    
    

    <script src="/js/jquery.githubRepoWidget.min.js"></script>


    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]-->

</body>
</html>
