<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      Deep Learning读书笔记2---深度前馈网络 | Kuekua&#39;s blog 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="Kuekua Wu">
    
    

    <meta name="description" content="1.基本概念1.1隐藏层：神经网络中输入与输出层之间的中间层，训练数据并没有给出这些层中的每一层所需的输出，所以叫隐藏层。 1.2模型的宽度：网络中的每个隐藏层通常都是向量值的。这些隐藏层的维数决定了模型的宽度。 向量的每个元素都可以被视为起到类似一个神经元的作用。除了将层想象成向量到向量的单个函数，我们也可以把层想象成由许多并行操作的单元组成，每个单元表示一个向量到标量的函数。每个单元在某">
<meta name="keywords" content="Machine Learning,Algorithm,Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning读书笔记2---深度前馈网络 | Kuekua&#39;s blog">
<meta property="og:url" content="http://yoursite.com/2017/08/08/Deep Learning读书笔记2---深度前馈网络/index.html">
<meta property="og:site_name" content="Kuekua&#39;s blog">
<meta property="og:description" content="1.基本概念1.1隐藏层：神经网络中输入与输出层之间的中间层，训练数据并没有给出这些层中的每一层所需的输出，所以叫隐藏层。 1.2模型的宽度：网络中的每个隐藏层通常都是向量值的。这些隐藏层的维数决定了模型的宽度。 向量的每个元素都可以被视为起到类似一个神经元的作用。除了将层想象成向量到向量的单个函数，我们也可以把层想象成由许多并行操作的单元组成，每个单元表示一个向量到标量的函数。每个单元在某">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://img.blog.csdn.net/20170907102812491?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dis
solve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170907104221249?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170907104150199?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170907112810709?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dis
solve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170911163754870?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170911165232295?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast">
<meta property="og:updated_time" content="2017-10-28T03:18:13.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep Learning读书笔记2---深度前馈网络 | Kuekua&#39;s blog">
<meta name="twitter:description" content="1.基本概念1.1隐藏层：神经网络中输入与输出层之间的中间层，训练数据并没有给出这些层中的每一层所需的输出，所以叫隐藏层。 1.2模型的宽度：网络中的每个隐藏层通常都是向量值的。这些隐藏层的维数决定了模型的宽度。 向量的每个元素都可以被视为起到类似一个神经元的作用。除了将层想象成向量到向量的单个函数，我们也可以把层想象成由许多并行操作的单元组成，每个单元表示一个向量到标量的函数。每个单元在某">
<meta name="twitter:image" content="http://img.blog.csdn.net/20170907102812491?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dis
solve/70/gravity/SouthEast">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon.ico">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css">

</head>
<body>

    <span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">Kuekua&#39;s blog</a></h1>
        <hr class="panel-cover__divider" />

        
        <p class="panel-cover__description">
          YesterDay you said tomorrow!
        </p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">首页</a></li>
              
                
                <li class="navigation__item"><a href="/archive" title="" class="">分类</a></li>
              
                
                <li class="navigation__item"><a href="/about" title="" class="">关于</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">Deep Learning读书笔记2---深度前馈网络</h1>

    

    <div class="post-meta">
      <time datetime="2017-08-08" class="post-meta__date date">2017-08-08</time> 

      <span class="post-meta__tags tags">

          
            <font class="categories">
            &#8226; 分类:
            <a class="categories-link" href="/categories/Deep-Learning-Book/">Deep Learning Book</a>
            </font>
          

          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-link" href="/tags/Algorithm/">Algorithm</a>, <a class="tags-link" href="/tags/Deep-Learning/">Deep Learning</a>, <a class="tags-link" href="/tags/Machine-Learning/">Machine Learning</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

<h1 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1.基本概念"></a>1.基本概念</h1><h2 id="1-1隐藏层："><a href="#1-1隐藏层：" class="headerlink" title="1.1隐藏层："></a>1.1隐藏层：</h2><p>神经网络中输入与输出层之间的中间层，训练数据并没有给出这些层中的每一层所需的输出，所以叫隐藏层。</p>
<h2 id="1-2模型的宽度："><a href="#1-2模型的宽度：" class="headerlink" title="1.2模型的宽度："></a>1.2模型的宽度：</h2><p>网络中的每个隐藏层通常都是向量值的。这些隐藏层的维数决定了模型的宽度。 向量的每个元素都可以被视为起到类似一个神经元的作用。<br>除了将层想象成向量到向量的单个函数，我们也可以把层想象成由许多并行操作的单元组成，每个单元表示一个向量到标量的函数。<br>每个单元在某种意义上类似一个神经元，它接收的输入来源于许多其他的单元，并计算它自己的激活值。  </p>
<h2 id="1-3激活函数："><a href="#1-3激活函数：" class="headerlink" title="1.3激活函数："></a>1.3激活函数：</h2><p>用于计算隐藏层值的函数，常见的有整流线性单元（分段函数max{0,f()}）。</p>
<h2 id="1-4常见的代价函数："><a href="#1-4常见的代价函数：" class="headerlink" title="1.4常见的代价函数："></a>1.4常见的代价函数：</h2><p>交叉熵（负的最大似然函数）</p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170907102812491?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dis
solve/70/gravity/SouthEast" width="40%"></div></p>
<div align="left">

<h1 id="2-输出单元"><a href="#2-输出单元" class="headerlink" title="2.输出单元"></a>2.输出单元</h1><h2 id="2-1-线性单元"><a href="#2-1-线性单元" class="headerlink" title="2.1 线性单元"></a>2.1 线性单元</h2><p>线性输出层经常被用来产生条件高斯分布的均值：<br>p(y∣x)=N(y;y^,I).</p>
<h2 id="2-2-logistic-sigmoid函数"><a href="#2-2-logistic-sigmoid函数" class="headerlink" title="2.2 logistic sigmoid函数"></a>2.2 logistic sigmoid函数</h2><p>sigmoid单元可用于Bernoulli输出分布，许多任务需要预测二值型变量  y  的值。<br>具有两个类的分类问题可以归结为这种形式。sigmoid输出单元定义为：  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170907104221249?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="20%"></div></p>
<div align="left"> 


<p><div align="center"><br><img src="http://img.blog.csdn.net/20170907104150199?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="25%"></div></p>
<div align="left">

<h2 id="2-3-softmax单元"><a href="#2-3-softmax单元" class="headerlink" title="2.3 softmax单元"></a>2.3 softmax单元</h2><p>任何时候当我们想要表示一个具有n个可能取值的离散型随机变量的分布时，我们都可以使用softmax函数。softmax函数的形式为:  </p>
<p><div align="center"><br><img src="http://img.blog.csdn.net/20170907112810709?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/dis
solve/70/gravity/SouthEast" width="30%"></div></p>
<div align="left">

<h1 id="3-隐藏单元"><a href="#3-隐藏单元" class="headerlink" title="3.隐藏单元"></a>3.隐藏单元</h1><h2 id="3-1整流线性单元"><a href="#3-1整流线性单元" class="headerlink" title="3.1整流线性单元"></a>3.1整流线性单元</h2><p>整流线性单元使用激活函数  g  (  z  )  =  max  (  0  ,  z  )</p>
<p><strong> 扩展： </strong>   </p>
<p>a. 当  z_i &lt; 0  时使用一个非零的斜率  $$α_i：h_i =  g  (z,  α  )  i =  max  (0  ,  z_i  )  +  α_i min  (  0  ,  z_i  )$$ 。<br>b. 绝对值整流固定  α  i  =  −  1  来得到  g  (  z  )  =  |  z  |  。<br>它用于图像中的对象识别{cite?}，其中寻找在输入照明极性反转下不变的特征是有意义的。<br>c. 渗漏整流线性单元将  α_i固定成一个类似0.01的小值，参数化整流线性单元（PReLU）将  α  i 作为学习的参数。<br>d. maxout单元将  z  划分为每组具有  k  个值的组，而不是使用作用于每个元素的函数  g  (  z  )  。<br>每个maxout单元则输出每组中的最大元素。</p>
<h2 id="3-2-logistic-sigmoid与双曲正切函数"><a href="#3-2-logistic-sigmoid与双曲正切函数" class="headerlink" title="3.2 logistic sigmoid与双曲正切函数"></a>3.2 logistic sigmoid与双曲正切函数</h2><p>在引入整流线性单元之前，大多数神经网络使用logistic sigmoid激活函数<br>g(z)=σ(z)<br>或者是双曲正切激活函数<br>g(z)=tanh(z)<br>这些激活函数紧密相关，因为  tanh  (  z  )  =  2  σ  (  2  z  )  −  1  。<br>与分段线性单元不同，sigmoid单元在其大部分定义域内都饱和——当  z  取绝对值很大的正值时，它们饱和到一个高值，当  z<br>取绝对值很大的负值时，它们饱和到一个低值，并且仅仅当  z  接近0时它们才对输入强烈敏感。<br>sigmoid单元的广泛饱和性会使得基于梯度的学习变得非常困难。 因为这个原因，现在不鼓励将它们用作前馈网络中的隐藏单元。<br>当使用一个合适的代价函数来抵消sigmoid的饱和性时，它们作为输出单元可以与基于梯度的学习相兼容。</p>
<p>当必须要使用sigmoid激活函数时，双曲正切激活函数通常要比logistic sigmoid函数表现更好。 在  tanh  (  0  )  =  0<br>而  σ  (  0  )  =  1  2  的意义上，它更像是单位函数。 因为  tanh  在0附近与单位函数类似，训练深层神经网络  $$y^  =w^⊤  tanh  (  U^⊤  tanh  (  V^⊤  x  ))$$  类似于训练一个线性模型  $$y^  =  w^⊤U^⊤V^⊤x $$ ，只要网络的激活能够被保持地很小。 这使得训练  tanh  网络更加容易。</p>
<h1 id="4-架构设计"><a href="#4-架构设计" class="headerlink" title="4.架构设计"></a>4.架构设计</h1><p><strong> 万能近似定理： </strong> 一个前馈神经网络如果具有线性输出层和至少一层具有任何一种”挤压”性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数。 前馈网络的导数也可以任意好地来近似函数的导数。 </p>
<p>在神经网络框架中，更深的神经网络远远比大模型尺寸的单层网络好得多，这说明神经网络中应该用许多简单的函数组成，而尽量不用复杂庞大的单模型。</p>
<h1 id="5-反向传播"><a href="#5-反向传播" class="headerlink" title="5.反向传播"></a>5.反向传播</h1><h2 id="5-1链式法则"><a href="#5-1链式法则" class="headerlink" title="5.1链式法则"></a>5.1链式法则</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170911163754870?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">

<p>从这里我们看到，变量  x  的梯度可以通过~Jacobian矩阵  ∂  y  ∂  x  和梯度  ∇  y  z  相乘来得到。<br>反向传播算法由图中每一个这样的~Jacobian~梯度的乘积操作所组成。</p>
<h2 id="5-2反向传播步骤："><a href="#5-2反向传播步骤：" class="headerlink" title="5.2反向传播步骤："></a>5.2反向传播步骤：</h2><p><div align="center"><br><img src="http://img.blog.csdn.net/20170911165232295?watermark/2/text/aHR0cDovL2Jsb2
cuY3Nkbi5uZXQvdTAxMjU1NDA5Mg==/font/5a6L5L2T/fontsize/40/fill/I0JBQkFCMA==/di
ssolve/70/gravity/SouthEast" width="70%"></div></p>
<div align="left">

<h2 id="5-3符号到符号的导数"><a href="#5-3符号到符号的导数" class="headerlink" title="5.3符号到符号的导数"></a>5.3符号到符号的导数</h2><p>代数表达式和计算图都对符号或不具有特定值的变量进行操作。 这些代数或者基于图的表达式被称为符号表示。<br>当我们实际使用或者训练神经网络时，我们必须给这些符号赋特定的值。 我们用一个特定的数值来替代网络的符号输入  x  ，例如  [  1.2  ,  3  ,7.65  ,  −  1.8  ]  ⊤  。</p>
<p>一些反向传播的方法采用计算图和一组用于图的输入的数值，然后返回在这些输入值处梯度的一组数值。 我们将这种方法称为符号到数值的微分。<br>这种方法用在诸如Torch和Caffe之类的库中。</p>
<p>另一种方法是采用计算图以及添加一些额外的节点到计算图中，这些额外的节点提供了我们所需导数的符号描述。<br>这是Theano和TensorFlow所采用的方法。这种方法的主要优点是导数可以使用与原始表达式相同的语言来描述。<br>因为导数只是另外一张计算图，我们可以再次运行反向传播，对导数再进行求导就能得到更高阶的导数。</p>
<p>基于符号到符号的方法的描述包含了符号到数值的方法。 符号到数值的方法可以理解为执行了与符号到符号的方法中构建图的过程中完全相同的计算。<br>关键的区别是符号到数值的方法不会显示出计算图。</p>
</div></div></div></div></div></div>
  </section>

  <section class="post-comments">


<section id="comment">
<!-- UY BEGIN -->
<div id="uyan_frame"></div>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2147613"></script>
<!-- UY END -->
</section>

    
</section>


</article>


            <footer class="footer">
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
复制代码
    <span class="footer__copyright">&copy; 2017-2020. | 由<a href="https://hexo.io/">Kukeua</a>强力驱动</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    
  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$$','$$'], ["\\(","\\)"] ],
      displayMath: [ ['$','$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
			    extensions: ["tex2jax.js"],
                jax: ["input/TeX", "output/HTML-CSS"],
                tex2jax: {inlineMath: [['[latex]','[/latex]'],['\\(','\\)']],
				          displayMath: [ ['$','$'], ["\\[","\\]"] ],
						  processEscapes: true
						  },
                "HTML-CSS": { availableFonts: ["TeX"] }						  
            });
        });
    </script>
	


    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
				autoDetectHeadings: true,
				enableToTopButton: true,
				displayNow: true,
				title: "文章目录",
				css: {
					fontSize: "16px",
					largeFontSize: "20px",
					},
            });
        });
    </script>


    
    

    <script src="/js/jquery.githubRepoWidget.min.js"></script>


    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]-->

</body>
</html>
